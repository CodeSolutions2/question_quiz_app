{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8311d7c",
   "metadata": {},
   "source": [
    "# Etudier pour Python Linkedin 😔\n",
    "\n",
    "Exam URL: https://www.linkedin.com/skill-assessments/Python%20(Programming%20Language)/quiz-intro/\n",
    "\n",
    "https://www.gcertificationcourse.com/linkedin-python-skill-quiz-answers/\n",
    "\n",
    "https://learn.microsoft.com/en-us/python/api/overview/azure/storage-file-share-readme?view=azure-python\n",
    "\n",
    "https://learn.microsoft.com/en-us/rest/api/storageservices/put-blob-from-url?tabs=azure-ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6980aa8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en-US\">\\n    <head>\\n        <!--start-optimizely--><!--end-optimizely-->\\n        <!-- Google Tag Manager -->\\n<script>\\nwindow.dataLayer = window.dataLayer || [];\\ndataLayer.push({\"environment\":\"production\"});\\ndataLayer.push({\"ads\":{\"display\":true}});\\n(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push(\\n{\\'gtm.start\\': new Date().getTime(),event:\\'gtm.js\\'}\\n);var f=d.getElementsByTagName(s)[0],\\nj=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\n\\'//www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n})(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-KCC7SF\\');</script>\\n<!-- End Google Tag Manager -->                    \\n\\n                    \\n                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=no, minimal-ui\" >\\n        \\n    <meta name=\"apple-itunes-app\" content=\"app-id=734887700\">\\n    <meta name=\"google-play-app\" content=\"app-id=com.studymode.cram\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n \\n    <link rel=\"stylesheet\" href=\"/assets/whiteboard/styles/smart-app-banner.css\" type=\"text/css\" media=\"screen\">\\n\\n        \\n        \\n        <link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"//www.cram.com/favicon.ico\">\\n        <link rel=\"canonical\" href=\"https://www.cram.com/flashcards/git-12843452\"><meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" >\\n<meta name=\"description\" content=\"Study Flashcards On Git_DATASCIENCE at Cram.com. Quickly memorize the terms, phrases and much more. Cram.com makes it easy to get the grade you want!\" >\\n<meta name=\"ROBOTS\" content=\"NOINDEX, NOFOLLOW\" ><title>Git_DATASCIENCE Flashcards - Cram.com</title>            <script type=\"text/javascript\" src=\"https://browser.sentry-cdn.com/5.24.2/bundle.tracing.min.js\" crossorigin=\\'anonymous\\'></script>\\n            <script type=\"text/javascript\">\\n                if(Sentry) {\\n                    Sentry.init({\\n                          dsn: \\'https://2d050d29bad34510a09dcc3b05120fa3@o133474.ingest.sentry.io/5411670\\',\\n                          version: \\'1.0.3\\',\\n                          ignoreErrors: [/NotAllowedError/,/NotSupportedError/,/SecurityError:/,/NetworkError:/,/AbortError:/,/TypeError:/,/Unable to get property/,/Cannot (read|set) property/,/ResizeObserver loop limit exceeded/,/Failed to fetch/,/No error message/,/Can\\'t execute code from a freed script/,/Extension context invalidated/,/JSON Parse error/,/is not (a|an) (object|constructor|function)/,/Non-Error promise rejection/,/Object expected/,/is (null|undefined)/,/Object doesn\\'t support this action/,/Unexpected token/],\\n                          blacklistUrls: [\\n                            /graph\\\\.facebook\\\\.com/i,\\n                            /connect\\\\.facebook\\\\.net\\\\/en_US\\\\/all\\\\.js/i,\\n                            /extensions\\\\//i,\\n                            /^chrome:\\\\/\\\\//i,\\n                            /127\\\\.0\\\\.0\\\\.1:4001\\\\/isrunning/i,\\n                          ]\\n                    });\\n                    console.log(\\'Sentry Initialized\\');\\n                }\\n          </script><script type=\"text/javascript\">\\n     var googletag = googletag || {};\\n     googletag.cmd = googletag.cmd || [];\\n     (function() {\\n         var gads = document.createElement(\"script\");\\n         gads.async = true;\\n         gads.type = \"text/javascript\";\\n         var useSSL = \"https:\" == document.location.protocol;\\n         gads.src = (useSSL ? \"https:\" : \"http:\") +\\n             \"//www.googletagservices.com/tag/js/gpt.js\";\\n         var node = document.getElementsByTagName(\"script\")[0];\\n         node.parentNode.insertBefore(gads, node);\\n     })();\\n </script>\\n <script type=\"text/javascript\">\\n     googletag.cmd.push(function() {googletag.pubads().enableSingleRequest();googletag.enableServices();});</script><link href=\"//beckett.cram.com/1.13/css/cram.1.13.3.min.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\" >\\n<link href=\"/fce/css/_sets/2969960122.css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\" >\\n<!--[if lte IE 8]> <link href=\"/assets/whiteboard/styles/ie8-and-down.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\" ><![endif]-->\\n<link href=\"//fonts.googleapis.com/css?family=Open+Sans:100,300,400,600,700\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\" >        <script type=\"text/javascript\">\\n        /*! LAB.js (LABjs :: Loading And Blocking JavaScript)\\n    v2.0.3 (c) Kyle Simpson\\n    MIT License\\n*/\\n(function(o){var K=o.$LAB,y=\"UseLocalXHR\",z=\"AlwaysPreserveOrder\",u=\"AllowDuplicates\",A=\"CacheBust\",B=\"BasePath\",C=/^[^?#]*\\\\//.exec(location.href)[0],D=/^\\\\w+\\\\:\\\\/\\\\/\\\\/?[^\\\\/]+/.exec(C)[0],i=document.head||document.getElementsByTagName(\"head\"),L=(o.opera&&Object.prototype.toString.call(o.opera)==\"[object Opera]\")||(\"MozAppearance\"in document.documentElement.style),q=document.createElement(\"script\"),E=typeof q.preload==\"boolean\",r=E||(q.readyState&&q.readyState==\"uninitialized\"),F=!r&&q.async===true,M=!r&&!F&&!L;function G(a){return Object.prototype.toString.call(a)==\"[object Function]\"}function H(a){return Object.prototype.toString.call(a)==\"[object Array]\"}function N(a,c){var b=/^\\\\w+\\\\:\\\\/\\\\//;if(/^\\\\/\\\\/\\\\/?/.test(a)){a=location.protocol+a}else if(!b.test(a)&&a.charAt(0)!=\"/\"){a=(c||\"\")+a}return b.test(a)?a:((a.charAt(0)==\"/\"?D:C)+a)}function s(a,c){for(var b in a){if(a.hasOwnProperty(b)){c[b]=a[b]}}return c}function O(a){var c=false;for(var b=0;b<a.scripts.length;b++){if(a.scripts[b].ready&&a.scripts[b].exec_trigger){c=true;a.scripts[b].exec_trigger();a.scripts[b].exec_trigger=null}}return c}function t(a,c,b,d){a.onload=a.onreadystatechange=function(){if((a.readyState&&a.readyState!=\"complete\"&&a.readyState!=\"loaded\")||c[b])return;a.onload=a.onreadystatechange=null;d()}}function I(a){a.ready=a.finished=true;for(var c=0;c<a.finished_listeners.length;c++){a.finished_listeners[c]()}a.ready_listeners=[];a.finished_listeners=[]}function P(d,f,e,g,h){setTimeout(function(){var a,c=f.real_src,b;if(\"item\"in i){if(!i[0]){setTimeout(arguments.callee,25);return}i=i[0]}a=document.createElement(\"script\");if(f.type)a.type=f.type;if(f.charset)a.charset=f.charset;if(h){if(r){e.elem=a;if(E){a.preload=true;a.onpreload=g}else{a.onreadystatechange=function(){if(a.readyState==\"loaded\")g()}}a.src=c}else if(h&&c.indexOf(D)==0&&d[y]){b=new XMLHttpRequest();b.onreadystatechange=function(){if(b.readyState==4){b.onreadystatechange=function(){};e.text=b.responseText+\"\\\\n//@ sourceURL=\"+c;g()}};b.open(\"GET\",c);b.send()}else{a.type=\"text/cache-script\";t(a,e,\"ready\",function(){i.removeChild(a);g()});a.src=c;i.insertBefore(a,i.firstChild)}}else if(F){a.async=false;t(a,e,\"finished\",g);a.src=c;i.insertBefore(a,i.firstChild)}else{t(a,e,\"finished\",g);a.src=c;i.insertBefore(a,i.firstChild)}},0)}function J(){var l={},Q=r||M,n=[],p={},m;l[y]=true;l[z]=false;l[u]=false;l[A]=false;l[B]=\"\";function R(a,c,b){var d;function f(){if(d!=null){d=null;I(b)}}if(p[c.src].finished)return;if(!a[u])p[c.src].finished=true;d=b.elem||document.createElement(\"script\");if(c.type)d.type=c.type;if(c.charset)d.charset=c.charset;t(d,b,\"finished\",f);if(b.elem){b.elem=null}else if(b.text){d.onload=d.onreadystatechange=null;d.text=b.text}else{d.src=c.real_src}i.insertBefore(d,i.firstChild);if(b.text){f()}}function S(c,b,d,f){var e,g,h=function(){b.ready_cb(b,function(){R(c,b,e)})},j=function(){b.finished_cb(b,d)};b.src=N(b.src,c[B]);b.real_src=b.src+(c[A]?((/\\\\?.*$/.test(b.src)?\"&_\":\"?_\")+~~(Math.random()*1E9)+\"=\"):\"\");if(!p[b.src])p[b.src]={items:[],finished:false};g=p[b.src].items;if(c[u]||g.length==0){e=g[g.length]={ready:false,finished:false,ready_listeners:[h],finished_listeners:[j]};P(c,b,e,((f)?function(){e.ready=true;for(var a=0;a<e.ready_listeners.length;a++){e.ready_listeners[a]()}e.ready_listeners=[]}:function(){I(e)}),f)}else{e=g[0];if(e.finished){j()}else{e.finished_listeners.push(j)}}}function v(){var e,g=s(l,{}),h=[],j=0,w=false,k;function T(a,c){a.ready=true;a.exec_trigger=c;x()}function U(a,c){a.ready=a.finished=true;a.exec_trigger=null;for(var b=0;b<c.scripts.length;b++){if(!c.scripts[b].finished)return}c.finished=true;x()}function x(){while(j<h.length){if(G(h[j])){try{h[j++]()}catch(err){}continue}else if(!h[j].finished){if(O(h[j]))continue;break}j++}if(j==h.length){w=false;k=false}}function V(){if(!k||!k.scripts){h.push(k={scripts:[],finished:true})}}e={script:function(){for(var f=0;f<arguments.length;f++){(function(a,c){var b;if(!H(a)){c=[a]}for(var d=0;d<c.length;d++){V();a=c[d];if(G(a))a=a();if(!a)continue;if(H(a)){b=[].slice.call(a);b.unshift(d,1);[].splice.apply(c,b);d--;continue}if(typeof a==\"string\")a={src:a};a=s(a,{ready:false,ready_cb:T,finished:false,finished_cb:U});k.finished=false;k.scripts.push(a);S(g,a,k,(Q&&w));w=true;if(g[z])e.wait()}})(arguments[f],arguments[f])}return e},wait:function(){if(arguments.length>0){for(var a=0;a<arguments.length;a++){h.push(arguments[a])}k=h[h.length-1]}else k=false;x();return e}};return{script:e.script,wait:e.wait,setOptions:function(a){s(a,g);return e}}}m={setGlobalDefaults:function(a){s(a,l);return m},setOptions:function(){return v().setOptions.apply(null,arguments)},script:function(){return v().script.apply(null,arguments)},wait:function(){return v().wait.apply(null,arguments)},queueScript:function(){n[n.length]={type:\"script\",args:[].slice.call(arguments)};return m},queueWait:function(){n[n.length]={type:\"wait\",args:[].slice.call(arguments)};return m},runQueue:function(){var a=m,c=n.length,b=c,d;for(;--b>=0;){d=n.shift();a=a[d.type].apply(null,d.args)}return a},noConflict:function(){o.$LAB=K;return m},sandbox:function(){return J()}};return m}o.$LAB=J();(function(a,c,b){if(document.readyState==null&&document[a]){document.readyState=\"loading\";document[a](c,b=function(){document.removeEventListener(c,b,false);document.readyState=\"complete\"},false)}})(\"addEventListener\",\"DOMContentLoaded\")})(this);\\n        </script>\\n\\n<!-- <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css\"> -->\\n          </head>\\n    <body >\\n\\n                <!-- Google Tag Manager (No Script) -->\\n        <noscript><iframe src=\"//www.googletagmanager.com/ns.html?id=GTM-KCC7SF\"\\n        height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\\n        <!-- Google Tag Manager (No Script) end -->\\n        <div class=\"body\">\\n                    \\t<div aria-labelledby=\"header\" class=\"patternlab white--bg gray-ultra-light--bg-small\">  \\n   <nav class=\"row top-nav top-nav--slim\">\\n      <div class=\"columns small-8 medium-4 large-3 full-height\">\\n         <a name=\"menu\" role=\"button\" href=\"javascript:void(0);\" id=\"hamburgerExpand\" aria-expanded=\"false\" aria-label=\"Main menu open\" class=\"burger full-height vert-align-middle display-inline-block cursor-pointer position-relative\" data-bv-click=\"toggleVis():\\'burger-popup-menu\\';toggleAttr(\\'aria-expanded\\',\\'true\\',\\'false\\'):\\'hamburgerExpand\\'\">\\n            <img class=\"vert-center burger__hero-img\" src=\"//beckett.cram.com/1.12/images/icons/burger.png\"/>                     \\n         </a>   \\n         <div class=\"card bv-popup card--bv-popup card--bv-popup--burger card--bv-popup--burger-left\" id=\"burger-popup-menu\">\\n               <ul class=\"card__menu-list \">\\n                  <li class=\"card__menu-list-item text-xs\">\\n                     <a class=\"card__menu-list-link display-block brand-primary--hover\" \\n                        href=\"/\"\\n                        >\\n                     Home\\n                     </a>\\n                  </li>\\n                  <li class=\"card__menu-list-item card__menu-list-header text-xs\">\\n                     <a class=\"charcoal-gray display-block \" style=\"text-decoration: none;\"\\n                        href=\"/flashcards\" >\\n                     Flashcards\\n                     </a>\\n                  </li>\\n                  <li class=\"card__menu-list-item text-xs\">\\n                     <a class=\"card__menu-list-link display-block green--hover\" \\n                        href=\"/flashcards/create\">\\n                     Create Flashcards\\n                     </a>\\n                  </li>\\n                  <li class=\"card__menu-list-item card__menu-list-header text-xs display-block\">\\n                     <a class=\"charcoal-gray display-block\" style=\"text-decoration: none;\" \\n                        href=\"/writing\">\\n                     Essays\\n                     </a>\\n                  </li>\\n                  <li class=\"card__menu-list-item  text-xs\">\\n                     <a class=\"charcoal-gray display-block \" style=\"text-decoration: none;\" href=\"/topics\">\\n                     Essay Topics                        \\n                     </a>\\n                  </li>\\n                  <li class=\"card__menu-list-item  text-xs\">\\n                     <a class=\"charcoal-gray display-block \" style=\"text-decoration: none;\" href=\"/writing-tool/edit\">\\n                     Language and Plagiarism Checks\\n                     </a>\\n                  </li>\\n               </ul>\\n         </div>         \\n         <div class=\"show-for-large display-inline-block full-height\">\\n            <a href=\"/\">\\n               <img name=\"Homepage\" class=\"vert-center\" src=\"//beckett.cram.com/1.12/images/logos/cram/logo-cram.png\" width=\"127\" height=\"30\"/>\\n            </a>\\n                   </div>\\n         <div class=\"hide-for-large-up display-inline-block vert-align-middle full-height\">\\n            <a href=\"/\">\\n               <img name=\"Homepage\" class=\"vert-center\" src=\"//beckett.cram.com/1.12/images/logos/cram/logo-cram-mobile.png\" width=\"84\" height=\"20\"/>\\n            </a>\\n         </div>\\n      </div>\\n      <div class=\"columns position-relative small-2 medium-6 large-5 full-height\">\\n                  <div class=\"search-box search-box--blue header-splat__search-box header-splat__search-box--custom-nav vert-center--medium \" id=\"cram-search-box\">\\n            <form class=\"toggle-trigger\" id=\"\" action=\"/search\" method=\"\">\\n               <a href=\"javascript:void(0)\" role=\"combobox\" class=\"center uppercase weight-500 text-xs cursor-pointer search-box__select cornflowerblue-btn-bg\" data-bv-click=\"toggleVis():\\'search-select-popup\\';\">\\n               <span class=\"search-box__select-text\">\\n               Flashcards\\n               </span>\\n               </a>\\n               <div class=\"card bv-popup card--bv-popup search-box__popup\" id=\"search-select-popup\">\\n                  <ul class=\"card__menu-list \">\\n                     <li class=\"card__menu-list-item cursor-pointer white align-left uppercase text-xs display-block\">\\n                        <a class=\"card__menu-list-link display-block blue blue--hover\" \\n                           data-bv-click=\"setFocus():\\'query\\';toggleVis():\\'search-select-popup\\';\">FlashCards</a>\\n                     </li>\\n                     <li class=\"card__menu-list-item cursor-pointer white align-left uppercase text-xs display-block\">\\n                        <a class=\"card__menu-list-link display-block blue--hover\" \\n                           href=\"/#essays\">Essays</a>\\n                     </li>                     \\n                  </ul>\\n               </div>\\n               <input type=\"text\" class=\"search-box__input search-box search-box__input--with-select search-box__input--splat-header\" name=\"query\" id=\"query\" placeholder=\"Search over 166 million flashcards\">\\n               <button aria-label=\"Search\" class=\"search-box__button search-box__button search-box__button--splat-header cornflowerblue-btn-bg\" type=\"submit\">\\n               <span class=\"search-box__icon search-box__icon--splat-header white\">\\n               <i class=\"icon icon-ui-24-search\"></i>\\n               </span>\\n               </button>\\n            </form>\\n         </div>\\n         <div class=\"header-splat__search-toggle-container header-splat__search-toggle-container--custom-nav\" data-bv-click=\"toggleClass(\\'conditionally-visible\\'):\\'cram-search-box\\';\">\\n            <i class=\"green weight-500 icon icon-ui-24-search header-splat__icon-ui-24-search-toggle\" id=\"header-splat__icon-ui-24-search-toggle\"></i>\\n         </div>\\n               </div>\\n      <div class=\"columns hide-for-small-only hide-for-medium-only large-3 full-height align-right\">\\n         <div class=\"top-nav__item vert-center\">\\n            <a href=\"/flashcards/create\" class=\"cornflowerblue text-xs\">Create Flashcards</a>\\n         </div>\\n      </div>\\n      <div class=\"columns small-2 medium-2 large-1 full-height\">\\n                  \\n         <div id=\"account-signin-link\" class=\"vert-center align-left\">\\n            <a href=\"/user/login\" class=\"cornflowerblue text-xs\">Sign&nbsp;in</a>\\n         </div>\\n                  \\n            \\n      </div>\\n   </nav>\\n</div>\\n\\n\\n\\n\\n\\n\\n<script src=\"/assets/libs/smartbanner.js\"></script>\\n    <script type=\"text/javascript\">\\n\\n    if ( !((/(iPhone)*(OS ([6-9]|1[0-9])_)/i.test(navigator.userAgent))&&(/Safari/.test(navigator.userAgent))) ) {\\n      new SmartBanner({\\n          daysHidden: 0,   // days to hide banner after close button is clicked (defaults to 15)\\n          daysReminder: 0, // days to hide banner after \"VIEW\" button is clicked (defaults to 90)\\n          appStoreLanguage: \\'us\\', // language code for the App Store (defaults to user\\'s browser language)\\n          title: \\'Cram\\',\\n          author: \\'Studymode\\',\\n          button: \\'VIEW\\',\\n          store: {\\n              ios: \\'On the App Store\\',\\n              android: \\'In Google Play\\',\\n              windows: \\'In Windows store\\'\\n          },\\n          price: {\\n              ios: \\'FREE\\',\\n              android: \\'FREE\\',\\n              windows: \\'FREE\\'\\n          }\\n           , theme: \\'ios\\' // put platform type (\\'ios\\', \\'android\\', etc.) here to force single theme on all device\\n           , icon: \\'/images/cram_icon_384.png\\' // full path to icon image if not using website icon image\\n           //, force: \\'android\\' // Uncomment for platform emulation\\n      });\\n    }\\n     if ( ((/(iPhone)*(OS ([6-9]|1[0-9])_)/i.test(navigator.userAgent))&&(navigator.userAgent.match(\\'CriOS\\')) )) {\\n      new SmartBanner({\\n          daysHidden: 0,   // days to hide banner after close button is clicked (defaults to 15)\\n          daysReminder: 0, // days to hide banner after \"VIEW\" button is clicked (defaults to 90)\\n          appStoreLanguage: \\'us\\', // language code for the App Store (defaults to user\\'s browser language)\\n          title: \\'Cram\\',\\n          author: \\'Studymode\\',\\n          button: \\'VIEW\\',\\n          store: {\\n              ios: \\'On the App Store\\',\\n              android: \\'In Google Play\\',\\n              windows: \\'In Windows store\\'\\n          },\\n          price: {\\n              ios: \\'FREE\\',\\n              android: \\'FREE\\',\\n              windows: \\'FREE\\'\\n          }\\n           , theme: \\'ios\\' // put platform type (\\'ios\\', \\'android\\', etc.) here to force single theme on all device\\n           , icon: \\'/images/cram_icon_384.png\\' // full path to icon image if not using website icon image\\n           //, force: \\'android\\' // Uncomment for platform emulation\\n      });\\n    }\\n    </script>\\n<div class=\"modal\" id=\"shareBoxModal\">\\n\\t<form action=\"/share\" id=\"share_set\" class=\"login_form\" method=\"post\">\\n        <div id=\"modal_header\"><h3>Share This Flashcard Set</h3> <a id=\"close_modal\" href=\"javascript:void(0);\"><span>Close</span></a></div>\\n        <div id=\"modal_body\" class=\"test\">\\n\\n                      <div class=\"modal_content sign_in\">\\n                <p>Please sign in to share these flashcards. We\\'ll bring you back here when you are done.</p>\\n                <p>\\n                    <a id=\"signInBtn\" class=\"cornflowerblue-btn small-btn round-btn btn thick-btn\" href=\"javascript:void(0);\">\\n                        <span>Sign in</span>\\n                    </a>\\n                </p>\\n                <p class=\"footer\">Don\\'t have an account? <a href=\"javascript:void(0);\" id=\"signUpModalBtn\">Sign Up &raquo;</a></p>\\n            </div>\\n                  </div>\\n    </form>\\n</div>\\n<div id=\"fb-root\"></div>\\n<div class=\"content-type1 study_content\">\\n  <input type=\"hidden\" id=\"setID\" value=\"\" />\\n  <input type=\"hidden\" id=\"setURLLink\" value=\"www.cram.com/flashcards/\" />\\n  <input type=\"hidden\" id=\"urlLink\" value=\"www.cram.com\" />\\n  <input id=\"userLoggedIn\" type=\"hidden\" value=\"0\" />\\n  <input id=\"s3ImageUri\" type=\"hidden\" value=\"https://images.cram.com\" />\\n      <div class=\"heading\">\\n      <h1>Private Set</h1>\\n    </div>\\n    <div class=\"main-content no-access\">\\n      <div class=\"sorry-msg\">\\n        <div>\\n          <h2>Sorry, this is a private set. You don\\'t have permissions to view it.</h2>\\n                      <p>If this set belongs to you, please sign in.</p>\\n            <p>\\n              <a class=\"btn medium-btn thick-btn cornflowerblue-btn round-btn\" title=\"Login\" href=\"/user/login\">\\n                Sign in              </a>\\n            </p>\\n                  </div>\\n      </div>\\n    </div>\\n  </div>\\n\\n  <div class=\"advertBox\">\\n    <div class=\"longerBannerAd\">\\n          </div>\\n  </div>\\n\\n<script type=\"text/javascript\">\\n  var Cards = null,\\n    Langs = {\"lang_front\":null,\"lang_back\":null};\\n</script>\\n\\n\\n\\n\\n<script type=\"text/javascript\">\\r\\nfunction loadJQ(callback) {\\r\\n  if(!window.jQuery) {\\r\\n    // Create jQuery script element.\\r\\n    var script = document.createElement(\\'script\\');\\r\\n    script.type = \\'text/javascript\\';\\r\\n    script.src = \\'//ajax.googleapis.com/ajax/libs/jquery/3.6.3/jquery.min.js\\';\\r\\n    document.body.appendChild(script);\\r\\n\\r\\n    script.onload = function(){ callback(jQuery); };\\r\\n    // IE 6 & 7 ala jfriend00\\r\\n    script.onreadystatechange = function() {\\r\\n      if (this.readyState == \\'complete\\') callback(jQuery);\\r\\n    }\\r\\n  } else {\\r\\n    callback(jQuery);\\r\\n  }\\r\\n}\\r\\n\\r\\nloadJQ(function($){\\r\\n  console.log($ === jQuery); // true. jquery is loaded.\\r\\n});\\r\\n</script>\\r\\n<footer>\\r\\n    <div class=\"footer\">\\r\\n                <div class=\"row\">\\r\\n            <div style=\"float: left;width:12%\"></div>\\r\\n            <div style=\"float:right;width:88%\"></div>\\r\\n        </div>    \\r\\n        <div class=\"nta\">\\r\\n            <img src=\"/fce/images/nta_logo.png\" width=\"200\" /> <span>Cram has partnered with the National Tutoring Association </span>\\r\\n            <a class=\"btn medium-btn thick-btn cornflowerblue-btn round-btn\" href=\"/nta\">Claim your access </a>\\r\\n        </div>\\r\\n                    <div class=\"menu-boxes\">\\r\\n                <div class=\"signup\"><p>Ready To Get Started?</p>\\r\\n                    <a \\r\\n                        id=\"buttonFooterCreateFlashcards\" \\r\\n                        class=\"btn medium-btn thick-btn cornflowerblue-btn round-btn\" \\r\\n                        href=\"/flashcards/create\"\\r\\n                        title=\"Create Flashcards\"\\r\\n                    >\\r\\n                        <span>Create Flashcards</span>\\r\\n                    </a>\\r\\n                </div>\\r\\n                <div>\\r\\n                    <p>Discover</p>\\r\\n                    <ul>\\r\\n                        <li><a id=\"linkFooterCreateFlashcards\" title=\"Create Flashcards\" href=\"/flashcards/create\">Create Flashcards</a></li>\\r\\n                        <li><a id=\"linkFooterMobileApps\" title=\"Mobile Apps\" href=\"/flashcards/apps\">Mobile Apps</a></li>\\r\\n                    </ul>\\r\\n                </div>\\r\\n                <div>\\r\\n                    <p>Company</p>\\r\\n                    <ul>\\r\\n                        <li><a id=\"linkFooterAbout\" title=\"About Cram\" href=\"/about\">About</a></li>\\r\\n                        <li><a id=\"linkFooterFAQ\" title=\"FAQ\" href=\"/docs/help\">FAQ</a></li>\\r\\n                        <li><a id=\"linkFooterSupport\" title=\"Support\" href=\"/support\">Support</a></li>\\r\\n                        <li><a id=\"linkLegal\" title=\"Legal\" href=\"/legal\">Legal</a></li>\\r\\n                        <li><a id=\"linkFooterAccessibility\" title=\"Accessibility\" rel=\"nofollow\" href=\"https://www.bned.com/accessibility/\">Accessibility</a></li>\\r\\n                    </ul>\\r\\n                </div>\\r\\n                <div class=\"social-panel\">\\r\\n                    <p>Follow</p>\\r\\n                    <ul>\\r\\n                        <li><a id=\"linkFooterFacebook\" class=\"facebook-like\" target=\"_blank\" title=\"Facebook\" href=\"https://www.facebook.com/CramMode\">Facebook</a></li>\\r\\n                        <li><a id=\"linkFooterTwitter\" class=\"twitter\" target=\"_blank\" title=\"Twitter\" href=\"https://twitter.com/flashcards\">Twitter</a></li>\\r\\n                    </ul>\\r\\n                </div>\\r\\n            </div>\\r\\n        \\r\\n        <div class=\"copyright\">\\r\\n            <ul>\\r\\n                <li>\\xc2\\xa92023 Cram.com</li>\\r\\n                <li><a href=\"/about-us/privacy\">Privacy Policy</a></li>\\r\\n                                    <li><a href=\"/about-us/privacy#advertising\">About Ads</a></li>\\r\\n                                <li><a id=\"linkFooterSiteMap\" title=\"Site Map\" href=\"/sitemap\">Site Map</a></li>\\r\\n                                    <li><a id=\"linkFooterAdvertise\" title=\"Advertise\" href=\"/advertise-with-us\">Advertise</a></li>\\r\\n                                <li tabindex=\"0\" class=\"ot-sdk-show-settings ot-li\"><a id=\"ot-sdk-btn\">Cookie Settings</a></li>\\r\\n                <script src=\"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\"  type=\"text/javascript\" charset=\"UTF-8\" data-domain-script=\\'a595ba2e-3318-43cc-9a20-698cffbb2cc8\\' ></script>\\r\\n            </ul>\\r\\n        </div>\\r\\n    </div>\\r\\n</footer>\\r\\n\\r\\n        </div>\\n                <!-- Quantcast Tag -->\\n        <script type=\"text/javascript\">\\n        var _qevents = _qevents || [];\\n        (function() {\\n        var elem = document.createElement(\\'script\\');\\n        elem.src =\\n        (document.location.protocol ==\\n        \"https:\" ? \"https://secure\" : \"http://edge\")\\n        + \".quantserve.com/quant.js\";\\n        elem.async = true;\\n        elem.type = \"text/javascript\";\\n        var scpt = document.getElementsByTagName(\\'script\\')[0];\\n        scpt.parentNode.insertBefore(elem, scpt);\\n        })();\\n\\n        _qevents.push({qacct:\"p-640Vrv-0V_6l-\"});\\n        </script>\\n        <noscript>\\n        <img src=\"//pixel.quantserve.com/pixel/p-640Vrv-0V_6l-.gif\"\\n            border=\"0\" height=\"1\" width=\"1\" alt=\"Quantcast\"/>\\n        </noscript>\\n        <!-- End Quantcast tag -->\\n<script type=\"text/javascript\">var _gaq = _gaq || [];</script><div id=\"js-tpl-var-container\" style=\"display:none;\">{\"cdnAssetsUrl\":\"\",\"facebook\":{\"clientId\":\"363499237066029\",\"version\":\"v12.0\",\"language\":\"en_US\"}}</div>\\n\\n<script type=\"text/javascript\" src=\"/assets/whiteboard/scripts/LAB.min.js\"></script><script type=\"text/javascript\">\\n    window.SM2_DEFER = true;\\n    function downloadJSAtOnload() {\\n            $LAB.script(\\'//ajax.googleapis.com/ajax/libs/jquery/3.6.3/jquery.min.js\\').wait()\\n                .script(\\'//ajax.googleapis.com/ajax/libs/jqueryui/1.13.2/jquery-ui.min.js\\').wait()\\n                    .script(\"/fce/js/_sets/2208406965.js\")\\n                    }\\n    // Check for browser support of event handling capability\\n    if (window.addEventListener)\\n       window.addEventListener(\"load\", downloadJSAtOnload, false);\\n    else if (window.attachEvent)\\n       window.attachEvent(\"onload\", downloadJSAtOnload);\\n    else window.onload = downloadJSAtOnload;\\n</script>        <!-- global JS -->\\n    <script type=\"text/javascript\" src=\"//beckett.cram.com/1.17/js/scripts.1.17.109.min.js\"></script>\\n\\n    </body>\\n</html>\\n'\n",
      "[<p>Please sign in to share these flashcards. We'll bring you back here when you are done.</p>, <p>\n",
      "<a class=\"cornflowerblue-btn small-btn round-btn btn thick-btn\" href=\"javascript:void(0);\" id=\"signInBtn\">\n",
      "<span>Sign in</span>\n",
      "</a>\n",
      "</p>, <p class=\"footer\">Don't have an account? <a href=\"javascript:void(0);\" id=\"signUpModalBtn\">Sign Up »</a></p>, <p>If this set belongs to you, please sign in.</p>, <p>\n",
      "<a class=\"btn medium-btn thick-btn cornflowerblue-btn round-btn\" href=\"/user/login\" title=\"Login\">\n",
      "                Sign in              </a>\n",
      "</p>, <p>Ready To Get Started?</p>, <p>Discover</p>, <p>Company</p>, <p>Follow</p>]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.gcertificationcourse.com/linkedin-python-skill-quiz-answers/'\n",
    "url = \"https://www.cram.com/flashcards/git-12843452\"\n",
    "\n",
    "# Making a get request\n",
    "response_code = requests.get(url)\n",
    "  \n",
    "print(response_code.content)\n",
    "\n",
    "soup = BeautifulSoup(response_code.content, 'html.parser')\n",
    "page = soup.find_all('p')\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa492c",
   "metadata": {},
   "source": [
    "# Enregister des fichier plus vite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d5b0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "with open(\"Q_Python/questions.txt\", 'r') as reader:\n",
    "    out.append(reader.read())\n",
    "out = out[0].split('\\n\\n\\n')\n",
    "out = [i for i in out if any(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f27e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "with open(\"questions.txt\", 'r') as reader:\n",
    "    out.append(reader.read())\n",
    "out = out[0].split('\\n\\n\\n')\n",
    "out = [i for i in out if any(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40e3f2c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your manager has asked you to create a binary classification model to predict whether a person has a disease. You need to detect possible classification errors. Which error type should you choose for the following description? “A person has a disease. The model classifies the case as having a disease”.\n",
      "False positives\n",
      "True negatives\n",
      "True positives\n",
      "False negatives\n",
      "True positives\n",
      "Your manager has asked you to create a binary classification model to predict whether a person has a disease. You need to detect possible classification errors. Which error type should you choose for the following description? “A person has a disease. The model classifies the case as having no disease”.\n",
      "True negatives\n",
      "False negatives\n",
      "False positives\n",
      "True positives\n",
      "False negatives\n",
      "You are a senior data scientist in the company and you are tasked with evaluating a completed binary classification machine learning model. You need to use the precision as the evaluation metric. Which visualization should you use?\n",
      "Gradient descent\n",
      "Scatter plot\n",
      "Violin plot\n",
      "Receiver Operating Characteristic (ROC) curve\n",
      "Receiver Operating Characteristic (ROC) curve\n",
      "4. You are a data scientist of a company and you are tasked with building a deep convolutional neural network (CNN) for image classification. The CNN model you built shows signs of overfitting. You need to reduce overfitting and converge the model to an optimal fit.\n",
      "Which two actions should you perform?\n",
      "(accroché) Add L1/L2 regularization\n",
      "Reduce the amount of training data\n",
      "Add an additional dense layer with 64 input units\n",
      "(accroché - option3) Add an additional dense layer with 512 input units\n",
      "(accroché) Use training data augmentation\n",
      "Correct. Adding more training records should decrease the overfitting.\n",
      "5. Your manager has provided you a dataset created for multiclass classification tasks that contains a normalized numerical feature set with 10,000 data points and 150 features. You use 75 percent of the data points for training and 25 percent for testing.\n",
      "Name\n",
      "Description\n",
      "X_train\n",
      "Training feature set\n",
      "Y_train\n",
      "Training class labels\n",
      "x_train\n",
      "Testing feature set\n",
      "y_train\n",
      "Testing class labels\n",
      "You need to apply the Principal Component Analysis (PCA) method to reduce the dimensionality of the feature set to 10 features in both training and testing sets.\n",
      "You are using the scikit-learn machine learning library in Python.\n",
      "You use X to denote the feature set and Y to denote class labels.\n",
      "You create the following Python data frames:\n",
      "From sklearn.decomposition import PCA\n",
      "pca – [...]\n",
      "x_train=[...] .fit_transform(X_train)\n",
      "x_test = pca.[...]\n",
      "How should you complete the code segment?\n",
      "1 / 1 point\n",
      "Box1: PCA(n_components=150);\n",
      "Box2: pca;\n",
      "Box3: x_test\n",
      "(accroché) Box1: PCA(n_components=10);\n",
      "Box2: pca;\n",
      "Box3: transform(x_test)\n",
      "Box1: PCA(n_components=10);\n",
      "Box2: model;\n",
      "Box3: transform(x_test)\n",
      "Box1: PCA(n_components=10000);\n",
      "Box2: pca;\n",
      "Box3: X_train\n",
      "Correct\n",
      "Box 1: PCA(n_components = 10) Need to reduce the dimensionality of the feature set to 10 features in both training and testing sets. Box 2: pca fit_transform(X )fits the model with X and apply the dimensionality reduction on X. Box 3: transform(x_test) transform(X) applies dimensionality reduction to X.\n",
      "6. You are creating a model to predict the price of a student’s artwork depending on the following variables: the student’s length of education, degree type, and art form.\n",
      "You start by creating a linear regression model. You need to evaluate the linear regression model.\n",
      "Solution: Use the following metrics: Relative Squared Error, Coefficient of Determination, Accuracy, Precision, Recall, F1 score, and AUC: Does the solution meet the goal?\n",
      "Yes\n",
      "(accroché) No\n",
      "7. What happens when a list is multiplied by 5?\n",
      "(accroché) The new list created has the length 5 times the original length with the sequence repeated 5 times.\n",
      "The new list remains the same size, but the elements are multiplied by 5.\n",
      "The new list created has the length 5 times the original length with the sequence repeated 5 times and also all the elements are also multiplied by 5.\n",
      "You are creating a model and you want to evaluate it. One metric yields an absolute metric in the same unit as the label. Which metric is described?\n",
      "Coefficient of Determination (known as R-squared or R2)\n",
      "Root Mean Square Error (RMSE)\n",
      "Mean Square Error (MSE)\n",
      "Nothing\n",
      "Root Mean Square Error (RMSE)\n",
      "It is well known that Python provides extensive functionality with powerful and statistical numerical libraries. What is Scikit-learn useful for?\n",
      "Providing attractive data visualizations\n",
      "Offering simple and effective predictive data analysis\n",
      "Analyzing and manipulating data\n",
      "Supplying machine learning and deep learning capabilities\n",
      "Offering simple and effective predictive data analysis\n",
      "What data values are influencing prediction models?\n",
      "Identifiers\n",
      "Features\n",
      "Dependent variables\n",
      "Labels\n",
      "Features\n",
      "2. Let’s suppose you want to create an AI system that can predict how many minutes late a flight will arrive based on the amount of snowfall at an airport. Which machine learning type should you use?\n",
      "Clustering\n",
      "(accroché) Regression\n",
      "Classification\n",
      "3. Imagine you work for a government institution that wants to predict the sea level in meters for the following 10 years. Which type of machine learning should you use?\n",
      "(accroché) Regression\n",
      "Correct. Regression is a supervised machine learning technique used to predict numeric values.\n",
      "Classification\n",
      "Clustering\n",
      "4. Let’s suppose you are working on an AI application that should predict the weather. From the dataset you have, you want to pick temperature and pressure to train the model. Which machine learning task enables you to do that?\n",
      "Feature engineering\n",
      "Model training\n",
      "(accroché) Feature selection\n",
      "Correct. Feature selection is the process of selecting a subset of relevant, useful features to use in building an analytical model.\n",
      "5. True or False?\n",
      "Azure Machine Learning designer supports custom JavaScript functions.\n",
      "True\n",
      "(accroché) False\n",
      "Correct\n",
      "Azure Machine Learning designer does not support custom JavaScript functions.\n",
      "6. Predicting whether someone uses a bicycle to travel to work based on the distance from home to work is a use case for?\n",
      "Regression\n",
      "(accroché) Classification\n",
      "Clustering\n",
      "Correct\n",
      "Classification is a supervised machine learning technique used to predict categories or classes.\n",
      "7. Which of the following metrics is used to evaluate a classification model?\n",
      "Coefficient of determination (R2)\n",
      "(accroché) True positive rate\n",
      "Correct. The best metric to evaluate a classification model is by looking at the true positive rate.\n",
      "Root mean squared error (RMSE)\n",
      "Mean absolute error (MAE)\n",
      "8. True or False?\n",
      "Automated machine learning can automatically infer the training data from the use case provided.\n",
      "True\n",
      "(accroché) False\n",
      "Correct. Automated machine learning cannot automatically infer the training data from the use case provided.\n",
      "9. True or False?\n",
      "Azure Machine Learning designer provides a drag-and-drop visual canvas to build, test, and deploy machine learning models.\n",
      "(accroché) True\n",
      "Correct. Azure Machine Learning provides a drag-and-drop visual canvas to build, test, and deploy machine learning models.\n",
      "False\n",
      "10. You want to create a CRM application that uses AI to segment customers into different groups to support a marketing department. Which machine learning type should you use?\n",
      "(accroché) Clustering\n",
      "Classification\n",
      "Regression\n",
      "Correct. Clustering is an unsupervised machine learning technique used to group similar entities based on their features.\n",
      "1. You create a new Azure subscription. No resources are provisioned in the subscription. You need to create an Azure Machine Learning workspace.\n",
      "What are three possible ways to achieve this goal? Each correct answer presents a complete solution.\n",
      "(accroché) Run Python code that uses the Azure ML SDK library and calls the Workspace.create method with name, subscription_id, resource_group, and location parameters.\n",
      "Correct. This is one way to achieve the goal.\n",
      "(accroché) Use the Azure Command Line Interface (CLI) with the Azure Machine Learning extension to call the az group create function with –name and –location parameters, and then the az ml workspace create function, specifying Cw and Cg parameters for the workspace name and resource group.\n",
      "Correct. This is one way to achieve the goal.\n",
      "Run Python code that uses the Azure ML SDK library and calls the Workspace.get method with name, subscription_id, and resource_group parameters.\n",
      "(Incorrect) Navigate to Azure Machine Learning studio and create a workspace.\n",
      "(accroché - essaie ça) Use an Azure Resource Management template that includes a Microsoft.MachineLearningServices/ workspaces resource and its dependencies.\n",
      "1. You create an Azure Machine Learning workspace. You are preparing a local Python environment on a laptop computer.\n",
      "You want to use the laptop to connect to the workspace and run experiments.\n",
      "You create the following config.json file:\n",
      " { \"workspace_name\" : \"ml-workspace\" }\n",
      "You must use the Azure Machine Learning SDK to interact with data and experiments in the workspace. You need to configure the config.json file to connect to the workspace from the Python environment. Which two additional parameters must you add to the config.json file in order to connect to the workspace? Each correct answer presents part of the solution.\n",
      "Key\n",
      "(accroché) Resource_group\n",
      "Correct. This parameter must be specified.\n",
      "Region\n",
      "Login\n",
      "(accroché) Subscription_id\n",
      "Correct. This parameter must be specified.\n",
      "2. An organization uses Azure Machine Learning service and wants to expand their use of machine learning. You have the following compute environments. The organization does not want to create another compute environment.\n",
      "Environment name, Compute Type\n",
      "nb_server, Compute instance\n",
      "aks_cluster, Azure Kubernetes Service\n",
      "mlc_cluster, Machine Learning compute\n",
      "You need to determine which compute environment to use for the following scenarios:\n",
      "1. Run an Azure Machine Learning Designer training pipeline.\n",
      "2. Deploying a web service from the Azure Machine Learning Designer.\n",
      "Which compute types should you use?\n",
      "(accroché) 1 mlc_cluster, 2 aks_cluster\n",
      "1 nb_server, 2 aks_cluster\n",
      "1 mlc_cluster, 2 nb_server\n",
      "1 nb_server, 2 mlc_cluster\n",
      "2. You are developing a data science workspace that uses an Azure Machine Learning service. You need to select a compute target to deploy the workspace. What should you use?\n",
      "Azure Data Lake Analytics\n",
      "(accroché) Azure Container Instances\n",
      "Correct. Azure Container Instances can be used a compute target for testing or developement. Use for low-scale CPU-based workloads that require less than 48 GB of RAM.\n",
      "Azure Databricks\n",
      "Apache Spark for HDInsight\n",
      "3. The finance team asked you to train a model using data in an Azure Storage blob container named finance-data.\n",
      "You need to register the container as a datastore in an Azure Machine Learning workspace and ensure that an error will be raised if the container does not exist.\n",
      "How should you complete the code?\n",
      "Datastore = Datastore.<add answer here> (workspace = ws,\n",
      "datastore_name = ‘finance_datastore’,\n",
      "container_name = ‘finance-data’,\n",
      "account_name = ‘fintrainingdatastorage’,\n",
      "account_key = ‘FdhIWHDaiwh2…’\n",
      "<add answer here>\n",
      "1 / 1 point\n",
      "register_azure_data_lake, create_if_not_exists = False\n",
      "register_azure_blob_container, overwrite = True\n",
      "(accroché) register_azure_blob_container, create_if_not_exists = False\n",
      "register_azure_data_lake, overwrite = False\n",
      "Correct\n",
      "register_azure_blob_container to Register an Azure Blob Container to the datastore and create_if_not_exists = False to create the file share if it does not exist, defaults to False.\n",
      "3. A coworker registers a datastore in a Machine Learning services workspace by using the following code:\n",
      "Datastore.register_azure_blob_container(workspace=ws,\n",
      "datastore_name=‘demo_datastore’,\n",
      "container_name=‘demo_datacontainer’,\n",
      "account_name=’demo_account’,\n",
      "account_key=’0A0A0A-0A00A0A-0A0A0A0A0A0’\n",
      "create_if_not_exists=True)\n",
      "You need to write code to access the datastore from a notebook. How should you complete the code segment?\n",
      "import azureml.core\n",
      "from azureml.core import Workspace, Datastore\n",
      "ws = Workspace.from_config()\n",
      "datastore = <add answer here> .get( <add answer here>, ‘<add answer here>’)\n",
      "Run, experiment, demo_datastore\n",
      "(accroché) DataStore, ws, demo_datastore\n",
      "Experiment, run, demo_account\n",
      "Run, ws, demo_datastore\n",
      "Correct. To get a specific datastore registered in the current workspace, use the get() static method on the Datastore class, like this:\n",
      "datastore = Datastore.get(ws, datastore_name='your datastore name')\n",
      "A set of CSV files contains sales records. All the CSV files have the same data schema.\n",
      "Each CSV file contains the sales record for a particular month and has the filename sales.csv. Each file is stored in a folder that indicates the month and year when the data was recorded. The folders are in an Azure blob container for which a datastore has been defined in an Azure Machine Learning workspace. The folders are organized in a parent folder named sales to create the following hierarchical structure:\n",
      "/sales\n",
      "  /01-2019\n",
      "    /sales.csv\n",
      "  /02-2019\n",
      "    /sales.csv\n",
      "  /03-2019\n",
      "    /sales.csv\n",
      "…\n",
      "At the end of each month, a new folder with that month's sales file is added to the sales folder. You plan to use the sales data to train a machine learning model based on the following requirements:\n",
      "- You must define a dataset that loads all of the sales data to date into a structure that can be easily converted to a dataframe.\n",
      "- You must be able to create experiments that use only data that was created before a specific previous month, ignoring any data that was added after that month.\n",
      "- You must register the minimum number of datasets possible.\n",
      "You need to register the sales data as a dataset in Azure Machine Learning service workspace. What should you do?\n",
      "0 / 1 point\n",
      "(Incorrect) Create a new tabular dataset that references the datastore and explicitly specifies each 'sales/mm-yyyy/sales.csv' file every month. Register the dataset with the name sales_dataset_MM-YYYY each month with appropriate MM and YYYY values for the month and year. Use the appropriate month-specific dataset for experiments.\n",
      "Create a tabular dataset that references the datastore and explicitly specifies each 'sales/mm-yyyy/sales.csv' file every month. Register the dataset with the name sales_dataset each month, replacing the existing dataset and specifying a tag named month indicating the month and year it was registered. Use this dataset for all experiments.\n",
      "Create a tabular dataset that references the datastore and specifies the path 'sales/*/sales.csv', register the dataset with the name sales_dataset and a tag named month indicating the month and year it was registered, and use this dataset for all experiments.\n",
      "(accroché) Create a tabular dataset that references the datastore and explicitly specifies each 'sales/mm-yyyy/sales.csv' file. Register the dataset with the name sales_dataset each month as a new version and with a tag named month indicating the month and year it was registered. Use this dataset for all experiments, identifying the version to be used based on the month tag as necessary.\n",
      "4. You are a lead data scientist for a project that tracks the health and migration of birds. You create a multi-class image classification deep learning model that uses a set of labeled bird photographs collected by experts.\n",
      "You have 100,000 photographs of birds. All photographs use the JPG format and are stored in an Azure blob container in an Azure subscription. You need to access the bird photograph files in the Azure blob container from the Azure Machine Learning service workspace that will be used for deep learning model training.\n",
      "You must minimize data movement. What should you do?\n",
      "Create an Azure Cosmos DB database and attach the Azure Blob containing bird photographs storage to the database.\n",
      "Create and register a dataset by using TabularDataset class that references the Azure blob storage containing bird photographs.\n",
      "Copy the bird photographs to the blob datastore that was created with your Azure Machine Learning service workspace.\n",
      "Create an Azure Data Lake store and move the bird photographs to the store.\n",
      "(accroché) Register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service.\n",
      "5. You train a machine learning model. You must deploy the model as a real-time inference service for testing. The service requires low CPU utilization and less than 48 MB of RAM. The compute target for the deployed service must initialize automatically while minimizing cost and administrative overhead. Which compute target should you use?\n",
      "(accroché) Azure Container Instance (ACI)\n",
      "attached Azure Databricks cluster\n",
      "Azure Machine Learning compute cluster\n",
      "Azure Kubernetes Service (AKS) inference cluster\n",
      "5. You create a deep learning model for image recognition on Azure Machine Learning service using GPU-based training. You must deploy the model to a context that allows for real-time GPU-based inferencing. You need to configure compute resources for model inferencing. Which compute type should you use?\n",
      "Machine Learning Compute\n",
      "Azure Container Instance\n",
      "Field Programmable Gate Array\n",
      "(accroché) Azure Kubernetes Service\n",
      "6. An organization creates and deploys a multi-class image classification deep learning model that uses a set of labeled photographs.\n",
      "The software engineering team reports there is a heavy inferencing load for the prediction web services during the summer. The production web service for the model fails to meet demand despite having a fully-utilized compute cluster where the web service is deployed.\n",
      "You need to improve performance of the image classification web service with minimal downtime and minimal administrative effort. What should you advise the IT Operations team to do?\n",
      "Create a new compute cluster by using larger VM sizes for the nodes, redeploy the web service to that cluster, and update the DNS registration for the service endpoint to point to the new cluster.\n",
      "Increase the minimum node count of the compute cluster where the web service is deployed.\n",
      "Increase the VM size of nodes in the compute cluster where the web service is deployed.\n",
      "(accroché) Increase the node count of the compute cluster where the web service is deployed.\n",
      "Correct. The Azure Machine Learning SDK does not provide support scaling an AKS cluster. To scale the nodes in the cluster, use the UI for your AKS cluster in the Azure Machine Learning studio. You can only change the node count (le nombre des VMs), not the VM size of the cluster.\n",
      "6. You use Azure Machine Learning designer to create a real-time service endpoint. You have a single Azure Machine Learning service compute resource. You train the model and prepare the real-time pipeline for deployment. You need to publish the inference pipeline as a web service. Which compute type should you use?\n",
      "(Incorrect) HDInsight\n",
      "Azure Databricks\n",
      "(Incorrect) A new Machine Learning Compute resource\n",
      "(Incorrect) The existing Machine Learning Compute resource\n",
      "(accroché) Azure Kubernetes Services\n",
      "7. You use the Azure Machine Learning Python SDK to define a pipeline that consists of multiple steps. When you run the pipeline, you observe that some steps do not run. The cached output from a previous run is used instead. You need to ensure that every step in the pipeline is run, even if the parameters and contents of the source directory have not changed since the previous run. What are two possible ways to achieve this goal? Each correct answer presents a complete solution. Set the outputs property of each step in the pipeline to True.\n",
      "Set the allow_reuse property of each step in the pipeline to False.\n",
      "Restart the compute cluster where the pipeline experiment is configured to run.\n",
      "Set the regenerate_outputs property of the pipeline to True.\n",
      "Use a PipelineData object that references a datastore other than the default datastore.\n",
      "Set the regenerate_outputs property of the pipeline to True. Set the allow_reuse property of each step in the pipeline to False.\n",
      "7. You deploy a model as an Azure Machine Learning real-time web service using the following code.\n",
      "# ws, model, inference_config, and deployment_config defined previously\n",
      "service = Model.deploy(ws, ‘classification-service’, [model], inference_config, deployment_config)\n",
      "service.wait_for_deployment(True)\n",
      "The deployment fails.\n",
      "You need to troubleshoot the deployment failure by determining the actions that were performed during deployment and identifying the specific action that failed.\n",
      "Which code segment should you run?\n",
      "(accroché) service.get_logs()\n",
      "Correct. You can print out detailed Docker engine log messages from the service object. You can view the log for ACI, AKS, and Local deployments.\n",
      "service.serialize()\n",
      "service.update_deployment_state()\n",
      "service.state\n",
      "8. You register a model that you plan to use in a batch inference pipeline.\n",
      "The batch inference pipeline must use a ParallelRunStep step to process files in a file dataset. The script has the ParallelRunStep step and the runs must process six input files each time the inferencing function is called.\n",
      "You need to configure the pipeline. Which configuration setting should you specify in the ParallelRunConfig object for the ParallelRunStep step?\n",
      "error_threshold= \"6\"\n",
      "node_count= \"6\"\n",
      "process_count_per_node= \"6\"\n",
      "(accroché) mini_batch_size= \"6\"\n",
      "8. You train and register a model in your Azure Machine Learning workspace.\n",
      "You must publish a pipeline that enables client applications to use the model for batch inferencing.\n",
      "You must use a pipeline with a single ParallelRunStep step that runs a Python inferencing script to get predictions from the input data.\n",
      "You need to create the inferencing script for the ParallelRunStep pipeline step.\n",
      "Which two functions should you include? Each correct answer presents part of the solution.\n",
      "main()\n",
      "score(mini_batch)\n",
      "(accroché) run(mini_batch)\n",
      "(accroché) init()\n",
      "batch()\n",
      "9. Yes or No?\n",
      "You must be able to explain the model's predictions by calculating the importance of each feature, both as an overall global relative importance value and as a measure of local importance for a specific set of predictions.\n",
      "You need to create an explainer that you can use to retrieve the required global and local feature importance values.\n",
      "Solution: Create a PFIExplainer. Does the solution meet the goal?\n",
      "Yes\n",
      "(accroché) No\n",
      "Correct. The PFIExplainer doesn't support local feature importance explanations.\n",
      "10. You create an Azure Machine Learning compute resource to train models. The compute resource is configured as follows: - Minimum nodes: 2 - Maximum nodes: 4. You must decrease the minimum number of nodes and increase the maximum number of nodes to the following values: - Minimum nodes: 0 - Maximum nodes: 8\n",
      "You need to reconfigure the compute resource. What are three possible ways to achieve this goal? Each correct answer presents a complete solution.\n",
      "Use the Azure portal.\n",
      "(quelque fois correct, quelque fois incorrect) Run the refresh_state() method of the BatchCompute class in the Python SDK.\n",
      "(accroché) Run the update method of the AmlCompute class in the Python SDK.\n",
      "Correct. The update(min_nodes=None, max_nodes=None, idle_seconds_before_scaledown=None) of the AmlCompute class updates the ScaleSettings for this AmlCompute target.\n",
      "Use the Azure Machine Learning designer.\n",
      "(accroché) Use the Azure Machine Learning studio.\n",
      "Correct. You can manage assets and resources in the Azure Machine Learning studio.\n",
      "9. Yes or No?\n",
      "You train a classification model by using a logistic regression algorithm. You must be able to explain the model's predictions by calculating the importance of each feature, both as an overall global relative importance value and as a measure of local importance for a specific set of predictions.\n",
      "You need to create an explainer that you can use to retrieve the required global and local feature importance values.\n",
      "Solution: Create a TabularExplainer. Does the solution meet the goal?\n",
      "1 / 1 point\n",
      "(accroché) Yes\n",
      "Correct. The TabularExplainer supports both global and local feature importance explanations.\n",
      "No\n",
      "10. You deploy a real-time inference service for a trained model. The deployed model supports a business-critical application, and it is important to be able to monitor the data submitted to the web service and the predictions the data generates. You need to implement a monitoring solution for the deployed model using minimal administrative effort. What should you do?\n",
      "Enable Azure Application Insights for the service endpoint and view logged data in the Azure portal.\n",
      "Create an ML Flow tracking URI that references the endpoint, and view the data logged by ML Flow.\n",
      "View the log files generated by the experiment used to train the model.\n",
      "View the explanations for the registered model in Azure ML studio.\n",
      "Enable Azure Application Insights for the service endpoint and view logged data in the Azure portal.\n",
      "1. You have an AirBnB housing dataframe which you preprocessed and filtered down to only the relevant columns.\n",
      "The columns are: id, host_name, bedrooms, neighbourhood_cleansed, price.\n",
      "You’ve written the function below name firstInitialFunction that returns the first initial from the host_name column:\n",
      "def firstInitialFunction(name):\n",
      "    return name[0]\n",
      "firstInitialFunction(\"George\")\n",
      "Because Python UDFs are much slower than Scala UDFs, you now want to create a Vectorized UDF in Python to speed up the computation.\n",
      "How would you code that?\n",
      "from pyspark.sql.functions import pandas_udf\n",
      "# We have a string input/output\n",
      "@pandas_udf(\"string\")\n",
      "create vectorizedUDF(name):\n",
      "return name.str[0]\n",
      "from pyspark.sql.functions import pandas_udf\n",
      "@pandas_udf(\"string\")\n",
      "def vectorizedUDF(host_name):\n",
      "get name.str[0]\n",
      "from pyspark.sql.functions import pandas_udf\n",
      "@pandas_udf(\"int\")\n",
      "def vectorizedUDF(name):\n",
      "return name.str[0]\n",
      "(accroché) from pyspark.sql.functions import pandas_udf\n",
      "@pandas_udf(\"string\")\n",
      "def vectorizedUDF(name):\n",
      "return name.str[0]\n",
      "# Correct. This is the correct code for the task. We used string as an argument because we have a string input/output.\n",
      "You have a Boston Housing dataset where you find a median value for a number variables such as the number of rooms, per capita crime and economic status of residents. You want to use Linear Regression to predict the median home value based on the average number of rooms. You’ve imported the dataset and created a column named features that has a single input variable named rm by using VectorAssembler. You now want to fit the Liner Regression model. How should you code that?\n",
      "(accroché) from pyspark.ml.regression import LinearRegression; lr = LinearRegression(featuresCol=\"features\", labelCol=\"medv\"); lrModel = lr.fit(bostonFeaturizedDF)\n",
      "from pyspark import LinearRegression; lr = LinearRegression(featuresCol=\"features\", labelCol=\"medv\")\n",
      "lrModel = lr.fit(bostonFeaturizedDF)\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "lr = LinearRegression(featuresCol=\"rm\", labelCol=\"medv\")\n",
      "lrModel = lr_fit(bostonFeaturizedDF)\n",
      "from pyspark.ml import LinearRegression\n",
      "lr = LinearRegression(featuresCol=\"rm \", labelCol=\"medv\")\n",
      "lrModel = lr_fit(bostonFeaturizedDF)\n",
      "3. You are using MLflow to track the runs of a Linear Regression model of an AirBnB dataset.\n",
      "You want to use all the features in the dataset.\n",
      "You’ve created the pipeline, logged the pipeline, and logged the parameters.\n",
      "Now you need to create predictions and metrics.\n",
      "How should you code that?\n",
      "predDF = pipelineModel.evaluate(testDF)\n",
      "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "(accroché) predDF = pipelineModel.transform(testDF)\n",
      "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "predDF = pipelineModel.transform(testDF)\n",
      "regression = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "predDF = pipelineModel.estimate(testDF)\n",
      "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "4. You are running Python code interactively in a Conda environment. The environment includes all required Azure Machine Learning SDK and MLflow packages.\n",
      "You must use MLflow to log metrics in an Azure Machine Learning experiment named mlflow-experiment.\n",
      "To answer, replace the bolded comments in the code with the appropriate code options in the answer area.\n",
      "How should you complete the code?\n",
      "import mlflow\n",
      "from azureml.core import Workspace\n",
      "ws = Workspace.from_config()\n",
      "#1 Set the MLflow logging target\n",
      "#2 Configure the experiment\n",
      "with #3 Begin the experiment run\n",
      "               #4 Log my_metric with value 1.00 (‘my_metric’, 1.00)\n",
      "print(“Finished!”)\n",
      "#1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #2 mlflow.get_run('mlflow-experiment), #3 mlflow.start_run(), #4 run.log()\n",
      "(accroché) #1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #2 mlflow.set_experiment('mlflow-experiment), #3 mlflow.start_run(), #4 mlflow.log_metric\n",
      "Correct. #1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()) In the following code, the get_mlflow_tracking_uri() method assigns a unique tracking URI address to the workspace, ws, and set_tracking_uri() points the MLflow tracking URI to that address.\n",
      "#2 mlflow.set_experiment(experiment_name) Set the MLflow experiment name with set_experiment() and start your training run with start_run().\n",
      "#3 mlflow.start_run()\n",
      "#4 mlflow.log_metric - Then use log_metric() to activate the MLflow logging API and begin logging your training run metrics.\n",
      "#1 mlflow.tracking.client = ws, #2 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #3 mlflow.active_run(), #4 mlflow.log_metric\n",
      "#1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #2 mlflow.get_run('mlflow-experiment), #3 mlflow.start_run(), #4 mlflow.log_metric\n",
      "5. You are evaluating a Python NumPy array that contains six data points defined as follows: data = [10, 20, 30, 40, 50, 60]\n",
      "You must generate the following output by using the k-fold algorithm implementation in the Python Scikit-learn machine learning library: train: [10 40 50 60], test: [20 30] train: [20 30 40 60], test: [10 50] train: [10 20 30 50], test: [40 60]\n",
      "You need to implement a cross-validation to generate the output.\n",
      "To answer, replace the bolded comments in the code with the appropriate code options in the answer area.\n",
      "How should you complete the code?\n",
      "from numpy import array\n",
      "from sklearn.model_selection import #1st option\n",
      "data – array ([10, 20, 30, 40, 50, 60])\n",
      "kfold – Kfold(n_splits- #2nd option, shuffle – True – random_state-1)\n",
      "for train, test in kFold, split( #3rd option):\n",
      "print (‘train’: %s, test: %5’ % (data[train], data[test])\n",
      "K-fold, 3, array\n",
      "CrossValidation, 3, data\n",
      "(accroché) K-fold, 3, data\n",
      "K-means, 6, array\n",
      "You use the following code to run a script as an experiment in Azure Machine Learning: from azureml.core import Workspace, Experiment, Run; from azureml.core import RunConfig, ScriptRunConfig; ws = Workspace.from_config(); run_config = RunConfiguration(); run_config.target=’local’; script_config = ScriptRunConfig(source_directory=’./script’,; script=’experiment.py’, run_config=run_config); experiment = Experiment(workspace=ws, name=’script experiment’); run = experiment.submit(config=script_config); run.wait_for_completion(). Which code do you need to add to retrieve the output file names?\n",
      "files = run.get_fine_names()\n",
      "run.get_details_with_logs()\n",
      "files = run.get_properties()\n",
      "files = run.get_metrics()\n",
      "run.get_details_with_logs()\n",
      "Your task is to predict if a person suffers from a disease by setting up a binary classification model. Your solution needs to be able to detect the classification errors that may appear. Considering the below description, which of the following would be the best error type? “A person does not suffer from a disease. Your model classifies the case as having a disease”.\n",
      "False negatives\n",
      "True positives\n",
      "True negatives\n",
      "False positives\n",
      "False positives\n",
      "Your task is to predict if a person suffers from a disease by setting up a binary classification model. Your solution needs to be able to detect the classification errors that may appear. Considering the below description, which of the following would be the best error type? “A person suffers from a disease. Your model classifies the case as having no disease”.\n",
      "True negatives\n",
      "False negatives\n",
      "True positives\n",
      "False positives\n",
      "False negatives\n",
      "As a senior data scientist, you need to evaluate a binary classification machine learning model. As evaluation metric, you have to use the precision. Considering this, which is the most appropriate visualization?\n",
      "Violin plot\n",
      "Gradient descent\n",
      "Scatter plot\n",
      "Receiver Operating Characteristic (ROC) curve\n",
      "Receiver Operating Characteristic (ROC) curve\n",
      "In order to foretell the price for a student’s craftwork, you have to rely on the following variables: the student’s length of education, degree type, and craft form. You decide to set up a linear regression model that you will have to evaluate. Solution: Apply the following metrics: Relative Squared Error, Coefficient of Determination, Accuracy, Precision, Recall, F1 score, and AUC: Is this solution effective?\n",
      "Yes\n",
      "No\n",
      "None\n",
      "None\n",
      "No\n",
      "What is the result for multiplying a list by 3?\n",
      "The new list created has the length 3 times the original length with the sequence repeated 3 times.\n",
      "The new list created has the length 3 times the original length with the sequence repeated 3 times and also all the elements are also multiplied by 3.\n",
      "The new list remains the same size, but the elements are multiplied by 3.\n",
      "None\n",
      "The new list created has the length 3 times the original length with the sequence repeated 3 times.\n",
      "Python is commonly known to ensure extensive functionality with powerful and statistical numerical libraries. What are the utilities of TensorFlow?\n",
      "Analyzing and manipulating data\n",
      "Supplying machine learning and deep learning capabilities\n",
      "Offering simple and effective predictive data analysis\n",
      "Providing attractive data visualizations\n",
      "Supplying machine learning and deep learning capabilities\n",
      "How should the following sentence be completed? One example of the machine learning […] type models is the Support Vector Machine algorithm.\n",
      "Clustering\n",
      "Regression\n",
      "Classification (Logistic Regression)\n",
      "None\n",
      "Classification (Logistic Regression)\n",
      "Choose from the list below the evaluation model that is described as a relative metric where the higher the value is, the better will be the fit of the model.\n",
      "Coefficient of Determination (known as R-squared or R2)\n",
      "Root Mean Square Error (RMSE)\n",
      "None\n",
      "Mean Square Error (MSE)\n",
      "Coefficient of Determination (known as R-squared or R2)\n",
      "You have a Pandas DataFrame entitled df_sales that contains the sales data from each day. You DataFrame contains these columns: year, month, day_of_month, sales_total. Which of the following codes should you choose if your goal is to return the average sales_total value?\n",
      "df_sales['sales_total'].mean()\n",
      "mean(df_sales['sales_total'])\n",
      "df_sales['sales_total'].avg()\n",
      "Nothing\n",
      "df_sales['sales_total'].mean()\n",
      "You decided to use the LinearRegression class from the scikit-learn library to create your model object. If you want to train the model, what should your next step be?\n",
      "Call the predict() method of the model object, specifying the training feature and label arrays\n",
      "Call the score() method of the model object, specifying the training feature and test feature arrays\n",
      "Nothing\n",
      "Call the fit() method of the model object, specifying the training feature and label arrays\n",
      "Call the fit() method of the model object, specifying the training feature and label arrays\n",
      "9. What is the effect that you obtain if you increase the Learning Rate parameter for the deep neural network that you are creating?\n",
      "1 / 1 point\n",
      "(accroché) Larger adjustments are made to weight values during backpropagation\n",
      "More records are included in each batch passed through the network\n",
      "More hidden layers are added to the network\n",
      "Correct. Increasing the learning rate causes backpropagation to make larger weight adjustments.\n",
      "10. Your task is to reduce the size of the feature maps that a convolutional layer generates when you create a convolutional neural network. What action should you take in this case?\n",
      "1 / 1 point\n",
      "Increase the number of filters in the convolutional layer\n",
      "(accroché) Add a pooling layer after the convolutional layer\n",
      "Reduce the size of the filter kernel used in the convolutional layer\n",
      "Correct. A pooling layer reduces the number of features in a feature map.\n",
      "11. Your task is to train a model entitled finance-data for the financial department, by using data in an Azure Storage blob container.\n",
      "Your container has to be registered in an Azure Machine Learning workspace as a datastore and you have to make sure that an error will appear if the container does not exist.\n",
      "Considering this scenario, what should be the continuation for the code below?\n",
      "Datastore = Datastore.<add answer here> (workspace = ws,\n",
      "datastore_name = ‘finance_datastore’,\n",
      "container_name = ‘finance-data’,\n",
      "account_name = ‘fintrainingdatastorage’,\n",
      "account_key = ‘FdhIWHDaiwh2…’\n",
      "<add answer here>\n",
      "register_azure_data_lake, create_if_not_exists = False\n",
      "register_azure_blob_container, overwrite = True\n",
      "(accroché) register_azure_blob_container, create_if_not_exists = False\n",
      "register_azure_data_lake, overwrite = False\n",
      "Correct\n",
      "register_azure_blob_container to Register an Azure Blob Container to the datastore and create_if_not_exists = False to create the file share if it does not exist, defaults to False.\n",
      "12. You have the role of lead data scientist in a project that keeps record of birds’ health and migration. You decide to use a set of labeled bird photographs collected by experts for your multi-class image classification deep learning model.\n",
      "The entire set of 200,000 birds’ photographs uses the JPG format and is being kept in an Azure blob container from an Azure subscription. You have to be able to ensure access from the Azure Machine Learning service workspace used for deep learning model training directly to the bird photograph files stored in the Azure blob container.\n",
      "You have to keep data movement to a minimum. What action should you take?\n",
      "0 / 1 point\n",
      "(Incorrect) Copy the bird photographs to the blob datastore that was created with your Azure Machine Learning service workspace.\n",
      "(accroché??) Register the Azure blob storage containing the bird photographs as a datastore in Azure Machine Learning service.\n",
      "(Incorrect) Create and register a dataset by using TabularDataset class that references the Azure blob storage containing bird photographs.\n",
      "(Incorrect) Create an Azure Cosmos DB database and attach the Azure Blob containing bird photographs storage to the database.\n",
      "(accroché) Create an Azure Data Lake store and move the bird photographs to the store.\n",
      "13. You decide to use the code below for the deployment of a model as an Azure Machine Learning real-time web service:\n",
      "# ws, model, inference_config, and deployment_config defined previously\n",
      "service = Model.deploy(ws, ‘classification-service’, [model], inference_config, deployment_config)\n",
      "service.wait_for_deployment(True)\n",
      "Your deployment does not succeed.\n",
      "You have to troubleshoot the deployment failure in order to determine what actions were taken while deploying and to identify the one action that encountered a problem and didn’t succeed.\n",
      "For this scenario, which of the following code snippets should you use?\n",
      "(accroché) service.get_logs()\n",
      "service.update_deployment_state()\n",
      "service.serialize()\n",
      "service.state\n",
      "14. You decide to register and train a model in your Azure Machine Learning workspace.\n",
      "Your pipeline needs to ensure that the client applications are able to use the model for batch inferencing.\n",
      "Your single ParallelRunStep step pipeline uses a Python inferencing script in order to obtain predictions from the input data.\n",
      "Your task is to configure the inferencing script for the ParallelRunStep pipeline step.\n",
      "Which are the most suitable two functions that you should use? Keep in mind that every correct answer presents a part of the solution.\n",
      "0 / 1 point\n",
      "(accroché) init()\n",
      "(accroché) run(mini_batch)\n",
      "main()\n",
      "score(mini_batch)\n",
      "batch()\n",
      "Vous n'avez pas sélectionné toutes les bonnes réponses\n",
      "15.\n",
      "Question 15\n",
      "After installing the Azure Machine Learning Python SDK, you decide to use it to configure on your subscription a workspace entitled “aml-workspace”.\n",
      "What code should you write in Python for this task?\n",
      "(accroché) azureml.core import Workspace\n",
      " ws = Workspace.create(name='aml-workspace',\n",
      " subscription_id='123456-abc-123...',\n",
      " resource_group='aml-resources',\n",
      " create_resource_group=False,\n",
      " location='eastus'\n",
      " )\n",
      "from azureml.core import Workspace\n",
      " ws = Workspace.create(name='aml-workspace',\n",
      " subscription_id='123456-abc-123...',\n",
      " resource_group='aml-resources',\n",
      " create_resource_group=True,\n",
      " location='eastus'\n",
      " )\n",
      "from azureml.core import Workspace\n",
      " ws = Workspace.create(name='aml-workspace',\n",
      " subscription_id='123456-abc-123...',\n",
      " resource_group='aml-resources',\n",
      " location='eastus'\n",
      " )\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "16.\n",
      "Question 16\n",
      "If your goal is to use a configuration file in order to ensure connection to your Azure ML workspace, what Python command would be the most appropriate?\n",
      "0 / 1 point\n",
      "from azureml.core import Workspace\n",
      "ws = Workspace.from.config\n",
      "from azureml.core import Workspace\n",
      "ws = from.config_Workspace()\n",
      "from azureml.core import Workspace\n",
      "ws = Workspace.from_config()\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "17.\n",
      "Question 17\n",
      "You decided to use the from_files method of the Dataset.File class to configure a file dataset.\n",
      "You then want to register the file dataset with the title img_files in a workspace.\n",
      "What SDK commands should you choose for this task?\n",
      "0 / 1 point\n",
      "from azureml.core import Dataset\n",
      "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
      "file_ds = file_ds.register(workspace=ws, name='img_files')\n",
      "from azureml.core import Dataset\n",
      "blob_ds = ws.get_default_datastore()\n",
      "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
      "from azureml.core import Dataset\n",
      "blob_ds = ws.get_default_datastore()\n",
      "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images/*.jpg'))\n",
      "file_ds = file_ds.register(workspace=ws, name='img_files')\n",
      "from azureml.core import Dataset\n",
      "blob_ds = ws.get_default_datastore()\n",
      "file_ds = Dataset.File.from_files(path=(blob_ds, 'data/files/images'))\n",
      "file_ds = file_ds.register(workspace=ws, name='img_files')\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "18.\n",
      "Question 18\n",
      "You want to create a pipeline for which you defined three steps entitled as step1, step2, and step3.\n",
      "Your goal is to run the pipeline as an experiment after the steps have been assigned to it.\n",
      "Which of the following SDK command should you choose for this task?\n",
      "0 / 1 point\n",
      "train_pipeline = Pipeline(workspace = ws, steps = [step1;step2;step3])\n",
      "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
      "pipeline_run = experiment.submit(train_pipeline)\n",
      "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2,step3])\n",
      "experiment = Experiment(workspace = ws)\n",
      "pipeline_run = experiment.submit(train_pipeline)\n",
      "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2,step3])\n",
      "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
      "pipeline_run = experiment.submit(train_pipeline)\n",
      "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2,step3])\n",
      "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
      "pipeline_run = experiment_submit(train_pipeline)\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "19. Your task is to deploy your service on an AKS cluster that is set up as a compute target.\n",
      "What SDK commands are able to return you the expected result?\n",
      "0 / 1 point\n",
      "from azureml.core.compute import ComputeTarget, AksCompute\n",
      "cluster_name = 'aks-cluster'\n",
      "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
      "production_cluster = ComputeTarget.deploy (ws, cluster_name, compute_config)\n",
      "production_cluster.wait_for_completion(show_output=True)\n",
      "from azureml.core.webservice import ComputeTarget, AksCompute\n",
      "cluster_name = 'aks-cluster'\n",
      "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
      "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
      "production_cluster.wait_for_completion(show_output=True)\n",
      "from azureml.core.compute import ComputeTarget, AksCompute\n",
      "cluster_name = 'aks-cluster'\n",
      "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
      "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
      "production_cluster.wait_for_completion(show_output=True)\n",
      "from azureml.core.webservice import ComputeTarget, AksWebservice\n",
      "cluster_name = 'aks-cluster'\n",
      "compute_config = AksCompute.provisioning_configuration(location='eastus')\n",
      "production_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
      "production_cluster.wait_for_completion(show_output=True)\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "20. You can combine the Bayesian sampling with an early-termination policy and you can use it only with these three parameter expressions: choice, uniform and quniform.\n",
      "0 / 1 point\n",
      "False\n",
      "True\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "21.\n",
      "Question 21\n",
      "What code should you write for an instance of a MimicExplainer if you have a model entitled loan_model?\n",
      "0 / 1 point\n",
      "from interpret.ext.blackbox import MimicExplainer\n",
      "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
      "mim_explainer = MimicExplainer(model=loan_model,\n",
      " initialization_examples=X_test,\n",
      " explainable_model = DecisionTree,\n",
      " classes=['loan_amount','income','age','marital_status'],\n",
      " features=['reject', 'approve'])\n",
      "from interpret.ext.blackbox import MimicExplainer\n",
      "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
      "mim_explainer = MimicExplainer(model=loan_model,\n",
      " initialization_examples=X_test,\n",
      " explainable_model = DecisionTreeExplainableModel,\n",
      " features=['loan_amount','income','age','marital_status'],\n",
      "from interpret.ext.blackbox import MimicExplainer\n",
      "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
      "mim_explainer = MimicExplainer(model=loan_model,\n",
      " explainable_model = DecisionTreeExplainableModel,\n",
      " classes=['loan_amount','income','age','marital_status'],\n",
      " features=['reject', 'approve'])\n",
      "from interpret.ext.blackbox import MimicExplainer\n",
      "from interpret.ext.glassbox import DecisionTreeExplainableModel\n",
      "mim_explainer = MimicExplainer(model=loan_model,\n",
      " initialization_examples=X_test,\n",
      " explainable_model = DecisionTreeExplainableModel,\n",
      " features=['loan_amount','income','age','marital_status'],\n",
      " classes=['reject', 'approve'])\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "22.\n",
      "Question 22\n",
      "What code should you write for a PFIExplainer if you have a model entitled loan_model?\n",
      "0 / 1 point\n",
      "from interpret.ext.blackbox import PFIExplainer\n",
      "pfi_explainer = PFIExplainer(model = loan_model,\n",
      " features=['loan_amount','income','age','marital_status'],\n",
      " classes=['reject', 'approve'])\n",
      "from interpret.ext.blackbox import PFIExplainer\n",
      "pfi_explainer = PFIExplainer(model = loan_model,\n",
      "      initialization_examples=X_test,\n",
      " classes=['loan_amount','income','age','marital_status'],\n",
      " features=['reject', 'approve'])\n",
      "from interpret.ext.blackbox import PFIExplainer\n",
      "pfi_explainer = PFIExplainer(model = loan_model,\n",
      "      explainable_model= DecisionTreeExplainableModel,\n",
      " features=['loan_amount','income','age','marital_status'],\n",
      " classes=['reject', 'approve'])\n",
      "from interpret.ext.blackbox\n",
      "pfi_explainer = PFIExplainer(model = loan_model,\n",
      "      initialization_examples=X_test,\n",
      " features=['loan_amount','income','age','marital_status'],\n",
      " classes=['reject', 'approve'])\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "23.\n",
      "Question 23\n",
      "Your task is to train a binary classification model in order for it to be able to target the correct subjects in a marketing campaign.\n",
      "What actions should you take if you want to ensure that your model is fair and will not be inclined to ethnic discrimination?\n",
      "0 / 1 point\n",
      "Compare disparity between selection rates and performance metrics across ethnicities.\n",
      "Evaluate each trained model with a validation dataset, and use the model with the highest accuracy score. An accurate model is inherently fair.\n",
      "Remove the ethnicity feature from the training dataset.\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "24.\n",
      "Question 24\n",
      "Your task is to back fill a dataset monitor for the previous 5 months based on changes made in data on a monthly basis.\n",
      "What code should you write in the SDK to achieve this goal?\n",
      "0 / 1 point\n",
      "import datetime as dt\n",
      "backfill = monitor_backfill( dt.datetime.now(), dt.timedelta(months=5), dt.datetime.now())\n",
      "import datetime as dt\n",
      "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(months=5), dt.datetime.now())\n",
      "import datetime as dt\n",
      "backfill = monitor.backfill( dt.datetime.now(), dt.timedelta(months=5), dt.datetime.now())\n",
      "import datetime as dt\n",
      "backfill = monitor_backfill( dt.datetime.now() - dt.timedelta(months=5), dt.datetime.now())\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "25.\n",
      "Question 25\n",
      "In order to track the runs of a Linear Regression model of your AirBnB dataset, you decide to use MLflow.\n",
      "You want to make use of all the features included in your dataset.\n",
      "At this point, you have created and logged the pipeline and you have logged the parameters.\n",
      "You now have to create some predictions and metrics.\n",
      "Considering this scenario, what code should you write?\n",
      "0 / 1 point\n",
      "predDF = pipelineModel.evaluate(testDF)\n",
      "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "predDF = pipelineModel.transform(testDF)\n",
      "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "predDF = pipelineModel.estimate(testDF)\n",
      "regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "predDF = pipelineModel.transform(testDF)\n",
      "regression = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
      "rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
      "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "26.\n",
      "Question 26\n",
      "You are using remote compute in Azure Machine Learning to run a training experiment.\n",
      "The Conda environment used for the experiment includes both the mlflow, and the azureml-contrib-run packages. In order to track the metrics that the experiment generates, you have to log package by using MLflow.\n",
      "To give the correct answer, you have to replace the code comments that are bolded with some suitable code options that you find in the answer area.\n",
      "Considering this, what snippet should you choose to complete the code?\n",
      "Import numpy as np\n",
      "#1 Import library to log metrics\n",
      "#2 Start logging for this run\n",
      "reg_rage = 0.01\n",
      "#3 Log the reg_rate metric\n",
      "#4 Stop loggin for this run\n",
      "0 / 1 point\n",
      "#1 from azureml.core import Run, #2 run = Run.get_context(), #3 logger.info(' ..'), #4 run.complete()\n",
      "#1 import logging, #2 mlflow.start_run(), #3 mlflow.log_metric(' ..'), #4 run.complete()\n",
      "#1 import mlflow, #2 mlflow.start_run(), #3 mlflow.log_metric(' ..'), #4 mlflow.end_run()\n",
      "#1 import mlflow, #2 mlflow.start_run(), #3 logger.info(' ..'), #4 mlflow.end_run()\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "27.\n",
      "Question 27\n",
      "You want to deploy in your Azure Container Instance a deep learning model.\n",
      "In order to call the model API, you have to use the Azure Machine Learning SDK.\n",
      "To invoke the deployed model, you have to use native SDK classes and methods.\n",
      "To give the correct answer, you have to replace the code comments that are bolded with some suitable code options that you find in the answer area.\n",
      "Considering this, what snippet should you choose to complete the code?\n",
      "from azureml.core import Workspace\n",
      "#1st code option\n",
      "Import json\n",
      "ws = Workspace.from_config()\n",
      "service_name = “mlmodel1-service”\n",
      "service = Webservice(name=service_name, workspace=ws)\n",
      "x_new = [[2, 101.5, 1, 24, 21], [1, 89.7, 4, 41, 21]]\n",
      "input_json = json.dumps({“data”: x_new})\n",
      "#2nd code option\n",
      "0 / 1 point\n",
      "from azureml.core.webservice import requests, predictions = service.run(input_json)\n",
      "from azureml.core.webservice import Webservice, predictions = service.deserialize(ws, input_json)\n",
      "from azureml.core.webservice import Webservice, predictions = service.run(input_json)\n",
      "from azureml.core.webservice import LocalWebservice, predictions = service.run(input_json)\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "28.\n",
      "Question 28\n",
      "One of the categorical variables of your AirBnB dataset is room type.\n",
      "You have three room types, as follows: private room, entire home/apt, and shared room.\n",
      "In order for the machine learning model to know how to handle the room types, you have to firstly encode every unique string into a number.\n",
      "What code should you write to achieve this goal?\n",
      "0 / 1 point\n",
      "from pyspark.ml.feature import Indexer\n",
      "uniqueTypesDF = airbnbDF.select(\"room_type\").distinct()\n",
      "indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\")\n",
      "indexerModel = indexer.fit(uniqueTypesDF)\n",
      "indexedDF = indexerModel.transform(uniqueTypesDF)\n",
      "display(indexedDF)\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "uniqueTypesDF = airbnbDF.select(\"room_type\").distinct()\n",
      "indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\")\n",
      "indexerModel = indexer.transform(uniqueTypesDF)\n",
      "indexedDF = indexerModel.transform(uniqueTypesDF)\n",
      "display(indexedDF)\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "uniqueTypesDF = airbnbDF.select(\"room_type\").distinct()\n",
      "indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\")\n",
      "indexerModel = indexer.fit(uniqueTypesDF)\n",
      "indexedDF = indexerModel.transform(uniqueTypesDF)\n",
      "display(indexedDF)\n",
      "from pyspark.ml.feature import StringIndexer\n",
      "uniqueTypesDF = airbnbDF.select(\"room_type\").distinct()\n",
      "indexer = StringIndexer(inputCol=\"room_type”)\n",
      "indexerModel = indexer.fit(uniqueTypesDF)\n",
      "indexedDF = indexerModel.transform(uniqueTypesDF)\n",
      "display(indexedDF)\n",
      "29. Your task is to extract from the experiments list the last run.\n",
      "What code should you write in Python to achieve this?\n",
      "runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=1)\n",
      "runs[0].data.metrics\n",
      "runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time asce\"], max_results=1)\n",
      "runs[0].data.metrics\n",
      "runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time\"], max_results=1)\n",
      "runs[0].data.metrics\n",
      "runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=3)\n",
      "runs[0].data.metrics\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "30.\n",
      "Question 30\n",
      "Choose from the list below the cross-validation technique that belongs to the exhaustive type.\n",
      "0 / 1 point\n",
      "Holdout cross-validation\n",
      "K-fold cross-validation\n",
      "Leave-one-out cross-validation\n",
      "Leave-p-out cross-validation\n",
      "Vous n'avez pas sélectionné toutes les bonnes réponses\n",
      "31. Your task is to clean up the deployments and terminate the “dev” ACI webservice by making use of the Azure ML SDK after your work with Azure Machine Learning has ended.\n",
      "What is the most suitable method in order to achieve this goal?\n",
      "(accroché) dev_webservice.delete()\n",
      "dev_webservice.terminate()\n",
      "dev_webservice.flush()\n",
      "dev_webservice.remove()\n",
      "32. The DataFrame you are currently working on contains data regarding the daily sales of ice cream. In order to compare the avg_temp and units_sold columns you decided to use the corr method which returned a result of 0.95.\n",
      "What information can you read from this result?\n",
      "0 / 1 point\n",
      "Days with high avg_temp values tend to coincide with days that have high units_sold values\n",
      "On the day with the maximum units_sold value, the avg_temp value was 0.95\n",
      "The units_sold value is, on average, 95% of the avg_temp value\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "33.\n",
      "Question 33\n",
      "You can enable the Application Insights when configuring the service deployment at the moment you want to deploy a new real-time service.\n",
      "By using the SDK, what code should you write to achieve this goal?\n",
      "0 / 1 point\n",
      "dep_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
      " memory_gb = 1,\n",
      " enable_app_insights=True)\n",
      "dep_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
      " memory_gb = 1,\n",
      " app_insights(True))\n",
      "dep_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
      " memory_gb = 1,\n",
      " app_insights=True)\n",
      "dep_config = AciWebservice.deploy_configuration(cpu_cores = 1,\n",
      " memory_gb = 1,\n",
      " appinsights=True)\n",
      "Incorrect\n",
      "Vous n’avez sélectionné aucune réponse.\n",
      "34.\n",
      "Question 34\n",
      "You decided to use Parquet files and Petastorm to train a distributed neural network by using Horovod.\n",
      "Your housing prices dataset from California is entitled cal_housing.\n",
      "In order to concatenate the features and labels of the model after you loaded the data, you configured from the Pandas DataFrame a Spark DataFrame.\n",
      "At this point, you want to set up Dense Vectors for the features.\n",
      "What code should you write in Python to achieve this?\n",
      "0 / 1 point\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "vecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"features\")\n",
      "vecTrainDF = vecAssembler.transform(trainDF).hook(\"features\", \"label\")\n",
      "display(vecTrainDF)\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "vecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"features\")\n",
      "vecTrainDF = vecAssembler.transform(trainDF).call(\"features\", \"label\")\n",
      "display(vecTrainDF)\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "vecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"features\")\n",
      "vecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\n",
      "display(vecTrainDF)\n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "vecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"labels \")\n",
      "vecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\n",
      "display(vecTrainDF)\n",
      "5. How should the following sentence be completed?\n",
      "One example of the machine learning […] type models is the Support Vector Machine algorithm.\n",
      "Clustering\n",
      "Regression\n",
      "(accroché) Classification\n",
      "Correct. Logistic Regression is a well-established algorithm for classification.\n",
      "6. You have a Pandas DataFrame entitled df_sales that contains the sales data from each day. You DataFrame contains these columns: year, month, day_of_month, sales_total. Which of the following codes should you choose if your goal is to return the average sales_total value?\n",
      "1 / 1 point\n",
      "(accroché) df_sales['sales_total'].mean()\n",
      "mean(df_sales['sales_total'])\n",
      "df_sales['sales_total'].avg()\n",
      "Correct\n",
      "This code will return the average of the sales_total column values.\n",
      "7.\n",
      "Question 7\n",
      "When you use the Support Vector Machine algorithm, what type of machine learning model is possible to train?\n",
      "1 / 1 point\n",
      "Regression\n",
      "Clustering\n",
      "(accroché) Classification\n",
      "Correct. Logistic Regression is a well-established algorithm for classification.\n",
      "8. You are able to associate the K-Means clustering algorithm with the following machine learning type:\n",
      "1 / 1 point\n",
      "Reinforcement learning\n",
      "(accroché) Unsupervised machine learning\n",
      "Supervised machine learning\n",
      "Correct. Clustering is a form of unsupervised machine learning in which the training data does not include known labels.\n",
      "9. Your deep neural network is in the process of training. You decided to set 30 epochs to the training process configuration.\n",
      "In this scenario, what would happen to the model’s behavior?\n",
      "0 / 1 point\n",
      "The training data is split into 30 subsets, and each subset is passed through the network\n",
      "(accroché) The entire training dataset is passed through the network 30 times\n",
      "The first 30 rows of data are used to train the model, and the remaining rows are used to validate it\n",
      "10. The layer described below is used to reduce the number of feature values that are extracted from images, while still retaining the key differentiating features.\n",
      "1 / 1 point\n",
      "Flattening layer\n",
      "(accroché) Pooling layer\n",
      "Convolutional layer\n",
      "Correct. After extracting feature values from images, pooling (or downsampling) layers are used to reduce the number of feature values while retaining the key differentiating features that have been extracted.\n",
      "11. You want to set up a new Azure subscription. The subscription doesn’t contain any resources.\n",
      "Your goal is to create an Azure Machine Learning workspace.\n",
      "Considering this scenario, which are three possible ways to obtain this result? Keep in mind that every correct answer presents a complete solution.\n",
      "1 / 1 point\n",
      "Navigate to Azure Machine Learning studio and create a workspace.\n",
      "(accroché) Use the Azure Command Line Interface (CLI) with the Azure Machine Learning extension to call the az group create function with –name and –location parameters, and then the az ml workspace create function, specifying Cw and Cg parameters for the workspace name and resource group.\n",
      "Correct\n",
      "This is one way to achieve the goal.\n",
      "(accroché) Use an Azure Resource Management template that includes a Microsoft.MachineLearningServices/ workspaces resource and its dependencies.\n",
      "Correct\n",
      "This is one way to achieve the goal.\n",
      "Run Python code that uses the Azure ML SDK library and calls the Workspace.get method with name, subscription_id, and resource_group parameters.\n",
      "(accroché) Run Python code that uses the Azure ML SDK library and calls the Workspace.create method with name, subscription_id, resource_group, and location parameters.\n",
      "Correct\n",
      "This is one way to achieve the goal.\n",
      "12.\n",
      "Question 12\n",
      "You decide to use GPU-based training to develop a deep learning model on Azure Machine Learning service that is able to recognize image.\n",
      "The context where you have to configure the model needs to allow real-time GPU-based inferencing.\n",
      "Considering that you have to set up compute resources for model inferencing, what is the most suitable compute type?\n",
      "1 / 1 point\n",
      "Field Programmable Gate Array\n",
      "Machine Learning Compute\n",
      "(accroché) Azure Kubernetes Service\n",
      "Azure Container Instance\n",
      "Correct\n",
      "You can use Azure Machine Learning to deploy a GPU-enabled model as a web service. Deploying a model on Azure Kubernetes Service (AKS) is a viable option. The AKS cluster provides a GPU resource that is used by the model for inference.\n",
      "13.\n",
      "Question 13\n",
      "You decide to use the code below for the deployment of a model as an Azure Machine Learning real-time web service:\n",
      "# ws, model, inference_config, and deployment_config defined previously\n",
      "service = Model.deploy(ws, ‘classification-service’, [model], inference_config, deployment_config)\n",
      "service.wait_for_deployment(True)\n",
      "Your deployment does not succeed.\n",
      "You have to troubleshoot the deployment failure in order to determine what actions were taken while deploying and to identify the one action that encountered a problem and didn’t succeed.\n",
      "For this scenario, which of the following code snippets should you use?\n",
      "1 / 1 point\n",
      "(accroché) service.get_logs()\n",
      "service.serialize()\n",
      "service.update_deployment_state()\n",
      "service.state\n",
      "Correct\n",
      "You can print out detailed Docker engine log messages from the service object.\n",
      "You can view the log for ACI, AKS, and Local deployments.\n",
      "14.\n",
      "Question 14\n",
      "Yes or No?\n",
      "In order to explain the model’s predictions, you have to calculate the importance of all the features, taking into account the overall global relative importance value, but also the measure of local importance for a certain set of predictions.\n",
      "You decide to obtain the global and local feature importance values that you need by using an explainer.\n",
      "Solution: Configure a PFIExplainer. Is this solution effective?\n",
      "1 / 1 point\n",
      "Yes\n",
      "(accroché) No\n",
      "Correct\n",
      "The PFIExplainer doesn't support local feature importance explanations.\n",
      "15.\n",
      "Question 15\n",
      "After installing the Azure Machine Learning Python SDK, you decide to use it to configure on your subscription a workspace entitled “aml-workspace”.\n",
      "What code should you write in Python for this task?\n",
      "0 / 1 point\n",
      "(accroché) from azureml.core import Workspace\n",
      " ws = Workspace.create(name='aml-workspace',\n",
      " subscription_id='123456-abc-123...',\n",
      " resource_group='aml-resources',\n",
      " create_resource_group=True,\n",
      " location='eastus'\n",
      " )\n",
      "from azureml.core import Workspace\n",
      " ws = Workspace.create(name='aml-workspace',\n",
      " subscription_id='123456-abc-123...',\n",
      " resource_group='aml-resources',\n",
      " location='eastus'\n",
      " )\n",
      "azureml.core import Workspace\n",
      " ws = Workspace.create(name='aml-workspace',\n",
      " subscription_id='123456-abc-123...',\n",
      " resource_group='aml-resources',\n",
      " create_resource_group=False,\n",
      " location='eastus'\n",
      " )\n",
      "16.\n",
      "Question 16\n",
      "What Python command should you choose in order to view the models previously registered in the Azure ML studio by using the Model object?\n",
      "1 / 1 point\n",
      "from azureml.core import Model\n",
      "for model in List.Model(ws):\n",
      "print(model.name, 'version:', model.version)\n",
      "from azureml.core import Model\n",
      "for model in Model.list(ws):\n",
      "get(model.name, 'version:', model.version)\n",
      "(accroché) from azureml.core import Model\n",
      "for model in Model.list(ws):\n",
      "print(model.name, 'version:', model.version)\n",
      "from azureml.core import Model\n",
      "for model in Model.object(ws):\n",
      "print(model.name, 'version:', model.version)\n",
      "17. If you want to extract a dataset after its registration, what are the most suitable methods you should choose from the Dataset class?\n",
      "1 / 1 point\n",
      "(accroché) get_by_name\n",
      "Correct\n",
      "This method will retrieve a dataset using its name.\n",
      "(accroché) get_by_id\n",
      "Correct\n",
      "This method will retrieve a dataset using its id.\n",
      "find_by_name\n",
      "find_by_id\n",
      "18. If you want to visualize the environments that you registered in your workspace, what are the most appropriate SDK commands that you should choose?\n",
      "1 / 1 point\n",
      "from azureml.core import Environment\n",
      "env_names = Environment_list(workspace=ws)\n",
      "for env_name in env_names:\n",
      "print('Name:',env_name)\n",
      "from azureml.core import Environment\n",
      "env_names = Environment.list(workspace=ws)\n",
      "for each env_name in env_names:\n",
      "print('Name:',env_name)\n",
      "from azureml.core import Environment\n",
      "env_names = Environment.list(workspace=ws)\n",
      "for env_name of env_names:\n",
      "print('Name:',env_name)\n",
      "(accroché) from azureml.core import Environment\n",
      "env_names = Environment.list(workspace=ws)\n",
      "for env_name in env_names:\n",
      "print('Name:',env_name)\n",
      "Correct\n",
      "These commands will show you the registered environments in your workspace.\n",
      "19.\n",
      "Question 19\n",
      "What object needs to be defined if your task is to create a schedule for your pipeline?\n",
      "1 / 1 point\n",
      "ScheduleSync\n",
      "ScheduleConfig\n",
      "(accroché) ScheduleRecurrence\n",
      "ScheduleTimer\n",
      "Correct. To schedule a pipeline to run at periodic intervals, you must define a ScheduleRecurrence that determines the run frequency, and use it to create a Schedule.\n",
      "20.\n",
      "Question 20\n",
      "If you want to set up a parallel run step, which of the SDK commands below should you choose?\n",
      "1 / 1 point\n",
      "parallelrun_step = ParallelRunStep(\n",
      " name='batch-score',\n",
      " parallel.run.config=parallel_run_config,\n",
      " inputs=[batch_data_set.as_named_input('batch_data')],\n",
      " output=output_dir,\n",
      " arguments=[],\n",
      " allow_reuse=True\n",
      "(accroché) parallelrun_step = ParallelRunStep(\n",
      " name='batch-score',\n",
      " parallel_run_config=parallel_run_config,\n",
      " inputs=[batch_data_set.as_named_input('batch_data')],\n",
      " output=output_dir,\n",
      " arguments=[],\n",
      " allow_reuse=True\n",
      "parallelrun_step = ParallelRunStep(\n",
      " name='batch-score',\n",
      " parallel_run_config=parallel.run.config,\n",
      " inputs=[batch_data_set.as_named_input('batch_data')],\n",
      " output=output_dir,\n",
      " arguments=[],\n",
      " allow_reuse=True\n",
      "parallelrun.step = ParallelRunStep(\n",
      " name='batch-score',\n",
      " parallel_run_config=parallel_run_config,\n",
      " inputs=[batch_data_set.as_named_input('batch_data')],\n",
      " output=output_dir,\n",
      " arguments=[],\n",
      " allow_reuse=True\n",
      "21. What Python code should you write if your goal is to extract the primary metric for a regression task?\n",
      "1 / 1 point\n",
      "from azureml.train.automl.utilities import catch_primary_metrics\n",
      "catch_primary_metrics(‘regression')\n",
      "from azureml.train.automl.utilities import pull_primary_metrics\n",
      "pull_primary_metrics(‘regression')\n",
      "(accroché) from azureml.train.automl.utilities import get_primary_metrics\n",
      "get_primary_metrics('regression')\n",
      "from azureml.train.automl.utilities import feed_primary_metrics\n",
      "feed_primary_metrics('regression')\n",
      "22. Your task is to extract local feature importance from a TabularExplainer.\n",
      "What code should you write in the SDK to achieve this goal?\n",
      "1 / 1 point\n",
      "local.tab_explanation = tab_explainer_explain_local(X_test[0:5])\n",
      "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
      "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
      "local_tab_explanation = tab_explainer.explain_local(X_test[0:5])\n",
      "local_tab_features = local_tab_explanation.get_feature_local_names()\n",
      "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
      "local_tab_explanation = tab_explainer.explain_local(X_test[0:5])\n",
      "local_tab_features = local_tab_explanation.get_feature_importance_dict ()\n",
      "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
      "(accroché) local_tab_explanation = tab_explainer.explain_local(X_test[0:5])\n",
      "local_tab_features = local_tab_explanation.get_ranked_local_names()\n",
      "local_tab_importance = local_tab_explanation.get_ranked_local_values()\n",
      "23.\n",
      "Question 23\n",
      "Your task is to train a binary classification model in order for it to be able to target the correct subjects in a marketing campaign.\n",
      "What actions should you take if you want to ensure that your model is fair and will not be inclined to ethnic discrimination?\n",
      "1 / 1 point\n",
      "Remove the ethnicity feature from the training dataset.\n",
      "(accroché) Compare disparity between selection rates and performance metrics across ethnicities.\n",
      "Evaluate each trained model with a validation dataset, and use the model with the highest accuracy score. An accurate model is inherently fair.\n",
      "Correct\n",
      "By using ethnicity as a sensitive field, and comparing disparity between selection rates and performance metrics for each ethnicity value, you can evaluate the fairness of the model.\n",
      "24. Your task is to back fill a dataset monitor for the previous 5 months based on changes made in data on a monthly basis.\n",
      "What code should you write in the SDK to achieve this goal?\n",
      "1 / 1 point\n",
      "(accroché) import datetime as dt\n",
      "backfill = monitor.backfill( dt.datetime.now() - dt.timedelta(months=5), dt.datetime.now())\n",
      "import datetime as dt\n",
      "backfill = monitor_backfill( dt.datetime.now(), dt.timedelta(months=5), dt.datetime.now())\n",
      "import datetime as dt\n",
      "backfill = monitor.backfill( dt.datetime.now(), dt.timedelta(months=5), dt.datetime.now())\n",
      "import datetime as dt\n",
      "backfill = monitor_backfill( dt.datetime.now() - dt.timedelta(months=5), dt.datetime.now())\n",
      "25. You decided to use the AirBnB Housing dataset and the Linear Regression algorithm for which you want to tune the Hyperparameters.\n",
      "At this point, for the Boston data set you have executed a test split and for the linear regression you have built a pipeline.\n",
      "You now want to test the maximum number of iterations by using the ParamGridBuilder() and you can do this no matter if you want to use an intercept with the y axis or fi you want to standardize the features.\n",
      "Considering this scenario, what code should you write?\n",
      "1 / 1 point\n",
      "from pyspark.ml.tuning import ParamGridBuilder\n",
      "paramGrid = (ParamGridBuilder(lr)\n",
      ".addGrid(lr.maxIter, [1, 10, 100])\n",
      ".addGrid(lr.fitIntercept, [True, False])\n",
      ".addGrid(lr.standardization, [True, False])\n",
      ".create()\n",
      ")\n",
      "from pyspark.ml.tuning import ParamGridBuilder\n",
      "paramGrid = (ParamGridBuilder(lr)\n",
      ".addGrid(lr.maxIter, [1, 10, 100])\n",
      ".addGrid(lr.fitIntercept, [True, False])\n",
      ".addGrid(lr.standardization, [True, False])\n",
      ".run()\n",
      ")\n",
      "(accroché) from pyspark.ml.tuning import ParamGridBuilder\n",
      "paramGrid = (ParamGridBuilder()\n",
      ".addGrid(lr.maxIter, [1, 10, 100])\n",
      ".addGrid(lr.fitIntercept, [True, False])\n",
      ".addGrid(lr.standardization, [True, False])\n",
      ".build()\n",
      ")\n",
      "from pyspark.ml.tuning import ParamGridBuilder\n",
      "paramGrid = (ParamGridBuilder()\n",
      ".addGrid(lr.maxIter, [1, 10, 100])\n",
      ".addGrid(lr.fitIntercept, [True, False])\n",
      ".addGrid(lr.standardization, [True, False])\n",
      ".search()\n",
      ")\n",
      "Correct\n",
      "This is the correct code for this task.\n",
      "26.\n",
      "Question 26\n",
      "You decided to use Python code interactively in your Conda environment. You have all the required Azure Machine Learning SDK and MLflow packages in the environment.\n",
      "In order to log metrics in your Azure Machine Learning experiment entitled mlflow-experiment, you have to use MLflow.\n",
      "To give the correct answer, you have to replace the code comments that are bolded with some suitable code options that you find in the answer area.\n",
      "Considering this, what snippet should you choose to complete the code?\n",
      "import mlflow\n",
      "from azureml.core import Workspace\n",
      "ws = Workspace.from_config()\n",
      "#1 Set the MLflow logging target\n",
      "#2 Configure the experiment\n",
      "with #3 Begin the experiment run\n",
      "   #4 Log my_metric with value 1.00 (‘my_metric’, 1.00)\n",
      "print(“Finished!”)\n",
      "1 / 1 point\n",
      "#1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #2 mlflow.get_run('mlflow-experiment), #3 mlflow.start_run(), #4 run.log()\n",
      "(accroché) #1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #2 mlflow.set_experiment('mlflow-experiment), #3 mlflow.start_run(), #4 mlflow.log_metric\n",
      "#1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #2 mlflow.get_run('mlflow-experiment), #3 mlflow.start_run(), #4 mlflow.log_metric\n",
      "#1 mlflow.tracking.client = ws, #2 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()), #3 mlflow.active_run(), #4 mlflow.log_metric\n",
      "Correct\n",
      "#1 mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri()) In the following code, the get_mlflow_tracking_uri() method assigns a unique tracking URI address to the workspace, ws, and set_tracking_uri() points the MLflow tracking URI to that address.\n",
      "#2 mlflow.set_experiment(experiment_name) Set the MLflow experiment name with set_experiment() and start your training run with start_run().\n",
      "#3 mlflow.start_run()\n",
      "#4 mlflow.log_metric - Then use log_metric() to activate the MLflow logging API and begin logging your training run metrics.\n",
      "27.\n",
      "Question 27\n",
      "For your experiment in Azure Machine Learning you decide to run the following code:\n",
      "from azureml.core import Workspace, Experiment, Run\n",
      "from azureml.core import RunConfig, ScriptRunConfig\n",
      "ws = Workspace.from_config()\n",
      "run_config = RunConfiguration()\n",
      "run_config.target=’local’\n",
      "script_config = ScriptRunConfig(source_directory=’./script’, script=’experiment.py’, run_config=run_config)\n",
      "experiment = Experiment(workspace=ws, name=’script experiment’)\n",
      "run = experiment.submit(config=script_config)\n",
      "run.wait_for_completion()\n",
      "The experiment run generates several output files that need identification.\n",
      "In order to retrieve the output file names, you must write some code. Which of the following code snippets should you choose to complete the script?\n",
      "0 / 1 point\n",
      "files = run.get_fine_names()\n",
      "files = run.get_details_with_logs()\n",
      "files = run.get_metrics()\n",
      "(accroché) files = run.get_properties()\n",
      "28.\n",
      "Question 28\n",
      "Which of the options listed below is able to show if you have missing values in the dataset when you want to find out the number of observations in the data set in the process of explanatory data analysis?\n",
      "1 / 1 point\n",
      "Mean\n",
      "Standard deviation\n",
      "(accroché) Count\n",
      "Correct\n",
      "Count gives us the number of observed values, indicating the size of the dataset and whether there are missing values.\n",
      "29.\n",
      "Question 29\n",
      "You are able to use the the MlflowClient object as the pathway in order to query previous runs in a programmatic manner.\n",
      "What code should you write in Python to achieve this?\n",
      "1 / 1 point\n",
      "from mlflow.pipelines import MlflowClient\n",
      "client = MlflowClient()\n",
      "list.experiments()\n",
      "from mlflow.tracking import MlflowClient\n",
      "client = MlflowClient()\n",
      "list.client_experiments()\n",
      "from mlflow.pipelines import MlflowClient\n",
      "client = MlflowClient()\n",
      "client.list_experiments()\n",
      "(accroché) from mlflow.tracking import MlflowClient\n",
      "client = MlflowClient()\n",
      "client.list_experiments()\n",
      "30.\n",
      "Question 30\n",
      "Which of the non-exhaustive cross validation techniques listed below enables you to assign data points in a random way to the training set and the test set?\n",
      "0 / 1 point\n",
      "Repeated random sub-sampling validation\n",
      "(accroché) K-fold cross-validation\n",
      "Holdout cross-validation\n",
      "31.\n",
      "Question 31\n",
      "Your task is to clean up the deployments and terminate the “dev” ACI webservice by making use of the Azure ML SDK after your work with Azure Machine Learning has ended.\n",
      "What is the most suitable method in order to achieve this goal?\n",
      "1 / 1 point\n",
      "dev_webservice.terminate()\n",
      "dev_webservice.flush()\n",
      "dev_webservice.remove()\n",
      "(accroché) dev_webservice.delete()\n",
      "Correct. Because ACI manages compute resources on your behalf, deleting the \"dev\" ACI webservice will remove all resources associated with the \"dev\" model deployment\n",
      "32. Your task is to store in the Azure ML workspace a model for whose training you ran an experiment. You want to do this so that other experiments and services can be applied to the model.\n",
      "Considering this scenario, what action should you take to achieve the result?\n",
      "1 / 1 point\n",
      "Save the model as a file in a compute instance\n",
      "Save the model as a file in a Key Vault instance\n",
      "(accroché) Register the model in the workspace\n",
      "Save the experiment script as a notebook\n",
      "Correct\n",
      "To store a model in the workspace, you have to register it.\n",
      "33.\n",
      "Question 33\n",
      "Which of the following methods are the ACI services and AKS services default authentication ones?\n",
      "0 / 1 point\n",
      "(accroché) Disabled for ACI services\n",
      "Disabled for AKS services\n",
      "(accroché) Key-based for AKS services\n",
      "Correct\n",
      "By default, authentication is set to key-based authentication for AKS services (for which primary and secondary keys are automatically generated).\n",
      "Token-based for AKS services.\n",
      "(Incorrect) Token-based for ACI services\n",
      "34. You decided to use Azure Machine Learning and your goal is to train a Diabetes Model and build a container image for it.\n",
      "You choose to make use of the scikit-learn ElasticNet linear regression model.\n",
      "You want to use Azure Kubernetes Service (AKS) for the model deployment to production.\n",
      "For deploying the model, you configured an AKS cluster.\n",
      "At this point, you have deployed the image of the model to the desired AKS cluster.\n",
      "After using different hyperparameters to train the new model, your goal is to deploy to the AKS cluster the new image of the model.\n",
      "What code should you write for this task?\n",
      "1 / 1 point\n",
      "prod_webservice.create (image=model_image_updated)\n",
      "prod_webservice.wait_for_deployment(show_output = True)\n",
      "prod_webservice.deploy (image=model_image_updated)\n",
      "prod_webservice.wait_for_deployment(show_output = True)\n",
      "(accroché) prod_webservice.update(image=model_image_updated)\n",
      "prod_webservice.wait_for_deployment(show_output = True)\n",
      "prod_webservice.delete (image=model_image_updated)\n",
      "prod_webservice.wait_for_deployment(show_output = True)\n",
      "Correct\n",
      "This is the correct code for this task.\n",
      "35. You create a machine learning model by using the Azure Machine Learning designer. You publish the model as a real-time service on an Azure Kubernetes Service (AKS) inference compute cluster. You make no change to the deployed endpoint configuration.\n",
      "You need to provide application developers with the information they need to consume the endpoint. Which two values should you provide to application developers? Each correct answer presents part of the solution.\n",
      "The name of the inference pipeline for the endpoint\n",
      "(accroché) The key for the endpoint\n",
      "(accroché) The URL of the endpoint\n",
      "The name of the AKS cluster where the endpoint is hosted\n",
      "The run ID of the inference pipeline experiment for the endpoint\n",
      "\\section{Vérifiez vos connaissances}\n",
      "1. Quelle est la différence entre les modèles de régression classique et les modèles de classification ?\n",
      "Les modèles de régression fournissent des étiquettes telles que « cerise »/« banane », alors que les modèles de classification calculent des nombres continus\n",
      "Les modèles de classification et les modèles de régression linéaire sont deux noms désignant la même chose.\n",
      "(accroché) Les modèles de classification fournissent des étiquettes telles que « cerise »/« banane », alors que les modèles de régression calculent des nombres continus\n",
      "2. Comment pouvons-nous améliorer les performances des modèles dans le monde réel ?\n",
      "En ajoutant des caractéristiques\n",
      "(accroché) L’ajout et la suppression de caractéristiques peuvent être utiles, en fonction de la situation\n",
      "En supprimant des caractéristiques\n",
      "3. Quelle est l’une des raisons pour lesquelles la régression logistique utilise la perte logarithmique plutôt qu’une fonction de coût plus intuitive ?\n",
      "(accroché) La perte logarithmique est plus stricte à l’égard des erreurs de modèle même si elles sont proches de la réalité.\n",
      "C’est la seule façon de calculer l’erreur pour les étiquettes catégoriques.\n",
      "La perte logarithmique est plus tolérante à l’égard d’une erreur de modèle si celle-ci est proche de la réalité.\n",
      "------------------------------------\n",
      "Initiation au Détecteur d’anomalies\n",
      "https://learn.microsoft.com/fr-fr/training/modules/intro-to-anomaly-detector/4a-exercise\n",
      "------------------------------------\n",
      "La détection d’anomalie est une technique d’intelligence artificielle utilisée pour déterminer si les valeurs d’une série sont comprises dans les paramètres attendus.\n",
      "Le Détecteur d’anomalies fait partie de la catégorie Services de décision d’Azure Cognitive Services.\n",
      "La limite est définie à l’aide d’une valeur de sensibilité. Par défaut, la limite supérieure et la limite inférieure de la détection d’anomalie sont calculées à l’aide de concepts nommés expectedValue (valeur attendue), upperMargin (marge haute) et lowerMargin (marge basse). Si une valeur dépasse l’une des deux limites, elle est identifiée comme une anomalie. Vous pouvez ajuster les limites en appliquant une marginScale à la marge supérieure et à la marge inférieure, comme le montre la formule suivante.\n",
      "upperBoundary = expectedValue + (100 – marginScale) × upperMargin\n",
      "Vous pouvez utiliser toutes les données numériques que vous avez enregistrées au fil du temps. Les principaux aspects des données envoyées sont la granularité, un horodateur et la valeur enregistrée pour cet horodateur.\n",
      "{\n",
      "    \"granularity\": \"hourly\",\n",
      "    \"series\": [\n",
      "      {\n",
      "        \"timestamp\": \"2021-03-01T01:00:00Z\",\n",
      "        \"value\": -10.56\n",
      "      },\n",
      "      {\n",
      "        \"timestamp\": \"2021-03-02T02:00:00Z\",\n",
      "        \"value\": -8.30\n",
      "      },\n",
      "      {\n",
      "        \"timestamp\": \"2021-03-02T03:00:00Z\",\n",
      "        \"value\": -10.30\n",
      "      },\n",
      "      {\n",
      "        \"timestamp\": \"2021-03-02T04:00:00Z\",\n",
      "        \"value\": 5.95\n",
      "      },\n",
      "    ]\n",
      "}\n",
      "Le service prend en charge au maximum 8 640 points de données.\n",
      "S’il peut y avoir des valeurs manquantes dans votre séquence de données, tenez compte des recommandations suivantes.\n",
      "    L’échantillonnage se produit à quelques minutes d’intervalle, et il manque moins de 10 % du nombre attendu de points. Dans ce cas, l’impact devrait être négligeable sur les résultats de la détection.\n",
      "    S’il manque plus de 10 %, il existe des solutions pour « remplir » le jeu de données. Vous pouvez utiliser une méthode d’interpolation linéaire pour compléter les valeurs manquantes dans le jeu de données. Elle comble les lacunes avec des valeurs réparties uniformément.\n",
      "------------------------------------\n",
      "La détection par lots est mieux utilisée lorsque vos données contiennent les éléments suivants :\n",
      "    Données de série chronologique à tendance stable affichant des pics ou des creux occasionnels\n",
      "    Données de série chronologique saisonnière affichant des anomalies occasionnelles\n",
      "        Le caractère saisonnier est considéré comme un pattern dans les données, qui se produit à intervalles réguliers\n",
      "Il crée un modèle en utilisant l’ensemble du jeu de données, il fait des predictions à chaque point de données du jeu de données nouveaux\n",
      "------------------------------------\n",
      "Détection en temps réel\n",
      "La détection en temps réel utilise des données de streaming en comparant des points de données vus précédemment au dernier point de données pour déterminer si votre dernier est une anomalie.\n",
      "------------------------------------\n",
      "Créer une ressource Détecteur d'anomalies (https://portal.azure.com/)\n",
      "Commençons par créer une ressource Détecteur d’anomalies dans votre abonnement Azure :\n",
      "    Sous un autre onglet de navigateur, ouvrez le portail Azure à l’adresse https://portal.azure.com en vous connectant avec votre compte Microsoft.\n",
      "    Cliquez sur le bouton ＋Créer une ressource, recherchez Détecteur d’anomalies, puis créez une ressource Détecteur d’anomalies avec les paramètres suivants :\n",
      "        Abonnement : votre abonnement Azure.\n",
      "        Groupe de ressources : sélectionnez un groupe de ressources ou créez-en un nouveau.\n",
      "        Région : choisissez n’importe quelle région disponible\n",
      "        Nom : entrez un nom unique.\n",
      "        Niveau tarifaire : F0 gratuit\n",
      "    Examinez et créez la ressource, puis attendez la fin du déploiement. Accédez ensuite à la ressource déployée.\n",
      "    Affichez la page Clés et point de terminaison de votre ressource Détecteur d'anomalies. Vous aurez besoin du point de terminaison et des clés pour vous connecter à partir d’applications clientes.\n",
      "# Lancer le script\n",
      ".\\detect-anomalies.ps1\n",
      "------------------------------------\n",
      "1. Qu’entend-on par données saisonnières ?\n",
      "Des données déterminées en fonction de la date ou de l’année à laquelle elles ont été enregistrées.\n",
      "L’écart par défaut entre les valeurs pour chaque période enregistrée.\n",
      "(accroché) Des données qui se produisent à intervalles réguliers.\n",
      "Correct. Une série chronologique saisonnière est considérée comme un pattern dans les données, qui se produit à intervalles réguliers (par exemple toutes les heures, tous les jours ou tous les mois).\n",
      "2. À quoi sert-il de spécifier la granularité d’un objet de données JSON ?\n",
      "(accroché) Cela permet d’indiquer le pattern d’enregistrement des données.\n",
      "Correct. La granularité est spécifiée ainsi : toutes les heures, tous les jours, toutes les semaines, etc.\n",
      "Elle indique au service comment segmenter les résultats retournés pour révision, indépendamment du pattern des données de série chronologique.\n",
      "Elle permet d’indiquer la plage de valeurs acceptables.\n",
      "3. Comment le service Détecteur d’anomalies évalue-t-il la présence d’anomalies dans des données en temps réel ?\n",
      "Elle collecte toutes les valeurs dans une fenêtre de temps et les évalue en une seule fois.\n",
      "(accroché) Elle évalue la valeur actuelle par rapport à la précédente.\n",
      "Correct. Elle évalue les points de données déjà vus pour déterminer si le dernier constitue une anomalie.\n",
      "Elle utilise une interpolation à partir de la valeur actuelle et de la valeur précédente pour prédire la valeur attendue.\n",
      "------------------------------------\n",
      "1. Qu'est-ce qui est vrai pour les tenseurs ?\n",
      "Les tenseurs sont un type chaîne représentant un vecteur.\n",
      "Les tenseurs sont une valeur mathématique dans Python pour représenter des coordonnées GPS.\n",
      "(accroché) Les tenseurs sont des structures de données spécialisées similaires aux tableaux et aux matrices.\n",
      "------------------------------------\n",
      "1. Quelle est la différence entre un DataSet PyTorch et un DataLoader PyTorch ?\n",
      "Un DataSet est conçu pour travailler avec des lots de données tandis qu'un DataLoader est conçu pour récupérer des éléments de données individuels.\n",
      "Un DataSet est conçu pour récupérer des éléments de données individuels, tandis qu’un DataLoader est conçu pour fonctionner avec des lots de données.\n",
      "Correct, un DataSet est conçu pour récupérer des éléments de données individuels, tandis qu’un DataLoader est conçu pour fonctionner avec des lots de données.\n",
      "La classe DataLoader est le parent de DataSet.\n",
      "La classe DataSet est le parent de DataLoader.\n",
      "2. Les transformations dans PyTorch sont conçues pour :\n",
      "Effectuer une manipulation des données pour qu’elles soient adaptées à l’apprentissage\n",
      "C’est correct.\n",
      "Convertir des tenseurs d’entrée en un tenseur de sortie qui contient une prédiction\n",
      "Convertir des éléments de données en représentations visuelles\n",
      "------------------------------------\n",
      "1.\n",
      "Quelle est la taille du tenseur pour représenter 2 secondes d’un clip vidéo 320x200 à 30 images par seconde ?\n",
      "30x200x320\n",
      "60x200x320\n",
      "Vous devez prendre en compte les couleurs\n",
      "60x3x200x320\n",
      "Très bien !\n",
      "2x200x320\n",
      "------------------------------------\n",
      "1. Vous souhaitez effectuer l’apprentissage d’un modèle de classification du diabète avec la bibliothèque scikit-learn. Vous voulez vous concentrer sur l’expérimentation du modèle et réduire l’effort nécessaire pour journaliser les résultats du modèle. Quelle méthode de journalisation devez-vous utiliser ?\n",
      "Journalisation automatique\n",
      "Correct. Vous pouvez utiliser la journalisation automatique avec scikit-learn. La journalisation automatique permet de réduire l’effort nécessaire pour journaliser les résultats du modèle.\n",
      "Journalisation personnalisée\n",
      "Combinaison de la journalisation automatique et de la journalisation personnalisée.\n",
      "2. Lorsque vous utilisez le suivi MLflow pour l’apprentissage d’un modèle dans un notebook exécuté sur le calcul Azure Machine Learning, sous quel onglet du studio pouvez-vous consulter les résultats du modèle ?\n",
      "Données\n",
      "Modèles\n",
      "Incorrect. L’onglet Modèles indique les modèles inscrits.\n",
      "travaux\n",
      "Correct. Les travaux affichent les exécutions de l’expérience MLflow, y compris toutes les métadonnées et tous les paramètres, métriques et artefacts journalisés.\n",
      "1.\n",
      "Vous envisagez de créer une application qui utilise le service Speech pour transcrire les enregistrements audio d’appels téléphoniques en texte, puis envoie le texte transcrit au service de langage pour en extraire les phrases clés. Vous souhaitez gérer l’accès et la facturation des services d’application dans une seule ressource Azure. Quel type de ressource Azure devez-vous créer ?\n",
      "Speech\n",
      "Incorrect. Cette ressource prend en charge la transcription vocale, mais pas l’extraction de phrases clés.\n",
      "Langage\n",
      "Cognitive Services\n",
      "Correct. Cette ressource prend en charge à la fois les services Speech et les services de langage.\n",
      "2.\n",
      "Vous voulez utiliser le service Speech pour créer une application qui lit l’objet des e-mails entrants à voix haute. Quelle API devez-vous utiliser ?\n",
      "Reconnaissance vocale\n",
      "Synthèse vocale\n",
      "Correct. L’API Synthèse vocale convertit le texte en parole audible.\n",
      "Traduction\n",
      "1.\n",
      "Qu’est-ce qu’un réseau neuronal ?\n",
      "Un système informatique conçu d’après le cerveau et le système nerveux humain.\n",
      "Correct ! Un réseau neuronal est un système informatique conçu d’après le cerveau et le système nerveux humain.\n",
      "Un système complexe de serveurs informatiques interconnectés.\n",
      "Un bundle de logiciels installés sur un ordinateur.\n",
      "2.\n",
      "À quoi fait référence la précision dans un modèle d’intelligence artificielle ?\n",
      "Au pourcentage de temps durant lequel un modèle a effectué une prédiction incorrecte.\n",
      "Au pourcentage de temps durant lequel le modèle a effectué une prédiction correcte.\n",
      "Correct ! Dans ce contexte, la précision fait référence à la fréquence à laquelle le modèle a effectué des prédictions correctes.\n",
      "Au nombre d’images dont vous avez besoin pour effectuer l’apprentissage du modèle afin qu’il soit parfait.\n",
      "3.\n",
      "Comment pouvez-vous augmenter la précision d’un modèle d’intelligence artificielle ?\n",
      "En entraînant le modèle cinq fois de plus.\n",
      "En réduisant le nombre d’époques.\n",
      "En ajoutant des images supplémentaires et en augmentant le nombre d’époques.\n",
      "Bonne réponse ! Vous pouvez augmenter la justesse d’un modèle d’intelligence artificielle en ajoutant des images et des époques lors de l’entraînement du modèle.\n",
      "1.\n",
      "Pourquoi avons-nous choisi un arbre de décision pour notre algorithme Machine Learning ?\n",
      "Un arbre de décision est l’algorithme le plus complexe et le plus précis.\n",
      "Un arbre de décision est facile à visualiser. C’est bien parce que le modèle ne peut effectuer que deux options : oui ou non.\n",
      "Correct ! Nous avons choisi un arbre de décision, car un arbre de décision est facile à visualiser. Il est facile à utiliser car le modèle n’a que deux choix.\n",
      "Un arbre de décision possède un grand nombre de branches, et le modèle peut faire de nombreux choix.\n",
      "2.\n",
      "Quel est l’intérêt de diviser votre jeu de données ?\n",
      "Rendre le modèle plus juste en éliminant les données incorrectes\n",
      "Tester différents algorithmes avec différentes données\n",
      "Disposer de données différentes pour l’entraînement et pour le test de votre modèle\n",
      "Bonne réponse ! Si vous entraînez et testez votre modèle Machine Learning avec les mêmes données, il sera seulement capable de vous indiquer les données dont vous disposez. Il ne permettra pas d’obtenir des prédictions justes.\n",
      "------------------------------------\n",
      "1.\n",
      "Vous envisagez de créer un espace de travail Azure Databricks et d’utiliser la vue de l’entité SQL dans le portail Azure Databricks. Parmi les niveaux tarifaires suivants, lequel pouvez-vous sélectionner ?\n",
      "Enterprise\n",
      "Incorrect. Il n’existe aucun niveau Entreprise pour Azure Databricks.\n",
      "Standard\n",
      "Premium\n",
      "Correct. Le niveau Premium est requis pour l’entité SQL.\n",
      "2.\n",
      "Vous devez utiliser Spark pour traiter des données dans des fichiers, en les préparant pour l’analyse. Quelle vue d’entité devez-vous utiliser dans le portail Azure Databricks ?\n",
      "Science des données et engineering données\n",
      "Correct. L’entité Science des données et d’engineering données est optimisée pour faciliter les tâches d’engineering données telles que le traitement des données.\n",
      "Machine learning\n",
      "SQL\n",
      "3.\n",
      "Vous avez créé un espace de travail Azure Databricks dans lequel vous prévoyez d’utiliser du code dans des notebooks pour traiter les données. Que devez-vous créer dans l’espace de travail ?\n",
      "Un entrepôt SQL\n",
      "Incorrect. Un entrepôt SQL est utilisé pour interroger des données dans des tables.\n",
      "Un cluster Spark\n",
      "Correct. Un cluster Spark est nécessaire pour traiter les données en utilisant du code dans les notebooks.\n",
      "Une machine virtuelle Windows Server\n",
      "1.\n",
      "Vous souhaitez insérer des données de la table Store.Product dans une table existante nommée Sales.Offer. Quelle instruction devez-vous utiliser ?\n",
      "INSERT INTO Sales.Offer SELECT ProductID, Name, Price*0.9 FROM Store.Product;\n",
      "Correct. Utilisez l’instruction INSERT ... SELECT pour insérer les résultats d’une requête dans une table existante.\n",
      "SELECT ProductID, Name, Price*0.9 FROM Store.Product INTO Sales.Offer;\n",
      "INSERT INTO Sales.Offer (ProductID, Name, Price*0.9) VALUES (Store.Product);\n",
      "2.\n",
      "Vous devez déterminer la colonne IDENTITY la plus récemment insérée dans la table Sales.Invoice. Quelle instruction devez-vous utiliser ?\n",
      "SELECT SCOPE_IDENTITY() FROM Sales.Invoice;\n",
      "Incorrect. Utilisez IDENT_CURRENT pour trouver la valeur d’identité actuelle d’une table spécifique.\n",
      "SELECT IDENT_CURRENT(’Sales.Invoice’);\n",
      "Correct. Utilisez IDENT_CURRENT pour trouver la valeur d’identité actuelle d’une table spécifique.\n",
      "SELECT NEXT VALUE FOR Sales.Invoice;\n",
      "3.\n",
      "Vous devez augmenter de 10 % le prix de tous les produits de la catégorie 2. Quelle instruction devez-vous utiliser ?\n",
      "UPDATE Store.Product SET Price = Price * 1.1, Category = 2;\n",
      "Incorrect. Utilisez UPDATE avec une clause WHERE pour mettre à jour des lignes spécifiques.\n",
      "UPDATE Store.Product SET Price = Price * 1.1 WHERE Category = 2;\n",
      "Correct. Utilisez UPDATE avec une clause WHERE pour mettre à jour des lignes spécifiques.\n",
      "SELECT Price * 1.1 INTO Store.Product FROM Store.Product WHERE Category = 2;\n",
      "1.\n",
      "Si les machines Windows et Linux accèdent à un volume Azure NetApp Files, quelle est la meilleure option de protocole de fichier à attribuer au volume ?\n",
      "NFSv4\n",
      "SMB3\n",
      "Incorrect. Les machines Windows peuvent uniquement accéder aux volumes Azure NetApp Files qui utilisent le protocole SMB3. Toutefois, le montage de volumes SMB dans Linux nécessite une configuration plus poussée.\n",
      "NFSv3 et SMB3\n",
      "Correct. Azure NetApp Files prend en charge la création de volumes qui utilisent NFSv3 et SMB3 simultanément. Cette capacité permet aux machines Linux et Windows de monter le volume sans configuration supplémentaire.\n",
      "2.\n",
      "Une application d’entreprise demande un débit de 128 Mio/s et une taille de volume de 2 Tio. Quel niveau de service Azure NetApp Files doit être sélectionné lors de la création du pool de capacités, en supposant que le pool utilise une QoS automatique ?\n",
      "Premium\n",
      "Correct. Le niveau Premium fournit jusqu’à 64 Mio/s de débit par Tio de capacité provisionnée. Ainsi, le provisionnement de 2 Tio pour le volume offre un débit pouvant atteindre 128 Mio/s.\n",
      "Ultra\n",
      "Standard\n",
      "Incorrect. Le niveau Standard fournit un débit allant jusqu’à 16 Mio/s pour 1 Tio de capacité provisionnée. Ainsi, le provisionnement de 2 Tio pour le volume offre un débit pouvant atteindre 32 Mio/s, soit moins que nécessaire.\n",
      "3.\n",
      "Supposons qu’un administrateur VDI qui est responsable de milliers de bureaux virtuels travaille sur une infrastructure. La possibilité de restaurer n’importe quel profil utilisateur à une configuration antérieure constitue un point clé pour l’infrastructure. Cette fonctionnalité est utile pour résoudre les problèmes. L’infrastructure doit également être en mesure de rétablir un volume entier à un état antérieur, ce qui peut aider l’entreprise à surmonter une attaque par ransomware. Laquelle des fonctionnalités Azure NetApp Files suivantes pouvez-vous utiliser à cette fin ?\n",
      "Qualité de service\n",
      "NetApp Cloud Sync\n",
      "Instantanés\n",
      "Correct. Un instantané est une image à un point dans le temps d’un volume qui vous permet de restaurer des fichiers et des répertoires dans un état antérieur et de rétablir l’ensemble du volume à une configuration antérieure.\n",
      "4.\n",
      "Supposons qu’un administrateur informatique a été invité à étudier la viabilité du déplacement des applications d’entreprise de la société vers Azure et de l’utilisation d’Azure NetApp Files pour le stockage partagé des applications. La contrainte principale à laquelle il est confronté est un budget minimal pour effectuer la migration réelle. En raison de cette contrainte, la migration doit être effectuée avec peu ou pas de modifications du code, des données ou de la configuration de l’application. Parmi les options suivantes, laquelle décrit le mieux ce type de migration ?\n",
      "Latence faible\n",
      "Incorrect. La latence est le temps nécessaire pour accéder à un emplacement de stockage spécifique et n’a rien à voir avec la migration de charges de travail vers le cloud.\n",
      "Migration lift-and-shift\n",
      "Correct. Une migration « lift-and-shift » vous permet de faire passer votre charge de travail dans le cloud sans avoir à changer les composants, la configuration ou le code de votre application.\n",
      "Délégation de sous-réseau\n",
      "5.\n",
      "Supposons qu’un administrateur évalue Azure NetApp Files en tant que solution de stockage partagé. Parmi les options suivantes, laquelle décrit le mieux un scénario dans lequel il déconseille d’utiliser Azure NetApp Files ?\n",
      "Une charge de travail à petite échelle et à basses performances qui se compose principalement de contenu statique avec une demande relativement constante\n",
      "Correct. Azure NetApp Files n’est pas idéal pour les petites charges de travail statiques. Azure NetApp Files convient pour les charges de travail à grande échelle et hautes performances avec un contenu et une demande élastiques.\n",
      "Une application de simulation de calcul hautes performances nécessitant peu d’IOPS et une latence faible\n",
      "Incorrect. Azure NetApp Files prend en charge jusqu’à 450 000 IOPS et une latence inférieure à la milliseconde. Il est donc idéal pour ce type d’application.\n",
      "Un déploiement de Microsoft SQL Server massif\n",
      "1.\n",
      "Vous avez été invité à choisir le réseau que vous souhaitez déployer pour votre application. Lesquelles des considérations suivantes sont valables :\n",
      "(accrocheé) Vous choisissez la 5G, car vous avez besoin d’une latence très faible.\n",
      "La 5G avec le système MEC privé Azure offre une faible latence pour les applications.\n",
      "Vous choisissez la technologie Wi-Fi, car vous avez besoin d’une faible latence.\n",
      "Vous choisissez la 5G, car elle est utilisable dans tous les cas.\n",
      "Même lorsque la 5G est entièrement déployée, plusieurs types de réseau sont censés coexister.\n",
      "2.\n",
      "Votre entreprise a décidé d’adopter la technologie 5G pour IoT. Quels sont les avantages de l’utilisation du système MEC privé Azure pour la 5G ?\n",
      "Le système MEC privé Azure peut être utilisé pour la voix et les données.\n",
      "Le système MEC privé Azure peut être utilisé pour les applications complexes comme les voitures autonomes.\n",
      "(accrocheé) Le système MEC privé d’Azure regroupe un portefeuille de services de Microsoft afin d’implémenter des technologies 5G dans l’entreprise.\n",
      "Il réunit un portefeuille de services qui vous aident à déployer un service 5G de bout en bout.\n",
      "3.\n",
      "Vous souhaitez déployer la technologie 5G dans un stade où la couverture du signal doit être possible pour un grand nombre de capteurs. Quelle caractéristique de la 5G serait la plus appropriée ici ?\n",
      "La 5G est sécurisée.\n",
      "La 5G est rapide.\n",
      "La vitesse n’est pas directement pertinente pour la couverture du signal.\n",
      "(accrocheé) La 5G permet une densité de capteurs élevée.\n",
      "La 5G permet une densité des capteurs élevée dans une zone géographique.\n",
      "1.\n",
      "Lorsque l’on combine la sortie de deux ensembles, comment les opérateurs UNION et UNION ALL traitent-ils les valeurs NULL ?\n",
      "Les lignes qui contiennent des valeurs NULL ne sont pas retournées, car les valeurs NULL ne peuvent pas être comparées.\n",
      "Incorrect. Les opérateurs UNION et UNION ALL retournent les lignes qui contiennent des valeurs NULL.\n",
      "Les valeurs NULL représentent des valeurs inconnues, qui ne peuvent pas être comparées. Les opérateurs UNION et UNION ALL retournent une erreur si les ensembles contiennent des valeurs NULL.\n",
      "Une valeur NULL d’un ensemble est traitée comme étant égale à une valeur NULL d’un autre ensemble.\n",
      "Correct. Les valeurs NULL sont comparées et traitées comme des doublons si elles apparaissent dans les deux ensembles.\n",
      "2.\n",
      "Il existe des jeux de résultats de requête employé et client, qui contiennent tous deux des colonnes ID (int) et pays/région (nvarchar(20)). Quelqu’un souhaite retourner la liste des pays/régions qui apparaissent dans les deux jeux de résultats. Quel opérateur d’ensemble doit-il utiliser ?\n",
      "EXCEPT\n",
      "INTERSECT\n",
      "Correct. L’opérateur d’ensemble INTERSECT retourne les lignes qui se trouvent dans les deux jeux de résultats.\n",
      "OUTER APPLY\n",
      "3.\n",
      "Quel type de résultat l’opérateur APPLY retourne-t-il ?\n",
      "Table\n",
      "Correct. APPLY retourne un résultat table plutôt qu’un résultat scalaire ou multivaleur.\n",
      "Scalaire\n",
      "Agrégat\n",
      "1.\n",
      "Parmi les commandes Azure Cloud Shell/Azure CLI suivantes, laquelle permet de créer un compte Stockage Azure ?\n",
      "az group create\n",
      "az account create\n",
      "az storage account create\n",
      "Correct ! Cette commande crée un compte de stockage.\n",
      "az storage create\n",
      "2.\n",
      "Quelle est la différence entre la précision et le rappel quand vous mesurez la justesse d’un modèle Machine Learning ?\n",
      "La précision mesure approximativement les faux positifs. La précision mesure approximativement les faux négatifs.\n",
      "Correct ! Techniquement, la précision correspond au (nombre de vrais positifs)/(nombre de vrais positifs + nombre de faux positifs), et le rappel correspond au (nombre de vrais positifs)/(nombre de vrais positifs + nombre de faux négatifs).\n",
      "La précision mesure approximativement les faux négatifs. La précision mesure approximativement les faux positifs.\n",
      "La précision correspond à la moyenne des taux de faux positifs calculés dans une plage de seuils.\n",
      "Le rappel correspond à la moyenne des taux de faux positifs calculés dans une plage de seuils.\n",
      "Incorrect. La précision et le rappel sont un peu plus simples que cela.\n",
      "3.\n",
      "Lorsque vous déployez une application de fonction, pourquoi est-il préférable de choisir le plan App Service plutôt que le plan Consommation ?\n",
      "Les applications de fonction qui sont hébergées dans un plan App Service prennent en charge l’exécution en parallèle. Les applications de fonction qui sont hébergées dans le plan Consommation ne prennent pas en charge l’exécution en parallèle.\n",
      "Les plans App Service qui hébergent des applications dans Azure Functions ne sont pas facturés.\n",
      "Les fonctions qui sont hébergées dans un plan App Service sont facturées uniquement quand elles sont exécutées.\n",
      "Incorrect. En fait, cet énoncé s’applique au modèle Consommation. Le modèle App Service exécute continuellement une instance de calcul pour prendre en charge vos applications de fonction.\n",
      "Les applications de fonction qui sont hébergées dans un plan App Service s’exécutent dès qu’elles sont déclenchées.\n",
      "Correct. Lors de l’utilisation du modèle Consommation, il peut s’avérer nécessaire de faire tourner une machine pendant quelques minutes en vue d’exécuter votre fonction dès son déclenchement. Avec le plan App Service, l’application de fonction s’exécute immédiatement.\n",
      "4.\n",
      "Quand vous déployez une base de données SQL dans Azure SQL Database, pourquoi faut-il ajouter une adresse IP client à la stratégie de sécurité ?\n",
      "Pour permettre à d’autres services Azure de se connecter à la base de données SQL.\n",
      "Pour permettre aux applications s’exécutant sur votre ordinateur de se connecter à la base de données\n",
      "Correct ! Votre adresse IP client est l’adresse IP de votre propre ordinateur. Le fait de l’ajouter aux règles de sécurité de la base de données permet à votre ordinateur de s’y connecter.\n",
      "Pour permettre à Microsoft de connaître l’adresse IP de votre ordinateur dans le cadre des mises à jour Windows.\n",
      "5.\n",
      "Sur quels systèmes d’exploitation Power BI Desktop s’exécute-t-il ?\n",
      "Windows uniquement\n",
      "Correct ! Power BI Desktop s’exécute uniquement sur Windows, mais vous pouvez consulter des rapports Power BI sur n’importe quelle plateforme.\n",
      "Windows et macOS\n",
      "Windows, macOS et Linux\n",
      "Incorrect. Power BI Desktop s’exécute uniquement sur Windows, mais vous pouvez consulter des rapports Power BI sur n’importe quelle plateforme.\n",
      "macOS et Linux\n",
      "1.\n",
      "Qu’est-il nécessaire de spécifier avec la fonction RANK ?\n",
      "PARTITION BY\n",
      "CURRENT ROW\n",
      "ORDER BY\n",
      "Correct. ORDER BY doit être spécifié avec la fonction RANK.\n",
      "2.\n",
      "Dans quel ordre les mots clés ROW, OVER et PARTITION BY se produisent-ils dans une requête ?\n",
      "PARTITION BY, OVER, ROW\n",
      "Ce n’est pas correct. OVER est la clause. PARTITION BY et ROW ou RANGE sont les arguments.\n",
      "ROW, OVER, PARTITION BY\n",
      "OVER PARTITION BY, ROW\n",
      "Correct. OVER est la clause. PARTITION BY et ROW ou RANGE sont les arguments.\n",
      "3.\n",
      "Quel type de fonction de fenêtre est PERCENT_RANK() ?\n",
      "Fonction analytique\n",
      "1.\n",
      "Quel est l’objectif de l’entraînement d’un modèle ?\n",
      "L’objectif est de le rendre plus rapide.\n",
      "L’objectif est de trouver les paramètres W et B qui donneront la meilleure prédiction.\n",
      "Correct !\n",
      "L’objectif est de trouver le meilleur optimiseur et la meilleure fonction de perte pour nos besoins.\n",
      "L’objectif est de classifier chacun des exemples d’entrée.\n",
      "Correct. PERCENT_RANK est similaire à la fonction CUME_DIST. Elle est classée comme étant une fonction analytique.\n",
      "Fonction de classement\n",
      "Fonction d’agrégation\n",
      "------------------------------------\n",
      "1.\n",
      "Lors de l’entraînement de la tâche de classification d’images sur les images avec la forme 32x32x3, quelle forme est l’entrée pour la couche neuronale Dense ?\n",
      "32x32x3\n",
      "1 024\n",
      "La dimension du tenseur d’entrée doit correspondre au nombre d’entrées disponibles, par exemple 32323\n",
      "3 072\n",
      "Correct, le tenseur d’entrée est juste aplati en un vecteur\n",
      "2.\n",
      "Notre jeu de données contient des images de taille 32x32x3. Que devrions-nous faire pour entraîner un réseau neuronal Dense sur ces images ?\n",
      "Utiliser la couche Flatten comme première couche du réseau pour reformer les images\n",
      "Changer la forme des éléments du jeu de données d’entraînement pour qu’ils soient des vecteurs de longueur 3072 et utiliser un réseau Dense d’une seule couche\n",
      "N’importe laquelle des solutions ci-dessus fonctionnera\n",
      "Correct, ces deux solutions sont valides.\n",
      "3.\n",
      "Nous souhaitons surveiller la justesse du modèle sur le jeu de données de validation au cours de l’entraînement. Que devons-nous faire ?\n",
      "Spécifier metrics=['acc'] dans un appel à model.compile\n",
      "Spécifier metrics=['acc'] dans un appel à model.fit\n",
      "Fournir un jeu de données de validation avec le paramètre validation_data dans model.fit\n",
      "Ce n’est pas tout\n",
      "Options (a) et (c)\n",
      "Correct, vous devez vous assurer que la métrique est spécifiée et que le jeu de données de validation est fourni\n",
      "Options (b) et (c)\n",
      "------------------------------------\n",
      "1.\n",
      "Vous disposez d’une trame de données ou d’un tibble, avec la dimension 20, 2. Qu’est-ce que cela vous dit à son sujet ?\n",
      "Le tibble est composé de 20 lignes et de 2 colonnes.\n",
      "Correct ! Une dimension de 20, 2 signifie que le tibble a 2 colonnes, chacune avec 20 éléments. Cela lui donne une structure rectangulaire de 20 lignes et 2 colonnes.\n",
      "Le tibble contient 2 colonnes, avec les valeurs 2 et 20.\n",
      "Le tibble contient 20 lignes, toutes avec la valeur 2.\n",
      "2.\n",
      "Vous disposez d’une trame de données nommée df_sales, qui contient des données de ventes quotidiennes. La trame de données affiche les colonnes year, month, day_of_month et sales_total. Vous souhaitez trouver la valeur moyenne de sales_total. Quel code devez-vous utiliser ?\n",
      "df_sales %>% pull(sales_total) %>% avg()\n",
      "mean(sales_total$df_sales)\n",
      "mean(df_sales$sales_total)\n",
      "Correct ! L’accesseur ($) extrait les éléments de la colonne sales_total de la trame de données df_sales. Les éléments sont ensuite transmis à la fonction mean(), qui calcule la moyenne.\n",
      "3.\n",
      "Vous disposez d’une trame de données qui affiche des données sur les ventes quotidiennes de crème glacée. Vous utilisez la fonction cor de R pour comparer les colonnes avg_temp et units_sold, et obtenez un résultat de 0,97. Qu’est-ce que ce résultat indique ?\n",
      "Le jour où la valeur maximale de units_sold a été atteinte, la valeur de avg_temp était de 0,97.\n",
      "Les jours avec des valeurs avg_temp élevées ont tendance à coïncider avec les jours où les valeurs de units_sold sont également élevées.\n",
      "Correct ! La fonction cor retourne la corrélation et une valeur proche de 1 indique une corrélation positive.\n",
      "La valeur units_sold est, en moyenne, égale à 97 pour cent de la valeur avg_temp.\n",
      "------------------------------------\n",
      "Qu'est-ce qui se trouve sont sur les axes x et y d'un tracé ROC?\n",
      "(accroché) axe des x : taux FP, axe des y : taux TP\n",
      "axe des x : nombre de FP, axes dees y : nombre de TP\n",
      "axe des x : nombre de TP, axes dees y : nombre de FP\n",
      "------------------------------------\n",
      "Qu'est-ce que la zone sous la courbe pour un tracé ROC nous indique?\n",
      "Le fonctionnement du modele à son seuil de décision optimal\n",
      "Quel est le seuil de décision optimal?\n",
      "(accroche) Elle fournit un résumé sur la façon dont on modele fonctionne sur\n",
      "------------------------------------\n",
      "Les résultats de l'API OCR se décomposent par région, par ligne, puis par mot, tandis que les résultants de l'API Read sont décomposés par page, ligne, puis mot.\n",
      "------------------------------------\n",
      "La classe de base pour tous les modules de réseau neuronal dans PyTorch est torch.nn.Module\n",
      "(accroche) Vrai\n",
      "Faux\n",
      "------------------------------------\n",
      "Vous devez régénér la clé d'abonnement principle pour une ressource Cognitive Services utilisée par une application. Que devez-vous faire en premier pour éviter les interruptions de service de l'application?\n",
      "(accrocher) Modifier l'application pour qu'elle utilise la clé secondaire\n",
      "Modifier le point de terminaison de la ressource\n",
      "------------------------------------\n",
      "Vous souhaitez stocker de maniere sécurisée les clés d'abonnement d'une ressource Cognitive Services, afin que les applications autorisées puissent les récupérer en cas de besoin. Quel type de ressource Azure devez-vous approvisionner?\n",
      "Stockage Azure\n",
      "(accroché) Azure Key Vault\n",
      "------------------------------------\n",
      "Azure Databricks offre des types des runtimes:\n",
      "0. Databricks Runtime : comprend Apache Spark, des composants\n",
      "------------------------------------\n",
      "Des services d'Azure:\n",
      "0. Azure App Service : créer vos front-ends des site web\n",
      "1. Azure Functions : créer une logique d'application pilotée par les événements qui s'exécute uniquement lorsque vous en avez besoin\n",
      "2. Azure Virtual Desktop : fournir rapidement un systeme d'exploitation et un environment logiciel personnalisés. Un service de virtualisation des postes de travail et des applications qui s'exécute dans le cloud.\n",
      "Avec l'informatique serverless, Azure gère l'infrastructure serveur ainsi que l'allocation et désallocation des resources en fonction de la demande.\n",
      "------------------------------------\n",
      "Vous avez publié votre application Conversational Language Understanding. De quelles informations un développeur d'applications clientes a besoin pour obenir des prédictions?\n",
      "(accroché) Le point de terminaison et la clé pour la ressource de prédiction de l'application\n",
      "------------------------------------\n",
      "Vous devez provisionner une ressource Azure qui sera utilisée pour créer une nouvelle application Language Understanding. Quel type de ressource Azure devez-vous créer?\n",
      "Service de language personnalisé\n",
      "(accroché) Service Language\n",
      "Cognitive Services\n",
      "------------------------------------\n",
      "Vous créez une application Conversational Language Understanding pour gérer une horloge internationale. Vous voulez que les utilisateurs puissent demander l'heure actuelle dans une ville spécifiée, par exemple 'Quelle heure est-il à Londres?'. Que devez-vous faire?\n",
      "(accroché) Définir une entité 'ville' et une intention 'ObtenirHeure' avec des énoncés qui indiquent l'intention 'ville'\n",
      "------------------------------------\n",
      "Parmi les services suivants, lesquels doivent être utilisés quand la principale préoccupation est d'effectuer un travail en résponse à un événement (souvent via une command REST) qui necessite une réponse en quelques secondes?\n",
      "(accroché) Azure Functions\n",
      "------------------------------------\n",
      "Votre entreprise dispose d'une équipe de télétravailleurs qui doivent utiliser des logiciels Windows pour développer les appplications de l'entreprise, mais les membres de votre equipe utilisent differents systems d'exploitation tels que macOS, Linux, et Windows. Quel service de calcul Azure peut être utile dans ce scénairo?\n",
      "Azure App Service\n",
      "(accroché) Azure Virtual Desktop\n",
      "------------------------------------\n",
      "Quelles sont les ressources de calcul Azure qui peuvent être déployées pour gérer un groupe des machines virtuelles identiques?\n",
      "(accroché) Groupes identiques de machines virtuelles\n",
      "Groupes à haute disponibilite de machines virtuelles\n",
      "Zone de disponibilité\n",
      "------------------------------------\n",
      "1.\n",
      "Quel outil peut être utilisé pour la planification agile lors de l’utilisation d’Azure DevOps ?\n",
      "Azure Boards\n",
      "Correct. Utilisez Azure Boards pour le suivi des éléments de travail, la visualisation et la création de rapports.\n",
      "Azure Repos\n",
      "Azure Pipelines\n",
      "2.\n",
      "Quelle activité peut faire partie de l’intégration continue ?\n",
      "Supervision\n",
      "Vérification lint\n",
      "Correct. Avec le linting, vous pouvez vérifier les erreurs programmatiques ou stylistiques dans vos scripts.\n",
      "Planification\n",
      "------------------------------------\n",
      "1. Qu’advient-il des transactions imbriquées lors de la restauration de la transaction externe ?\n",
      "La transaction interne est exécutée.\n",
      "La transaction interne empêche la restauration de la transaction externe. Les transactions interne et externe sont exécutées.\n",
      "Ce n’est pas correct. Si la transaction externe est restaurée, la transaction interne n’est pas exécutée. La transaction interne n’empêchera pas la restauration de la transaction externe.\n",
      "La transaction interne est également restaurée.\n",
      "Correct. Toutes les transactions internes sont restaurées lorsque la transaction externe est restaurée.\n",
      "2. Lequel des mots clés T-SQL suivants est utilisé pour contrôler les transactions ?\n",
      "BEGIN TRANSACTION\n",
      "Correct. Les mots clés BEGIN TRANSACTION sont utilisés pour démarrer une transaction explicite.\n",
      "BEGIN TRY\n",
      "BREAK\n",
      "3. Que teste XACT_STATE ?\n",
      "Indique s’il existe des transactions imbriquées.\n",
      "État de la requête actuelle.\n",
      "Correct. XACT_STATE teste l’état de la requête actuelle.\n",
      "Renvoie le numéro de l’erreur qui a provoqué l’échec de la transaction.\n",
      "4. Quel est le niveau d’isolation de transaction par défaut pour Azure SQL Database ?\n",
      "READ_COMMITTED_SNAPSHOT_ON\n",
      "Correct. READ_COMMITTED_SNAPSHOT_ON est le niveau d’isolation de transaction par défaut pour Azure SQL Database.\n",
      "READ_COMMITTED_SNAPSHOT_OFF\n",
      "XACT_ABORT\n",
      "------------------------------------\n",
      "1.\n",
      "Quelle est la première chose qu’un scientifique des données doit faire lorsqu’un élément de travail lui est attribué pour améliorer un modèle (qui est déjà en production) ?\n",
      "Créer un dépôt.\n",
      "Créez un problème.\n",
      "Créer une branche dans le dépôt.\n",
      "Correct. En fonction d’un élément de travail ou d’un problème attribué, un scientifique des données doit créer une branche.\n",
      "2.\n",
      "Quand un scientifique des données doit-il commiter quand il travaille en local dans Visual Studio Code ?\n",
      "Une fois que toutes les modifications ont été apportées.\n",
      "Après un petit changement.\n",
      "Correct. Il est recommandé de commiter de petites modifications et de le faire souvent.\n",
      "Pour charger toutes les modifications apportées au dépôt principal.\n",
      "------------------------------------\n",
      "1.\n",
      "Parmi les éléments suivants, lequel a été utilisé pour créer un mécanisme de chargement des données d’image dans un compte de stockage Azure en tant que données blob ?\n",
      "Création d’une table dans le compte de stockage Azure\n",
      "Création d’un conteneur dans le compte de stockage Azure\n",
      "Correct. Un conteneur a été utilisé pour charger des données d’image dans un compte de stockage Azure en tant que données blob.\n",
      "Création d’un partage de fichiers dans le compte de stockage Azure.\n",
      "2.\n",
      "Quel est l’objectif de l’instance de calcul créée dans notre instance Azure Machine Learning Studio ?\n",
      "Pour fournir à un environnement de développement la possibilité d’exécuter du code Python et d’exécuter des notebooks Jupyter en direct\n",
      "Correct. L’instance de calcul nous permettra de développer et d’exécuter du code personnalisé à partir d’un ordinateur préconfiguré accessible dans le cloud.\n",
      "Pour nous permettre de stocker et de récupérer des données d’image à des fins de formation et de validation\n",
      "Pour permettre la gestion des workflows de Machine Learning\n",
      "3.\n",
      "Quelles valeurs sont requises pour créer un magasin de données Azure Machine Learning de type Azure Blob Storage ?\n",
      "Uniquement le nom du compte de stockage et le nom du conteneur d’objets Blob\n",
      "Incorrect. La clé de compte (accès) ou un jeton SAS valide est également requis pour créer un magasin de données Azure Machine Learning de type Azure Blob Storage.\n",
      "Nom du compte de stockage, nom du partage de fichiers et clé du compte (accès).\n",
      "Nom du compte de stockage, nom du conteneur d’objets blob et clé du compte (accès).\n",
      "Correct. Ces valeurs sont requises pour créer un magasin de données Azure Machine Learning de type Azure Blob Storage.\n",
      "------------------------------------\n",
      "1.\n",
      "Dans ce module, nos données d’image ont été référencées avec un magasin de données attaché à un...\n",
      "Stockage Blob Azure dans un compte de stockage Azure\n",
      "Correct. Une ressource de Stockage Blob Azure existante a été référencée par notre magasin de données.\n",
      "Pipeline de données Azure Synapse Analytics\n",
      "Partage de fichiers dans un compte de stockage Azure.\n",
      "2.\n",
      "L’étiquetage assisté par ML n’était pas nécessaire dans ce module, quel est son intérêt dans un projet d’étiquetage de données ?\n",
      "L’étiquetage assisté par ML crée un modèle pouvant pré-étiqueter les données dans de nouveaux échantillons d’images dont l’exactitude peut être vérifiée par la suite par les membres de l’équipe.\n",
      "Correct. Cette fonctionnalité est un grand avantage qui peut réduire la durée d’étiquetage des données d’image dans les nouveaux échantillons.\n",
      "L’étiquetage assistée par ML peut déterminer des objets dans une image à partir d’un modèle existant qui détecte divers objets communs.\n",
      "Vous permet d’envoyer des données sans étiquette pour qu’elles soient étiquetées par un service tiers utilisant des personnes pour faire l’étiquetage.\n",
      "Incorrect. Le comportement décrit n’est pas une fonctionnalité de l’étiquetage assisté par ML.\n",
      "3.\n",
      "La raison pour laquelle nous avons étiqueté 10 images dans notre jeu de données est de répondre aux conditions suivantes :\n",
      "10 est le nombre total d’images qui composent notre jeu de données\n",
      "10 est le nombre maximal d’images pouvant être utilisées pour entraîner un modèle de détection d’objets.\n",
      "Les expériences de détection d’objets Azure ML Studio demandent un minimum de 10 échantillons étiquetés pour réussir à entraîner notre modèle de détection d’objets\n",
      "Correct. Si nous fournissons moins d’images, nous recevons une erreur pendant la tentative d’entraînement d’un modèle de détection d’objets à partir de notre jeu de données exporté.\n",
      "1.\n",
      "Le Kit de développement logiciel (SDK) Python AzureML nous a permis de nous connecter et d’interagir avec notre espace de travail Azure Machine Learning studio par ...\n",
      "Fourniture d’un fichier config.json qui a spécifié les détails à connecter à l’espace de travail approprié.\n",
      "Correct. Le fichier config.json a pour valeur subscription_id, resource_group et workspace_name de l’espace de travail Machine Learning Studio.\n",
      "Fournir une chaîne de connexion dans une cellule de connexion dans le bloc-notes Jupyter.\n",
      "Utilisation de nos informations d’identification Azure qui ont été transmises de manière transparente lors de l’accès au notebook Jupyter.\n",
      "2.\n",
      "Le Kit de développement logiciel (SDK) Python AzureML nous a permis de provisionner une instance de calcul dans notre espace de travail Azure Machine Learning Studio. Cette instance peut ensuite être référencée dans le code Python pour envoyer des tâches AutoML à exécuter sur cet ordinateur en tant que cible.\n",
      "Vrai\n",
      "Correct. Le Kit de développement logiciel (SDK) Python AzureML peut automatiser différentes tâches, notamment l’approvisionnement d’une instance de calcul et l’envoi d’expériences.\n",
      "Faux\n",
      "3.\n",
      "Azure AutoML vous permet d’ajuster les hyperparamètres pour découvrir automatiquement les paramètres optimaux de votre modèle.\n",
      "Faux\n",
      "Vrai\n",
      "Correct. Azure AutoML peut utiliser des valeurs d’hyperparamètre par défaut ou permettre un balayage de différentes valeurs d’hyperparamètre pour découvrir un paramètre optimal via l’itération.\n",
      "1. Supposons qu’un utilisateur exécute ce script de lot T-SQL pour insérer trois lignes. Après exécution, il obtient une erreur. Combien de lignes sont créées ? INSERT INTO HumanResources.PossibleSkills (SkillName, Category, Credit) VALUES('Database Administration', 'IT Professional', 5);INSERT INTO HumanResources.PossibleSkills (SkillName, Category, Credit) VALUES('C#.NET', 'Developer', 4);INSERT INTO HumanResources.PossibleSkills (SkillName, Category, Credit) VALUES('Project Management', 'Management', 'Two');GO\n",
      "0\n",
      "1\n",
      "(accroche) 2\n",
      "Correct. Deux lignes sont créées. Il n’y a aucune erreur de syntaxe dans ce lot. SQL peut donc commencer à exécuter l’instruction. Les deux premières instructions INSERT sont exécutées avant que l’erreur ne se produise.\n",
      "2. Un utilisateur souhaite remplir une table en créant 15 lignes supplémentaires. Avant de créer les lignes, il doit vérifier que la table existe. Parmi les mots clés T-SQL suivants, lequel doit-il utiliser ?\n",
      "IF\n",
      "Correct. Il utilise l’instruction IF pour vérifier que la table existe avant de commencer à insérer les lignes.\n",
      "WHILE\n",
      "INSERT\n",
      "Incorrect. L’instruction INSERT est utilisée pour insérer les 15 lignes dans la table.\n",
      "3. Une variable peut-elle être déclarée dans un lot et référencée dans plusieurs lots ?\n",
      "Oui\n",
      "Incorrect. Les variables sont locales dans l’étendue du lot où elles sont déclarées.\n",
      "Non\n",
      "Correct. Les variables sont locales dans l’étendue du lot où elles sont déclarées.\n",
      "Parfois\n",
      "------------------------------------\n",
      "1. Un scientifique des données souhaite expérimenter en effectuant l’apprentissage d’un modèle Machine Learning et en le suivant avec Azure Machine Learning. Quel outil doit être utilisé pour effectuer l’apprentissage du modèle en exécutant un script dans son environnement de prédilection ?\n",
      "Azure Machine Learning studio\n",
      "Le kit de développement logiciel (SDK) Python.\n",
      "Correct. Le scientifique des données connaît probablement déjà Python et peut facilement utiliser le kit SDK Python pour exécuter le script d’apprentissage.\n",
      "l’interface de ligne de commande Azure.\n",
      "2. Un modèle Machine Learning permettant de prédire les ventes a été développé. Chaque semaine, de nouvelles données de ventes sont ingérées. Le modèle doit alors faire l’objet d’un nouvel apprentissage sur les dernières données pour pouvoir générer la nouvelle prévision. Quel outil doit être utilisé pour effectuer un nouvel apprentissage du modèle chaque semaine ?\n",
      "Azure Machine Learning studio\n",
      "Le kit de développement logiciel (SDK) Python.\n",
      "Incorrect. Bien que le kit SDK Python puisse être utilisé, Azure CLI offre davantage d’options pour déclencher et automatiser des travaux Azure Machine Learning.\n",
      "l’interface de ligne de commande Azure.\n",
      "Correct. L’interface Azure CLI est conçue pour automatiser les tâches. Si vous utilisez des fichiers YAML pour définir le mode d’apprentissage du modèle, les tâches de Machine Learning s’avèrent reproductibles, cohérentes et fiables.\n",
      "     \n",
      "------------------------------------\n",
      "1. Un scientifique des données doit avoir accès à l’espace de travail Azure Machine Learning pour exécuter un script en tant que travail. Quel rôle doit être utilisé pour donner au scientifique des données l’accès nécessaire à l’espace de travail ?\n",
      "Lecteur.\n",
      "Scientifique des données AzureML.\n",
      "Correct. Un scientifique des données AzureML est autorisé à envoyer un travail.\n",
      "Opérateur de calcul AzureML.\n",
      "Incorrect. Un scientifique des données AzureML est autorisé à envoyer un travail.\n",
      "2. Le scientifique des données souhaite exécuter un script unique pour effectuer l’apprentissage d’un modèle. Quel type de travail est le mieux adapté pour exécuter un script unique ?\n",
      "Commande\n",
      "Correct. Un travail de commande permet d’exécuter un script unique.\n",
      "Pipeline\n",
      "Sweep\n",
      "1. Supposons que vous voulez exécuter une appliance réseau sur une machine virtuelle. Quelle option de charge de travail devez-vous choisir ?\n",
      "Usage général\n",
      "Optimisé pour le calcul\n",
      "Les machines virtuelles optimisées pour le calcul sont conçues pour avoir un ratio processeur/mémoire élevé. Convient pour les serveurs web avec un trafic moyen, les appliances réseau, les processus de traitement par lots et les serveurs d’application.\n",
      "À mémoire optimisée\n",
      "Optimisé pour le stockage\n",
      "2. Vrai ou faux : Les modèles Resource Manager sont des fichiers JSON ?\n",
      "Vrai\n",
      "Les modèles Azure Resource Manager sont des fichiers JSON qui définissent les ressources nécessaires au déploiement de votre solution. Le modèle peut ensuite être utilisé pour recréer facilement plusieurs versions de votre infrastructure, comme des versions de préproduction et de production.\n",
      "Faux\n",
      "1.\n",
      "Combien d’espaces de travail Azure Machine Learning l’équipe doit-elle créer ?\n",
      "Un\n",
      "Correct. Avec une si petite équipe, un seul espace de travail suffit.\n",
      "Deux\n",
      "Trois\n",
      "2.\n",
      "Quand devons-nous réentraîner le modèle ?\n",
      "Toutes les semaines.\n",
      "Lorsque les métriques du modèle sont inférieures au point de référence.\n",
      "Correct. La chose la plus importante est que le modèle fonctionne comme prévu. Lorsque les performances du modèle sont compromises, nous devons réentraîner le modèle.\n",
      "En cas de dérive de données.\n",
      "Incorrect. Il est indiqué que les nouvelles données ne sont pas considérées comme fiables. Actuellement, nous ne devons pas nous appuyer sur la surveillance des données pour déterminer quand réentraîner le modèle.\n",
      "1.\n",
      "Un scientifique des données souhaite lire les données stockées dans un référentiel GitHub accessible au public. Les données seront lues dans un notebook Jupyter dans l’espace de travail Azure Machine Learning pour une expérimentation rapide. Quel protocole doit être utilisé pour lire les données dans le notebook ?\n",
      "azureml\n",
      "http(s)\n",
      "Correct. Ce protocole est utilisé lors de l’accès aux données stockées dans un emplacement http(s) accessible au public.\n",
      "abfs(s)\n",
      "2.\n",
      "Quel type de ressource de données un utilisateur doit-il créer lorsque le schéma change fréquemment et que les données sont utilisées dans de nombreux travaux différents ?\n",
      "Fichier URI\n",
      "Dossier URI\n",
      "MLTable\n",
      "Correct. MLTable est idéal lorsque le schéma change fréquemment. Ensuite, il vous suffit d’apporter des modifications dans un seul emplacement au lieu de plusieurs.\n",
      "1.\n",
      "Un scientifique des données souhaite effectuer l'apprentissage d’un modèle Machine Learning pour prédire les ventes d’articles de supermarché afin d’ajuster l’offre à la demande projetée. Quel type de tâche Machine Learning le modèle effectuera-t-il ?\n",
      "Classification.\n",
      "régression ;\n",
      "Incorrect. La régression prédit une valeur numérique.\n",
      "Prévision de série chronologique\n",
      "Correct. La prévision de séries chronologiques est utilisée pour prédire les ventes futures.\n",
      "2.\n",
      "Le scientifique des données a reçu des données pour effectuer l'apprentissage d’un modèle afin de prédire les ventes d’articles de supermarché. Le scientifique des données souhaite itérer rapidement sur plusieurs options de caractérisation et d’algorithmes en fournissant uniquement les données et en modifiant certaines configurations. Quel outil serait le mieux utilisé dans cette situation ?\n",
      "Concepteur\n",
      "Machine Learning automatisé\n",
      "Correct. Vous n’aurez qu’à fournir les données et le Machine Learning automatisé itérera sur différents algorithmes et approches de caractérisation.\n",
      "Cognitive Services\n",
      "1.\n",
      "Pour déclencher un pipeline Azure Machine Learning avec l’interface CLI (v2), un ingénieur Machine Learning utilise GitHub Actions. Dans quel répertoire le workflow doit-il être stocké ?\n",
      ".github/workflows/\n",
      "Correct. GitHub reconnaît les workflows de ce répertoire à utiliser pour des actions.\n",
      ".gitignore/\n",
      ".pipelines/\n",
      "Incorrect. Stockez les fichiers YAML pour Azure Pipelines dans ce répertoire lorsque vous utilisez Azure DevOps.\n",
      "2.\n",
      "Un pipeline Azure Machine Learning pour entraîner un modèle est déclenché avec Azure Pipelines. Tous les travaux dans Azure Pipelines ont été correctement exécutés, mais aucun nouveau modèle n’est trouvé dans les sorties spécifiées. Où le scientifique des données doit-il aller pour trouver le message d’erreur ?\n",
      "Onglet Pipelines dans Azure DevOps.\n",
      "Onglet Expériences dans Azure Machine Learning.\n",
      "Correct. Les messages d’erreur sont stockés avec la sortie d’une exécution d’expérience.\n",
      "Onglet Modèles dans Azure Machine Learning.\n",
      "Incorrect. Seuls les modèles entraînés et inscrits sont visibles sous l’onglet Modèles.\n",
      "1.\n",
      "Quelle est la première étape à effectuer pour partager un fichier image en tant qu’objet blob dans le Stockage Azure ?\n",
      "Créer un conteneur Stockage Azure pour stocker l’image\n",
      "Création d’un compte Azure Storage.\n",
      "C’est correct. Vous devez créer un compte de stockage Azure avant de pouvoir utiliser les fonctionnalités du Stockage Azure.\n",
      "Charger le fichier image et créer un conteneur\n",
      "Utiliser un jeton de signature d’accès partagé (SAS) pour restreindre l’accès à l’image\n",
      "2.\n",
      "Quelle option du Stockage Azure convient le mieux au stockage des données à des fins de sauvegarde et de restauration, de reprise d’activité après sinistre et d’archivage ?\n",
      "Stockage Fichier Azure\n",
      "Ce n’est pas correct. Le Stockage Blob Azure constitue la meilleure option pour stocker les fichiers et les archives de reprise d’activité.\n",
      "Stockage sur disque Azure\n",
      "Stockage Blob Azure\n",
      "C’est correct. Le Stockage Blob Azure constitue la meilleure option pour stocker les fichiers et les archives de reprise d’activité.\n",
      "1.\n",
      "Quelle est la première étape que nous vous recommandons d’effectuer pour analyser des journaux dans Journaux Azure Monitor ?\n",
      "Écrire une requête KQL pour récupérer toutes les données pertinentes.\n",
      "Vous devez identifier les données pertinentes pour votre analyse avant d’utiliser KQL pour extraire les données.\n",
      "Examiner attentivement toutes les données de journaux dans votre espace de travail Log Analytics.\n",
      "Définir des objectifs d’analyse et évaluer les tables de votre espace de travail Log Analytics qui contiennent des données pertinentes pour votre analyse.\n",
      "Les données de journaux brutes contiennent une quantité considérable d’informations qui sont souvent difficiles à comprendre. Une première étape plus efficace pour analyser les journaux dans Journaux Azure Monitor consiste à rechercher la table qui contient des données spécifiques en fonction d’objectifs d’analyse.\n",
      "------------------------\n",
      "2.\n",
      "Que pouvez-vous faire pour vous familiariser avec les données de journaux brutes que vous avez collectées dans une table ?\n",
      "Exécuter des requêtes simples, comme take 10 et distinct <column name>, sur une table spécifique ou utiliser l’interface utilisateur Log Analytics pour lancer des recherches dans les données, les filtrer et les parcourir.\n",
      "Des requêtes simples, comme take 10 et distinct <column name>, vous permettent de vous concentrer sur de plus petites quantités de données et de filtrer les valeurs répétées, ce qui peut être utile pour évaluer des données brutes.\n",
      "Utiliser la fonction d’agrégation make_set() pour regrouper toutes les données de la table et tout voir au même endroit.\n",
      "Exporter les données vers Excel ou un outil externe.\n",
      "------------------------\n",
      "3.\n",
      "Comment pouvez-vous enrichir les résultats d’une requête avec des données supplémentaires qui ne sont pas disponibles dans la table que vous avez interrogée dans Log Analytics ?\n",
      "Utiliser l’opérateur KQL join pour récupérer des informations d’une autre table.\n",
      "L’opérateur KQL join vous permet de mettre en corrélation les données de deux tables ayant une colonne en commun.\n",
      "Utiliser l’opérateur KQL import pour importer les données à partir d’un autre emplacement.\n",
      "Ajouter une colonne aux résultats de la requête et remplir manuellement les données manquantes.\n",
      "------------------------\n",
      "1.\n",
      "Vous travaillez dans Visual Studio Code. Vous avez cloné un dépôt GitHub dans Visual Studio Code et vous modifiez le code dans un notebook Jupyter. Pour tester le code, vous souhaitez exécuter une cellule dans le notebook. Quelle calcul devez-vous utiliser ?\n",
      "Instance de calcul\n",
      "Correct. Vous pouvez utiliser une instance de calcul pour exécuter un notebook dans Visual Studio Code.\n",
      "Cluster de calcul\n",
      "Cluster Azure Databricks\n",
      "------------------------\n",
      "2.\n",
      "Vous expérimentez avec des pipelines basés sur des composants dans le concepteur. Vous souhaitez itérer et expérimenter rapidement, car vous essayez différentes configurations d’un pipeline. Vous utilisez un cluster de calcul. Pour réduire le temps de démarrage chaque fois que vous envoyez un pipeline, quel paramètre devez-vous modifier ?\n",
      "Taille de calcul\n",
      "Incorrect. Vous ne devez pas modifier la taille de calcul lorsque vous souhaitez que le cluster de calcul reste disponible entre les exécutions de travaux de pipeline.\n",
      "Temps d’inactivité avant scale-down\n",
      "Correct. En augmentant le temps d’inactivité avant un scale-down, vous pouvez exécuter plusieurs pipelines consécutivement sans que le cluster de calcul soit redimensionné à zéro nœud entre les travaux.\n",
      "Nombre maximal d'instances\n",
      "------------------------\n",
      "1.\n",
      "Lucas a formé un modèle à l’aide d’Azure Databricks et veut maintenant le servir pour le scoring en temps réel. Quelle doit être l’action suivante ?\n",
      "Gérer la version du modèle.\n",
      "Effectuer la transition du modèle à la phase Archivé.\n",
      "Inscrire le modèle.\n",
      "Correct. L’inscription d’un modèle avec MLflow vous permet de servir le modèle pour le scoring en temps réel. Cela facilite l’utilisation du modèle formé, car le processus de service génère un wrapper et expose automatiquement une API REST pour le calcul de score.\n",
      "------------------------\n",
      "2.\n",
      "Une exécution MLflow exécute un script Python pour l’apprentissage d’un modèle. Le modèle est enregistré au cours de l’exécution, mais il doit également être enregistré. Qu’est-ce qui doit être inclus dans le code pour inscrire le modèle lors de l’exécution ?\n",
      "register_model\n",
      "Incorrect. Cette méthode est utilisée lors de l’inscription d’un modèle directement à partir d’une expérience.\n",
      "registered_model_name\n",
      "Correct. Pendant une exécution, vous pouvez inscrire un modèle en nommant registered_model_name.\n",
      "log_model\n",
      "------------------------\n",
      "3.\n",
      "Hanna dispose d’autorisations de lecture et souhaite faire passer un modèle de la phase de préproduction à celle de production par le biais de l’interface utilisateur Azure Databricks. Que peut faire une personne disposant d’autorisations de lecture ?\n",
      "Effectuer la transition.\n",
      "Incorrect. Elle n’a pas les autorisations suffisantes pour effectuer la transition.\n",
      "Demander la transition.\n",
      "Correct. Tout utilisateur disposant d’autorisations de lecture ou de mieux peut demander une transition.\n",
      "Elle ne peut lire que la phase actuelle.\n",
      "------------------------\n",
      "1. Quel type de join contient toutes les colonnes et uniquement les lignes correspondantes des deux tables ?\n",
      "kind=leftouter\n",
      "kind=inner\n",
      "Correct. Le type inner contient une ligne dans la sortie pour chaque combinaison de lignes correspondantes dans les tables de gauche et de droite.\n",
      "kind=fullouter\n",
      "------------------------\n",
      "2. Quel opérateur retourne une table avec des lignes ajoutées de plusieurs tables ?\n",
      "L’opérateur lookup\n",
      "L’opérateur join\n",
      "L’opérateur union\n",
      "Correct. L’opérateur union prend au moins deux tables et retourne une table avec toutes les lignes ajoutées.\n",
      "------------------------\n",
      "3. Quelle fonction devez-vous utiliser pour capturer les résultats d’une expression tabulaire ?\n",
      "La fonction materialize()\n",
      "Correct. La fonction materialize() capture le résultat d’une expression tabulaire afin qu’il puisse être référencé plusieurs fois par la requête sans recalcul.\n",
      "La fonction cache()\n",
      "Incorrect. Le langage de requête Kusto n’a pas de fonction cache().\n",
      "Instruction let\n",
      "------------------------\n",
      "4. Chaque minute, un objet JSON est extrait d’un appareil IoT (Internet des objets). Quel est le type des données extraites ?\n",
      "Données structurées\n",
      "Données semi-structurées\n",
      "Correct. Un objet JSON est considéré comme semi-structuré.\n",
      "Données non structurées\n",
      "------------------------\n",
      "2. Quand un scientifique des données extrait des objets JSON d’un appareil IoT et combine toutes les données transformées dans un fichier CSV, quel magasin de données doit-il utiliser de préférence ?\n",
      "Stockage Blob Azure\n",
      "Azure Data Lake Storage\n",
      "Correct. Vous pouvez stocker des fichiers CSV dans un lac de données sans aucune contrainte de capacité.\n",
      "Azure SQL Database\n",
      "------------------------\n",
      "Quelle transformation dans le flux de données de mappage est utilisée pour router des lignes de données vers différents flux en fonction de conditions de correspondance ?\n",
      "Recherche :\n",
      "Fractionnement conditionnel\n",
      "Correct. Une transformation de fractionnement conditionnel route les lignes de données vers différents flux en fonction de conditions de correspondance. La transformation de fractionnement conditionnel est similaire à une structure de décision CASE dans un langage de programmation.\n",
      "Sélectionnez\n",
      "------------------------\n",
      "2. Quelle transformation est utilisée pour charger des données dans un magasin de données ou une ressource de calcul ?\n",
      "Fenêtre\n",
      "Source.\n",
      "Incorrect. Une transformation de la source configure votre source de données pour le flux de données. Lors de la conception de flux de données, la première étape consiste toujours à configurer une transformation de source.\n",
      "Récepteur\n",
      "Correct. Une transformation de récepteur vous permet de choisir une définition de jeu de données pour les données de sortie de destination. Vous pouvez utiliser autant de transformations de réception que nécessaire pour votre flux de données.\n",
      "Le service peut également analyser les données à la recherche de modifications ayant eu lieu au fil du temps (on appelle cela la dérive de données) et déclencher une alerte.\n",
      "La dernière façon de publier vos modèles consiste à télécharger le modèle au format ONNX (Open Neural Network Exchange),\n",
      "Azure Event Grid est un service d’ingestion d’événements qui peut alerter et automatiser les réponses aux modifications apportées aux systèmes qu’il analyse.\n",
      "------------------------\n",
      "1.\n",
      "Parmi les fonctionnalités suivantes, laquelle est disponible dans Azure ML Studio Designer ?\n",
      "Vue d’ensemble et contrôles de sécurité réseau\n",
      "Canevas visuels avec des contrôles glisser-déplacer.\n",
      "Correct. Azure ML Designer fait parti d’Azure ML studio. Il vous permet de créer des pipelines ML en faisant glisser et en déposant des modules pour créer des workflows ou des tâches exécutés dans un ordre particulier.\n",
      "Contrôle de version et stockage de modèle\n",
      "------------------------\n",
      "2.\n",
      "Laquelle des descriptions suivantes décrit précisément un pipeline ?\n",
      "Environnement de codage pour le déploiement et l’analyse des modèles\n",
      "Ressource de niveau supérieur pour la gestion de tous les composants que vous créez lorsque vous utilisez Azure Machine Learning\n",
      "Workflow d’un processus ou d’une tâche de Machine Learning complète\n",
      "Correct. Les pipelines peuvent encapsuler une tâche telle que le nettoyage, l’extraction de données ou un workflow complet pour la formation et le déploiement des modèles.\n",
      "------------------------\n",
      "3.\n",
      "Quelles sont les fonctionnalités MLOps principales qui se trouvent dans Azure Machine Learning ?\n",
      "Déploiement de modèles plus rapide\n",
      "Environnements et pipelines de formation reproductibles\n",
      "Tous les éléments ci-dessus\n",
      "Correct. Azure ML incorpore tous ces principes dans ses fonctionnalités pour vous aider à créer des modèles reproductibles et fiables.\n",
      "------------------------\n",
      "1. Un scientifique des données souhaite utiliser le Machine Learning automatisé pour rechercher le modèle avec la meilleure métrique AUC_weighted. Quel paramètre de la fonction de classification doit être configuré ?\n",
      "    task='AUC_weighted'\n",
      "    target_column_name='AUC_weighted'\n",
      "    primary_metric='AUC_weighted'\n",
      "    Correct. Définissez la métrique principale sur le score de performance pour lequel vous souhaitez optimiser le modèle.\n",
      "    2. Un scientifique des données a prétraité les données d’entraînement, et souhaite utiliser le Machine Learning automatisé pour itérer rapidement au sein de différents algorithmes. Les données ne doivent pas être modifiées. Quel doit être le mode de caractérisation pour entraîner un modèle sans laisser le Machine Learning automatisé apporter des modifications aux données ?\n",
      "    auto\n",
      "    custom\n",
      "    off\n",
      "    Correct. Si vous ne souhaitez pas que les données soient prétraitées, désactivez la caractérisation.\n",
      "1. Un scientifique des données expérimente la formation du modèle. Après quelques itérations, un modèle semble adapté au déploiement, mais n’est pas encore prêt pour la consommation. Un pipeline Azure est créé pour effectuer l’apprentissage du modèle. Dans quel environnement le modèle doit-il d’abord être formé ?\n",
      "Production\n",
      "Préproduction\n",
      "Développement\n",
      "Correct. Une première étape sûre consiste à effectuer l’apprentissage d’un modèle dans l’environnement de développement, déclenché par un pipeline Azure.\n",
      "2. Comment un ingénieur Machine Learning peut-il contrôler le déplacement d’un modèle du développement vers la pré-production ?\n",
      "Ajouter un déclencheur.\n",
      "Ajoutez une vérification d’approbation.\n",
      "Correct. Une vérification d’approbation nécessite qu’une personne passe en revue un pipeline avant son exécution.\n",
      "Ajoutez RBAC.\n",
      "1. Comment pouvez-vous identifier les types de données dans chaque colonne de votre jeu de données ?\n",
      "Utiliser l’opérateur getschema.\n",
      "Vrai. L’opérateur getschema renvoie le schéma du jeu de données, qui contient chaque nom de colonne et son type de données.\n",
      "Utilisez l’opérateur render\n",
      "Mettez en surbrillance les colonnes et examinez le résumé dans la grille de résultats.\n",
      "2. Comment pouvez-vous trouver facilement la valeur min/max et la moyenne d’une sélection de cellules entières dans la grille de résultats ?\n",
      "Triez la grille de résultats en fonction de la colonne souhaitée.\n",
      "Utilisez l’opérateur render pour tracer la colonne et évaluez les résultats.\n",
      "Mettez en surbrillance les cellules et examinez le résumé dans le coin inférieur de la grille de résultats.\n",
      "Vrai. Les cellules en surbrillance afficheront un résumé dans le coin inférieur droit de la grille de résultats.\n",
      "3. Comment pouvez-vous vérifier si une série chronologique manque des données ?\n",
      "Comptez le nombre de lignes présentes dans la série chronologique.\n",
      "Faux. Le comptage n’est un test significatif que si vous savez à combien de lignes vous attendre.\n",
      "Prenez un échantillon de 10 lignes de données.\n",
      "Tracez le nombre d’événements par rapport au temps pour voir si des données sont manquantes.\n",
      "Vrai. Cette visualisation vous aidera à déterminer si des données sont manquantes\n",
      "1. Vous créez un point de terminaison de lot que vous souhaitez utiliser en vue de prédire de nouvelles valeurs pour un volume important de fichiers de données. Vous voulez que le pipeline exécute le script de scoring sur plusieurs nœuds et assemble les résultats. Quelle action de sortie devez-vous choisir pour le déploiement ?\n",
      "summary_only\n",
      "append_row\n",
      "Correct. Vous devez utiliser append_row pour ajouter chaque prédiction à un fichier de sortie.\n",
      "concurrency\n",
      "2. Plusieurs modèles sont déployés sur un point de terminaison de lot. Vous appelez le point de terminaison sans indiquer le modèle que vous souhaitez utiliser. Quel est le modèle déployé qui va réellement effectuer le scoring par lots ?\n",
      "Dernière version du modèle déployé.\n",
      "Dernier modèle déployé.\n",
      "Modèle déployé par défaut.\n",
      "Correct. Le déploiement par défaut sera utilisé pour effectuer le scoring par lots réel lorsque le point de terminaison est appelé\n",
      "------------------------\n",
      "1. Quel est l’avantage d’utiliser les tableaux de bord Azure Data Explorer ?\n",
      "Lien dynamique vers les données : les vignettes sont mises à jour chaque fois que vos données changent.\n",
      "C’est vrai. Les tableaux de bord Azure Data Explorer sont dynamiques et mis à jour chaque fois que vos données changent.\n",
      "Des capacités de requête supplémentaires en plus du langage de requête standard.\n",
      "Aucune autorisation de base de données n’est nécessaire.\n",
      "2. Après avoir ajouté un paramètre de tableau de bord, rien n’a changé dans les vignettes. Quelle est l’étape suivante ?\n",
      "Ajoutez une nouvelle vignette.\n",
      "Ajoutez le paramètre en tant que filtre dans la requête de vignette, puis choisissez une valeur pour le paramètre.\n",
      "Vrai. Les paramètres sont actifs uniquement quand ils sont utilisés dans la requête de vignette.\n",
      "Ajoutez un filtre croisé.\n",
      "3. Comment la liste de valeurs de paramètres peut-elle être remplie ?\n",
      "Automatiquement. La liste des valeurs est remplie une fois le nom généré.\n",
      "La liste doit être saisie manuellement ou générée par une requête.\n",
      "Vrai. La liste peut être saisie manuellement ou générée par une requête.\n",
      "Il n’est pas nécessaire de générer une liste de valeurs de paramètres.\n",
      "1. Est-ce que le Lecteur immersif fonctionne avec des images de téléphone ?\n",
      "Non, il peut être incorporé uniquement dans une application.\n",
      "Incorrect. Le Lecteur immersif fonctionne sur votre téléphone par le biais de Microsoft Lens, application iOS et Android qui rend lisibles les images de tableaux blancs et de documents en permettant à l’utilisateur d’accéder à des fonctions de lecture à voix haute, d’espacement du texte et de changement des couleurs.\n",
      "Oui, avec Microsoft Lens.\n",
      "Correct. Microsoft Lens est une application iOS et Android qui rend lisibles les images de tableaux blancs et de documents en permettant à l’utilisateur d’accéder à des fonctions de lecture à voix haute, d’espacement du texte et de changement des couleurs.\n",
      "Oui, il vous suffit de le charger sur votre ordinateur.\n",
      "2. Combien de langues le Lecteur immersif peut-il traduire automatiquement ?\n",
      "Aucun.\n",
      "Toutes les langues connues.\n",
      "80 langues.\n",
      "Correct. La traduction intégrée proposée par le Lecteur immersif est disponible dans 80 langues, et les utilisateurs peuvent traduire des mots, des phrases ou un document entier.\n",
      "3. Quel avantage offre la fonction Catégories grammaticales ?\n",
      "Les catégories grammaticales, comme les substantifs et les verbes, sont étiquetées et peuvent être mises en évidence avec des couleurs différentes.\n",
      "Correct. Les catégories grammaticales peuvent aider les utilisateurs à apprendre la grammaire, tout en renforçant leur compréhension et leur attention en utilisant différentes couleurs pour mettre en évidence les mots.\n",
      "Les mots sont découpés en syllabes.\n",
      "Il explique ce que sont les catégories grammaticales\n",
      "------------------------\n",
      "     \n",
      "1.\n",
      "Quels sont les besoins utilisateur les plus adaptés à l’utilisation de HDInsight Interactive Query ?\n",
      "Lorsque vous souhaitez utiliser MapReduce sur des données non structurées avec des contrôles d’accès en fonction du rôle.\n",
      "Lorsque vous souhaitez utiliser des requêtes de type SQL sur des données structurées avec des contrôles au niveau des lignes et des colonnes.\n",
      "Interactive Query offre des requêtes de type SQL sur des données structurées, avec des contrôles au niveau des lignes et des colonnes.\n",
      "Lorsque vous souhaitez utiliser des requêtes de type SQL sur des données à haut niveau de concurrence pour des calculs sur de longues périodes.\n",
      "2.\n",
      "Quels sont les formats de fichier pris en charge par Interactive Query ?\n",
      ".xml, .doc, .log\n",
      ".json, .csv, .txt\n",
      "Les fichiers .JSON, .csv et .txt sont pris en charge par Interactive Query.\n",
      ".pdf, .dbk, .md\n",
      "3.\n",
      "Quel scénario est le mieux adapté à HDInsight Interactive Query ?\n",
      "Traitement par lots\n",
      "Le traitement par lots consiste à lire les données à partir d’un emplacement, à les transformer et à les écrire à un autre emplacement. Interactive Query est idéal pour interroger les données en l’état, sans avoir à les transformer outre mesure.\n",
      "Diffusion de données\n",
      "requêtes ad hoc ;\n",
      "Les scénarios de requête ad hoc requièrent des réponses rapides aux requêtes utilisateur interactives. Un bon scénario pour Interactive Query\n",
      "4.\n",
      "Pourquoi Hive Warehouse Connector est-il nécessaire ?\n",
      "Hive et Spark sont des types de cluster différents.\n",
      "Hive et Spark ont deux metastores différents. Un connecteur est nécessaire pour les relier entre eux.\n",
      "Hive et Spark ont des metastores différents et ont besoin d’un pont pour connecter les deux.\n",
      "Hive est destiné aux données statiques et Spark est destiné aux données de diffusion en continu.\n",
      "5.\n",
      "Pourquoi l’utilisation de Hive Warehouse Connector est-elle plus efficace et évolutive que l’utilisation d’une connexion JDBC standard de Spark à Hive ?\n",
      "Parce que la bibliothèque charge des données entre le HiveServer et le pilote Spark en parallèle\n",
      "La bibliothèque ne charge pas de données entre le HiveServer et le pilote Spark.\n",
      "Parce que Hive Warehouse Connector est optimisé pour les données de diffusion en continu\n",
      "Parce que la bibliothèque charge les données de démons LLAP dans des exécuteurs Spark en parallèle\n",
      "La bibliothèque charge des données à partir de démons LLAP dans des exécuteurs Spark en parallèle, ce qui rend Hive Warehouse Connector efficace et évolutif.\n",
      "------------------------\n",
      "1.\n",
      "Supposons que votre corpus de texte contient 80 000 mots différents. Qu’est-ce qui est généralement fait pour réduire la dimensionnalité du vecteur d’entrée au classificateur neuronal ?\n",
      "Sélectionner 10 % des mots de façon aléatoire et ignorer le reste\n",
      "Utiliser la couche convolutive avant la couche de classifieur entièrement connecté\n",
      "Les couches convolutives ne réduisent pas la dimensionnalité des vecteurs d’entrée\n",
      "Utiliser la couche d’incorporation avant la couche de classifieur entièrement connecté\n",
      "C’est correct\n",
      "Sélectionner 10 % des mots les plus fréquemment utilisés et ignorer le reste\n",
      "------------------------\n",
      "1. Nous souhaitons former un réseau neuronal afin de générer de nouveaux mots drôles pour un livre pour enfants. Quelle architecture puis-je utiliser ?\n",
      "LSTM au niveau du mot\n",
      "Les réseaux au niveau du mot opèrent sur un vocabulaire prédéfini de mots et ne peuvent pas générer de nouveaux mots.\n",
      "LSTM au niveau du caractère\n",
      "Correct, les LSTM de niveau caractère capturent souvent des syllabes utilisées et placent ces séquences ensemble pour générer de nouveaux mots.\n",
      "RNN au niveau du mot\n",
      "Perceptron au niveau du caractère\n",
      "2. Le réseau neuronal récurrent est appelé ainsi pour les raisons suivantes :\n",
      "Un réseau est appliqué à chaque élément d’entrée, et la sortie de l’application précédente est passée à la suivante\n",
      "Correct.\n",
      "Il est formé par un processus récurrent\n",
      "Il se compose de couches qui incluent d’autres sous-réseaux\n",
      "3. Quelle est l’idée principale de l’architecture réseau LSTM ?\n",
      "Nombre fixe de blocs LSTM pour l’ensemble du jeu de données\n",
      "Il contient de nombreuses couches de réseaux neuronaux récurrents\n",
      "Gestion d’état explicite avec oubli et déclenchement d’état\n",
      "Dans LSTM, chaque bloc reçoit un état de sortie. Il est manipulé dans le bloc en fonction de l’entrée et de l’état précédent.\n",
      "------------------------\n",
      "1.\n",
      "Le réseau neuronal récurrent est appelé ainsi pour les raisons suivantes :\n",
      "Un réseau est appliqué à chaque élément d’entrée, et la sortie de l’application précédente est passée à la suivante\n",
      "C’est exact.\n",
      "Il est formé par un processus récurrent\n",
      "Il se compose de couches qui incluent d’autres sous-réseaux.\n",
      "------------------------\n",
      "1. Qu’est-ce que le taux d’échantillonnage ?\n",
      "Fréquence mappée au temps.\n",
      "Les canaux audio.\n",
      "Échantillonnage du son analogique à des intervalles de temps cohérents pour créer une représentation numérique du son.\n",
      "Correct !\n",
      "2. Qu’est-ce que la forme d’onde ?\n",
      "Fréquence mappée au temps.\n",
      "Incorrect, une fréquence associée à un instant T est un spectrogramme.\n",
      "Taux d’échantillonnage et fréquence visualisées.\n",
      "Correct ! Vous pouvez visualiser vos données à l’aide d’une forme d’onde pour associer le taux d’échantillonnage et la fréquence.\n",
      "Les canaux audio.\n",
      "------------------------\n",
      "1.\n",
      "Lorsque vous rééchantillonnez le son, vous...\n",
      "augmentez la taille.\n",
      "réduisez la taille.\n",
      "Correct ! Vous pouvez réduire la taille du fichier en réduisant le taux d’échantillonnage pour la piste audio.\n",
      "2.\n",
      "Qu’est-ce qu’un spectrogramme ?\n",
      "Il associe la fréquence à un instant T d’un fichier audio.\n",
      "Correct !\n",
      "Les canaux audio.\n",
      "Taux d’échantillonnage et fréquence visualisées.\n",
      "3.\n",
      "La classification audio ne peut être effectuée qu’avec la vision par ordinateur sur des spectrogrammes.\n",
      "Vrai\n",
      "Faux\n",
      "Correct ! Il existe plusieurs façons de créer des modèles de classification audio.\n",
      "------------------------\n",
      "1.\n",
      "Le framework tidymodels a été utilisé dans R pour former un modèle de régression à partir d’un jeu de données de données de ventes. Comment faire pour évaluer le modèle pour vous assurer qu’il se prédira correctement avec les nouvelles données ?\n",
      "Fractionner les données de manière aléatoire en deux sous-ensembles. Utiliser un sous-ensemble pour effectuer l’apprentissage du modèle et l’autre pour l’évaluer.\n",
      "Correct. Une méthode courante d’apprentissage et d’évaluation de modèles consiste à conserver un jeu de données d’évaluation lors de l’apprentissage.\n",
      "Utiliser toutes données pour effectuer l’apprentissage du modèle. Puis utiliser toutes les données pour l’évaluer.\n",
      "Effectuez l’apprentissage du modèle à l’aide uniquement des colonnes de caractéristiques. Ensuite, évaluez-le en utilisant uniquement la colonne d’étiquette.\n",
      "2.\n",
      "Une spécification de modèle de régression a été créée à l’aide de la fonction linear_reg() dans le package tidymodels. Que faut-il faire pour former le modèle ?\n",
      "Appelez la fonction predict() et spécifiez la spécification, la formule et les données du modèle.\n",
      "Appelez la fonction recipe() et spécifiez la spécification, la formule et les données du modèle.\n",
      "Appelez la fonction fit() et spécifiez la spécification, la formule et les données du modèle.\n",
      "Correct. Une fois qu’une spécification de modèle est effectuée, la formation du modèle peut ensuite être effectuée avec la fonction fit() ou fit_xy().\n",
      "3.\n",
      "Un modèle de régression a été formé à l’aide du framework tidymodels. Lorsqu’il est évalué avec des données de test, le modèle obtient un R-carré de 0,95. Qu’est-ce que cette métrique vous apprend sur le modèle ?\n",
      "Le modèle a une précision de 95 %.\n",
      "Le modèle décrit la majeure partie de la variance entre les valeurs prédites et réelles.\n",
      "Correct. La métrique R-squared est une mesure de la quantité de la variance qui peut être expliquée par le modèle.\n",
      "En moyenne, les prédictions sont 0,95 supérieures aux valeurs réelles.\n",
      "------------------------\n",
      "1.\n",
      "Vous envisagez d’utiliser le framework tidymodels pour former un modèle qui prédit le risque de défaut de crédit. Le modèle doit prédire une valeur de 0 pour les demandes de prêt qui doivent être approuvées automatiquement, et 1 pour les demandes pour lesquelles il existe un risque de valeur par défaut qui nécessite une prise en compte humaine. Quel type de modèle est requis ?\n",
      "Un modèle de classification binaire.\n",
      "Correct. Un modèle de classification binaire prédit la probabilité pour deux classes.\n",
      "Un modèle de classification multiclasse.\n",
      "Un modèle de régression linéaire.\n",
      "2.\n",
      "Vous avez formé une spécification de modèle de classification dans tidymodels. Vous souhaitez utiliser le modèle, logreg_cls_fit, pour retourner des étiquettes pour un nouveau jeu de données appelé new_data. Quel code devez-vous utiliser ?\n",
      "predict(logreg_cls_fit, new_data)\n",
      "Correct. Utilisez la méthode parsnip::predict.model_fit() pour inférer les étiquettes pour les nouvelles données.\n",
      "fit(logreg_cls_fit, new_data)\n",
      "fit_resamples(logreg_cls_fit, new_data)\n",
      "3.\n",
      "Vous formez un modèle de classification binaire à l’aide du framework tidymodels. Lorsque vous l’évaluez avec des données de test, vous déterminez que le modèle atteint une métrique Rappel globale de 0,81. Que signifie cette mesure ?\n",
      "Le modèle a correctement prédit 81 pour cent des cas de test.\n",
      "81 pour cent des cas prédits comme positifs par le modèle étaient réellement positifs.\n",
      "Incorrect. Vous trouverez ces informations à l’aide de la métrique de précision.\n",
      "Le modèle a correctement identifié 81 pour cent des cas positifs comme étant positifs.\n",
      "Correct. La métrique de rappel indique le pourcentage de cas positifs réels que le classifieur a correctement identifiés.\n",
      "------------------------\n",
      "1.\n",
      "Un scientifique des données souhaite exécuter un script en tant que travail de commande pour entraîner un modèle PyTorch, en définissant les hyperparamètres de taille de lot et de taux d’apprentissage sur les valeurs spécifiées à chaque exécution du travail. Que doit faire le scientifique des données ?\n",
      "Créer plusieurs fichiers de script : un pour chaque combinaison de taille de lot et de taux d’apprentissage que vous voulez utiliser.\n",
      "Définir les propriétés de taille de lot et de taux d’apprentissage du travail de commande avant de soumettre le travail.\n",
      "Ajouter des arguments pour la taille de lot et le taux d’apprentissage au script, puis les définir dans la propriété command du travail de commande.\n",
      "Correct. Pour utiliser des valeurs différentes à chaque fois, définir des arguments dans le script et les passer à l’aide du paramètre arguments du travail de commande.\n",
      "2.\n",
      "Un scientifique des données a entraîné un modèle dans un notebook. Le modèle doit être réentraîné chaque semaine sur de nouvelles données. Que doit faire le scientifique des données pour que le code soit prêt pour la production ?\n",
      "Copier et coller le code de chaque cellule dans un script.\n",
      "Convertir le code en une seule fonction dans un script qui lit les données et entraîne le modèle.\n",
      "Incorrect. Un script comprenant plusieurs fonctions est plus facile à tester et à gérer, notamment par d’autres personnes.\n",
      "Convertir le code en plusieurs fonctions dans un script qui lisent les données et entraînent le modèle.\n",
      "Correct. Il est préférable d’utiliser un script composé de plusieurs fonctions pour les charges de travail de production.\n",
      "------------------------\n",
      "1.\n",
      "Un scientifique des données forme un modèle de régression et souhaite suivre les performances du modèle en stockant l’erreur quadratique moyenne (RMSE) à l’exécution de l’expérimentation. Quelle méthode peut être utilisée pour enregistrer le RMSE ?\n",
      "mlflow.log_param()\n",
      "mlflow.log_artifact()\n",
      "mlflow.log_metric()\n",
      "Correct. Utiliser mlflow.log_metric() pour enregistrer une métrique comme RMSE.\n",
      "2.\n",
      "Quand un scientifique des données active la journalisation MLflow, où se trouvent toutes les ressources du modèle ?\n",
      "Dans le dossier model sous Outputs + logs.\n",
      "Correct. Les ressources de modèle comme le fichier de modèle pickle sont stockées dans le dossier model sous Outputs + logs.\n",
      "Dans le dossier outputs, sous Outputs + logs.\n",
      "Dans le dossier model sous Metrics.\n",
      "------------------------\n",
      "1.\n",
      "Vous avez entraîné un modèle à l’aide du SDK Python pour Azure Machine Learning. Vous souhaitez déployer le modèle pour obtenir des prédiction en temps réel. Vous souhaitez gérer l’infrastructure sous-jacente utilisée par le point de terminaison. Quel type de point de terminaison devez-vous créer ?\n",
      "Un point de terminaison en ligne managé.\n",
      "Incorrect. Si vous utilisez un point de terminaison en ligne managé, Azure Machine Learning gère toute l’infrastructure.\n",
      "Un point de terminaison de traitement par lots.\n",
      "Un point de terminaison en ligne Kubernetes.\n",
      "Correct. Vous devez utiliser un point de terminaison en ligne Kubernetes si vous souhaitez gérer les clusters Kubernetes sous-jacents.\n",
      "2.\n",
      "Vous déployez un modèle comme un service d’inférence en temps réel. Quelles fonctions le script de scoring doit-il inclure pour le déploiement ?\n",
      "main() et score()\n",
      "base() et train()\n",
      "init() et run()\n",
      "Correct. Vous devez implémenter les fonctions init et run dans le script d’entrée (scoring).\n",
      "------------------------\n",
      "1.\n",
      "Quel type d’algorithme Machine Learning utiliseriez-vous pour entraîner un modèle qui prédit la quantité de précipitations en pouces sur une journée donnée ?\n",
      "classification ;\n",
      "régression ;\n",
      "Correct. Un modèle de régression prédit une valeur numérique.\n",
      "Clustering\n",
      "2.\n",
      "Quelle classe Spark devez-vous utiliser pour explorer et nettoyer les données d’entraînement d’un projet Machine Learning ?\n",
      "Dataframe\n",
      "C’est correct. Un dataframe est une excellente classe à utiliser pour travailler avec des données.\n",
      "RDD\n",
      "Modèle\n",
      "3.\n",
      "Parmi les métriques suivantes, lesquelles utiliseriez-vous pour évaluer un modèle de classification ?\n",
      "Erreur quadratique moyenne\n",
      "Silhouette\n",
      "Rappel\n",
      "C’est correct. Le rappel est une métrique utilisée pour évaluer un modèle de clustering.\n",
      "------------------------\n",
      "1.\n",
      "Dans notre répertoire actif, nous souhaitons trouver les trois fichiers comprenant le moins de lignes. Quelle commande devez-vous utiliser ?\n",
      "wc -l * > sort -n > head -n 3\n",
      "wc -l * | sort -n | head -n 1-3\n",
      "wc -l * | sort -n | head -n 3\n",
      "Correct. Cette commande compte les lignes d’un fichier, trie la sortie dans l’ordre croissant (numérique) et affiche les trois premières lignes.\n",
      "wc -l * | head -n 3 | sort -n\n",
      "2.\n",
      "Quelles sont les correspondances établies par l’expression régulière Fr[ea]nc[eh] ?\n",
      "French, France, Frence, Franch\n",
      "Correct. Cette expression régulière est construite de telle façon qu’elle établit une correspondance avec les termes mal orthographiés Frence et Franch.\n",
      "Frenche, Franceh, Frenceh, Franche\n",
      "France, French\n",
      "Freanceh, Fraenche\n",
      "3.\n",
      "L’option -v de la commande grep inverse les critères spéciaux. Ainsi, seules les lignes qui ne correspondent pas au modèle sont imprimées. Laquelle des commandes suivantes trouve tous les fichiers dans le répertoire /data dont les noms se terminent par s.txtet qui ne contiennent pas la chaîne net ? shuttles.txt ou software.txt sont des exemples, mais pas planets.txt.\n",
      "find data -name *s.txt | grep -v net\n",
      "Incorrect. L’interpréteur de commandes développe *s.txt au lieu de passer l’expression générique à rechercher.\n",
      "grep -v 'net' $(find data -name '*s.txt')\n",
      "find data -name '*s.txt' | grep -v net\n",
      "Correct. Le fait de placer l’expression de correspondance entre guillemets empêche le shell de la développer. Elle est donc transmise à la commande find.\n",
      "Aucune des propositions ci-dessus\n",
      "4.\n",
      "Pour économiser du stockage, vous souhaitez supprimer certains fichiers de données traités et conserver uniquement vos fichiers bruts et votre script de traitement. Les fichiers bruts se terminent par .dat, et les fichiers traités se terminent par .txt. Laquelle des solutions suivantes permet de supprimer tous les fichiers de données traités, et aucun autre fichier ?\n",
      "rm ?.txt\n",
      "rm *.txt\n",
      "Correct. Cette expression supprime tous les fichiers qui se terminent par une extension .txt.\n",
      "rm * .txt\n",
      "rm *.*\n",
      "------------------------\n",
      "1.\n",
      "Quelle méthode devez-vous utiliser pour enregistrer la valeur d’évaluation rmse de votre modèle dans une exécution MLflow ?\n",
      "mlflow.log_metric\n",
      "Correct. Consigner les métriques de performances à l’aide de la méthode mlflow.log_metric.\n",
      "mlflow.log_param\n",
      "mlflow.spark.log_model\n",
      "2.\n",
      "Vous avez enregistré un modèle dans une exécution d’expérience et vous prévoyez de le déployer dans un service d’inférence en temps réel. Que devez-vous faire ?\n",
      "Reproduire l’expérience\n",
      "Inscrire le modèle\n",
      "Correct. Inscrire un modèle avant de le déployer pour l’inférence.\n",
      "Enregistrer le modèle en tant que fichier ONXX dans le système de fichiers DFFS.\n",
      "3.\n",
      "Vous souhaitez utiliser votre modèle pour prédire les étiquettes en continu à partir de données de caractéristiques quand elles sont stockées dans une table delta. Quel type d’inférence devez-vous configurer ?\n",
      "Points de terminaison en temps réel\n",
      "Incorrect. Un point de terminaison en temps réel permet aux applications d’effectuer l’inférence à la demande via une interface REST.\n",
      "Diffusion en continu\n",
      "Correct. Une solution d’inférence de streaming traite les données d’une table delta et transmet les résultats en continu à une autre table.\n",
      "Batch\n",
      "------------------------\n",
      "1. Translator peut convertir du texte à partir de quels types de fichiers et de données ?\n",
      "Fichiers PowerPoint et vidéos MP4\n",
      "Chaînes et documents.\n",
      "Correct. Translator peut convertir des chaînes et des documents, comme des documents PDF et Word, d’une langue vers de nombreuses autres.\n",
      "Images JPG et PNG.\n",
      "2. Avez-vous besoin de spécifier la langue source pour une traduction ?\n",
      "Non, la détection automatique de la langue fonctionne pour toutes les langues.\n",
      "Oui, la détection automatique de la langue n’est pas disponible pour Translator.\n",
      "Incorrect. Translator peut détecter automatiquement la langue source pour certaines langues.\n",
      "Pour certaines langues, vous devez spécifier la langue source, mais la détection automatique de la langue fonctionne pour plus de 50 langues.\n",
      "Correct. Translator peut détecter automatiquement la langue source pour certaines langues, ce qui signifie que vous ne devez pas toujours spécifier la langue source.\n",
      "3.\n",
      "Quel est l’avantage de créer un modèle de traduction personnalisé ?\n",
      "Les modèles personnalisés peuvent être adaptés aux terminologies et styles spécifiques à votre secteur d’activité ou à un domaine, ce qui permet une traduction plus précise.\n",
      "Correct. Les modèles personnalisés peuvent être adaptés pour traduire du texte et des documents dans des terminologies et des styles spécifiques d’un secteur d’activité ou d’un domaine.\n",
      "Les modèles personnalisés peuvent extraire des données de photographies, en améliorant vos workflows existants.\n",
      "Les modèles personnalisés peuvent être ajustés pour comprendre l’émotion du document, ce qui permet à un score de sentiment de fournir plus de données à l’utilisateur final.\n",
      "------------------------\n",
      "1.\n",
      "Quels types d’analytique sont proposés avec Azure Synapse ?\n",
      "Analytique descriptive et associative.\n",
      "Analytique prédictive et prescriptive.\n",
      "Correct. Synapse offre à la fois ces deux fonctionnalités, ainsi que l’analytique descriptive et de diagnostic.\n",
      "Analyse exploratoire et de diagnostic.\n",
      "2.\n",
      "Parmi les instructions suivantes, laquelle décrit le mieux une offre de services liés Azure Synapse Studio ?\n",
      "Le service lié personnalisé peut être créé dans Synapse.\n",
      "Correct. Synapse offre la possibilité de créer votre propre service lié si vous ne trouvez pas ce dont vous avez besoin parmi les services liés disponibles.\n",
      "Plus de 2 000 services liés sont disponibles pour les sources de données et les applications.\n",
      "Incorrect. Synapse propose plus de 200 services liés.\n",
      "Un service lié est disponible pour SAP et un pour OData.\n",
      "3.\n",
      "Parmi les instructions suivantes, laquelle décrit le mieux une méthode d’utilisation avec Azure Synapse Studio ?\n",
      "Les rapports Power BI doivent être créés séparément de Azure Synapse Studio.\n",
      "La gestion et la surveillance des travaux ou des pipelines peuvent être effectuées dans Azure Synapse Studio.\n",
      "Les scripts T-SQL et les notebooks personnalisés peuvent être utilisés pour interagir avec les moteurs analytiques dans Azure Synapse Studio.\n",
      "Correct. Le scénario de module a utilisé un script T-SQL, mais vous pouvez également utiliser des notebooks personnalisés.\n",
      "4.\n",
      "Quelles sont les meilleures options de données dans Synapse Analytics ?\n",
      "Synapse Analytics permet l'importation de données à partir de plusieurs sources de données.\n",
      "Correct. Le scénario de module implique deux sources de données, mais vous pouvez en ajouter d’autres.\n",
      "Synapse Analytics peut combiner des données à partir d’un maximum de deux sources.\n",
      "Synapse Analytics fonctionne uniquement avec les données contenues dans les pools SQL.\n",
      "------------------------\n",
      "1.\n",
      "Quelle fonction devez-vous utiliser pour lancer des essais Hyperopt ?\n",
      "hyperopt.tpe.suggest\n",
      "Incorrect. La constante hyperopt.tpe.suggest est utilisée pour spécifier un algorithme de recherche TPE.\n",
      "hyperopt.hp.choice\n",
      "hyperopt.fmin\n",
      "Correct. La fonction fmin contrôle le processus Hyperopt pour réduire une fonction objective.\n",
      "2.\n",
      "Lequel de ces objets définit l’ensemble de valeurs d’hyperparamètres qu’Hyperopt peut mixer lors de l’exécution des essais ?\n",
      "Fonction objective\n",
      "Incorrect. Une fonction objective ne définit pas un ensemble de valeurs d’hyperparamètres possibles.\n",
      "Espace de recherche\n",
      "Correct. Un espace de recherche définit un ensemble de valeurs d’hyperparamètres possibles.\n",
      "Algorithme de recherche\n",
      "3.\n",
      "Quelle classe devez-vous utiliser pour suivre les détails de chaque exécution de réglage des hyperparamètres lors de l’utilisation de Spark MLLib ?\n",
      "Essais\n",
      "Correct. La classe Essais effectue le suivi des détails de chaque exécution d’évaluation.\n",
      "SparkTrials\n",
      "RDD\n",
      "------------------------\n",
      "1.\n",
      "Que fait AutoML ?\n",
      "Entraîne un modèle basé sur des données que vous spécifiez en utilisant plusieurs algorithmes et hyperparamètres afin d’identifier le meilleur modèle.\n",
      "Correct. AutoML teste plusieurs combinaisons d'entraînement afin de trouver le modèle optimal pour vos données.\n",
      "Exécute des scripts d’entraînement de modèle en fonction d'une planification que vous spécifiez pour automatiser les opérations Machine Learning.\n",
      "Entraîne et déploie des modèles Machine Learning destinés à être utilisés dans les véhicules à conduite autonome.\n",
      "2.\n",
      "Vous comptez utiliser AutoML dans l'interface utilisateur Azure Databricks. Que devez-vous faire en premier ?\n",
      "Entraîner un modèle de référence en utilisant la bibliothèque Spark MLlib.\n",
      "Charger les données d'entraînement dans une table de votre espace de travail Azure Databricks.\n",
      "Correct. Vous devez fournir les données d'entraînement sous la forme d'une table dans le metastore Hive.\n",
      "Créer un notebook vide.\n",
      "3.\n",
      "Vous souhaitez utiliser l'API AutoML pour entraîner un modèle qui prédit le prix attendu d'une maison en fonction de la taille, du nombre de chambres et du code postal ? Quelle méthode devez-vous utiliser ?\n",
      "classify\n",
      "regress\n",
      "Correct. Un modèle de régression prédit une valeur numérique.\n",
      "forecast\n",
      "Incorrect. Un modèle de prévision prédit des valeurs en fonction du temps.\n",
      "------------------------\n",
      "1.\n",
      "Un script Python pour la formation d’un modèle est créé dans Azure Databricks et attaché à un cluster avec Databricks Runtime pour Machine Learning. Pour exécuter le code et activer le MLflow automatisé lors de l’optimisation des hyperparamètres, quelle méthode doit être utilisée ?\n",
      "ParamGridBuilder()\n",
      "CrossValidator\n",
      "Correct. Vous pouvez utiliser CrossValidator ou TrainValidationSplit pour optimiser les hyperparamètres avec MLflow automatisé.\n",
      "RegressionEvaluator()\n",
      "2.\n",
      "Quels sont les arguments nécessaires pour exécuter la fonction Hyperopt fmin() ?\n",
      "La métrique d’évaluation, le modèle et les données.\n",
      "La fonction objective, l’espace de recherche et le modèle.\n",
      "La fonction objective, l’espace de recherche et l’algorithme de recherche.\n",
      "Correct. La fonction objective fn, l’espace de recherche space et l’algorithme de recherche algo sont des arguments pour la fonction fmin().\n",
      "------------------------\n",
      "1.\n",
      "Quelle est la fonction du terme « P » dans le contrôle PID ?\n",
      "Applique l’expertise humaine à une décision.\n",
      "Prédit l’impact d’une décision de contrôle.\n",
      "Évite le dépassement du contrôleur.\n",
      "Incorrect. Le « I » ou partie intégrale du contrôleur PID est ce qui permet d’éviter le dépassement du système.\n",
      "Dirige le système vers l’objectif cible.\n",
      "Un « P » correct signifie Proportionnel et il dirige les systèmes vers la cible.\n",
      "2. Quelle technique de théorie du contrôle comprend également une routine d’optimisation ?\n",
      "PID\n",
      "Ouvrir le contrôle en boucle\n",
      "Contrôle de transfert de flux\n",
      "Contrôle prédictif du modèle (MPC)\n",
      "Correct. MPC contient un modèle du système ou du processus et un algorithme d’optimisation en arrière-plan.\n",
      "3. Parmi les techniques suivantes, laquelle n’est PAS une technique d’intelligence automatisée ?\n",
      "Systèmes experts\n",
      "Incorrect. Les systèmes experts ou « manuels » sont une forme d’intelligence automatisée.\n",
      "Réseaux neuronaux\n",
      "Correct. Les réseaux neuronaux ou les algorithmes Machine Learning appartiennent au groupe de l’intelligence autonome.\n",
      "Algorithmes d’optimisation\n",
      "Théorie du contrôle\n",
      "4. Comment la théorie du contrôle détermine-t-elle la prochaine action à entreprendre dans un système ?\n",
      "Recherchez les options possibles et sélectionnez-les en fonction de critères objectifs.\n",
      "Essayez des actions et testez si elles sont plus proches ou plus éloignées de l’objectif.\n",
      "Incorrect. (Profond) L’apprentissage par renforcement est la technologie qui effectue des essais et des erreurs avec une fonction d’objectif ou de récompense.\n",
      "Calculez l’action suivante à l’aide d’équations mathématiques, de physique ou de chimie.\n",
      "Correct. La théorie du contrôle utilise les mathématiques pour déterminer la meilleure action à entreprendre.\n",
      "Recherchez les options d’une table.\n",
      "5. Parmi les phrases suivantes, laquelle décrit les points forts des algorithmes d’optimisation ?\n",
      "Convient aux situations pour lesquelles vous n’avez pas beaucoup d’expertise sur la façon de contrôler le système.\n",
      "Correct. Les algorithmes d’optimisation sont l’outil idéal à explorer lorsque nous ne connaissons pas la tâche ou l’espace de recherche.\n",
      "Optimal pour comprendre les concepts et les stratégies nécessaires à l’exécution de la tâche.\n",
      "Génère des décisions rapidement avec peu de ressources informatiques requises.\n",
      "S’adapte bien aux conditions floues et incertaines.\n",
      "Incorrect. Les algorithmes d’optimisation ne s’adaptent pas aux conditions floues ou/et incertaines.\n",
      "------------------------\n",
      "6. Quelle technique d’intelligence automatisée choisiriez-vous pour contrôler la vitesse des rotors d’un drone ?\n",
      "Théorie du contrôle\n",
      "Correct. C’est la technologie la plus simple et la plus fiable pour le contrôle de bas niveau, ce qui est nécessaire pour contrôler les rotors d’un drone.\n",
      "Systèmes experts\n",
      "Optimization\n",
      "Programmation procédurale\n",
      "------------------------\n",
      "1. Vous prévoyez d'utiliser un conteneur Cognitive Services sur un hôte Docker local. Parmi les affirmations suivantes, laquelle est vraie ?\n",
      "Les applications clientes doivent transmettre une clé d'abonnement au point de terminaison de la ressource Azure avant d'utiliser le conteneur.\n",
      "Le conteneur doit pouvoir se connecter au point de terminaison de la ressource Azure pour envoyer les données d'utilisation à des fins de facturation.\n",
      "Correct. Les métriques d'utilisation du conteneur sont envoyées à la ressource Azure Cognitive Services à des fins de facturation.\n",
      "Toutes les données transmises au conteneur par l'application cliente sont acheminées vers le point de terminaison de la ressource Azure.\n",
      "------------------------\n",
      "2. Parmi les paramètres suivants, lequel devez-vous spécifier lors du déploiement d'une image conteneur Cognitive Services ?\n",
      "CLUF\n",
      "Correct. Vous devez spécifier un paramètre EULA (CLUF) avec la valeur « Yes » pour accepter explicitement le contrat de licence.\n",
      "ResourceGroup\n",
      "SubscriptionName\n",
      "------------------------\n",
      "3. Vous prévoyez d’utiliser la fonctionnalité de détection de la langue du service Langage de Cognitive Services dans un conteneur. Quelle image conteneur devez-vous déployer ?\n",
      "mcr.microsoft.com/azure-cognitive-services/textanalytics\n",
      "mcr.microsoft.com/azure-cognitive-services\n",
      "mcr.microsoft.com/azure-cognitive-services/textanalytics/language\n",
      "Correct. Vous devez déployer l'image spécifique à la détection de la langue.\n",
      "------------------------\n",
      "1. Vous envisagez d’utiliser le paramétrage d’hyperparamètres pour rechercher des valeurs discrètes optimales pour un ensemble d’hyperparamètres. Vous souhaitez essayer toutes les combinaisons possibles d’un ensemble de valeurs discrètes spécifiées. Quel type d’échantillonnage devez-vous utiliser ?\n",
      "Échantillonnage aléatoire\n",
      "Échantillonnage par grille\n",
      "Correct. Vous devez utiliser un échantillonnage par grille pour tester l’ensemble des combinaisons de valeurs d’hyperparamètres discrets.\n",
      "Échantillonnage bayésien\n",
      "2. Vous utilisez le réglage des hyperparamètres pour effectuer l’apprentissage d’un modèle optimal basé sur une métrique cible nommée « AUC ». Que devez-vous faire dans votre script de formation ?\n",
      "Importez le package de journalisation et utilisez une instruction logging.info() pour consigner l’AUC.\n",
      "Incluez une instruction print() pour écrire la valeur AUC dans le flux de sortie standard.\n",
      "Utilisez une instruction mlflow.log_metric() pour journaliser la valeur AUC.\n",
      "Correct. Votre script doit utiliser MLflow pour consigner la métrique principale sur l’exécution à l’aide du même nom que celui spécifié dans le travail de balayage.\n",
      "------------------------\n",
      "Vous créez un pipeline qui comprend deux étapes. L’étape 1 prépare certaines données et l’étape 2 utilise les données prétraitées pour effectuer l’apprentissage d’un modèle. Quelle option devez-vous utiliser comme entrée à la deuxième étape pour effectuer l’apprentissage du modèle ?\n",
      "pipeline_job_input\n",
      "prep_data.outputs.output_data\n",
      "Correct. prep_data.outputs.output_data est la sortie de l’étape qui prépare les données.\n",
      "train_model.outputs.model_output\n",
      "2.\n",
      "Vous avez créé un pipeline que vous souhaitez exécuter chaque semaine. Vous souhaitez adopter une approche simple pour créer une planification. Quelle classe pouvez-vous utiliser pour créer la planification qui s’exécute une fois par semaine ?\n",
      "RecurrencePattern\n",
      "JobSchedule\n",
      "RecurrenceTrigger\n",
      "Correct. Vous avez besoin de la classe RecurrenceTrigger pour créer une planification qui s’exécute à intervalles réguliers.\n",
      "Avec quel logiciel de gestion de version Azure Data Factory s'intègre-t-il ?\n",
      "Team Foundation Server.\n",
      "Source Safe.\n",
      "Référentiels Git.\n",
      "Correct. Azure Data Factory vous permet de configurer un référentiel git avec Azure Repos ou GitHub. C’est un système de gestion de version qui facilite le suivi des modifications et la collaboration.\n",
      "2. Quelle fonctionnalité permet d’apporter des modifications sur les projets Azure Data Factory dans une branche personnalisée créée avec la branche principale dans un référentiel Git ?\n",
      "Repo.\n",
      "Demande de tirage.\n",
      "Correct. Une fois que le développeur est satisfait de ses modifications, il crée une demande de tirage (pull) à partir de sa branche de fonctionnalité vers la branche principale ou la branche de collaboration pour que ces modifications soient examinées par des pairs.\n",
      "Commiter.\n",
      "3. Quelle fonctionnalité dans les alertes peut être utilisée pour déterminer la façon dont une alerte est déclenchée ?\n",
      "Ajouter une règle.\n",
      "Incorrect. Une règle est la définition principale de l’alerte qui comprend un nom de règle et une description.\n",
      "Ajoutez une gravité.\n",
      "Ajouter des critères.\n",
      "Correct. La fonctionnalité « Ajouter des critères » vous permet de déterminer la façon dont une alerte est déclenchée\n",
      "------------------------\n",
      "1. Quel composant Hadoop est utilisé pour gérer les ressources sur un système Hadoop ?\n",
      "YARN\n",
      "Correct. Il s’agit du composant chargé de gérer les ressources sur un cluster Hadoop.\n",
      "Spark\n",
      "MapReduce\n",
      "Inexact. MapReduce est un modèle de programmation qui vous permet de traiter et d’analyser des données.\n",
      "2. Dans quel type de système de fichiers Apache Hadoop stocke-t-il des données ?\n",
      "Il n’a pas de stockage. Les données sont stockées en mémoire.\n",
      "RDD\n",
      "HDFS\n",
      "Correct. HDFS est l’acronyme de « Hadoop Distributed File System », magasin de données d’un système Hadoop.\n",
      "3. Laquelle des options suivantes est un framework de traitement parallèle ?\n",
      "Apache Hive\n",
      "Apache Spark\n",
      "Correct. Apache Spark prend en charge le traitement en mémoire, qui aide à améliorer les performances des applications d’analytique de Big Data.\n",
      "Apache Kafka\n",
      "4. Dans un cluster Hadoop sur HDInsight, lequel des nœuds suivants est responsable du traitement des données ?\n",
      "Nœud principal\n",
      "Nœud Worker\n",
      "Correct. Les nœuds Worker sont responsables du traitement des données dans un cluster.\n",
      "5. Lequel des scénarios suivants pour le traitement de Big Data permet à une organisation de préparer le Big Data à une analyse plus poussée ?\n",
      "Entrepôt de données\n",
      "Science des données\n",
      "Traitement par lots\n",
      "Correct. Les organisations utilisent des tâches de traitement par lots pour préparer le Big Data à une analyse plus poussée.\n",
      "1. Quel est le composant Azure Data Factory qui orchestre un travail de transformation ou exécute une commande de déplacement des données ?\n",
      "Services liés\n",
      "Groupes de données\n",
      "Activités\n",
      "Correct. Les activités contiennent la logique de transformation ou les commandes d’analyse du travail d’Azure Data Factory.\n",
      "2. Vous déplacez des données d’un magasin Azure Data Lake Gen2 vers Azure Synapse Analytics. Quel runtime d’intégration Azure Data Factory est utilisé dans une activité de copie de données ?\n",
      "Azure-SSIS\n",
      "Azure\n",
      "Correct. Quand vous déplacez des données entre des technologies de plateforme de données Azure, le runtime d’intégration Azure est utilisé lors de la copie des données entre deux plateformes de données Azure.\n",
      "Auto-hébergé\n",
      "Incorrect. Le runtime d’intégration auto-hébergé est utilisé lors du déplacement des données à partir de réseaux privés vers le cloud et vice versa\n",
      "Un scientifique des données a créé un script qui effectue l'apprentissage d’un modèle Machine Learning à l’aide de la bibliothèque open source scikit-learn. Le scientifique des données souhaite tester rapidement l’exécution du script sur le cluster de calcul existant, quel type d’environnement le scientifique des données doit-il utiliser ?\n",
      "Default\n",
      "Incorrect. Les environnements par défaut n’existent pas.\n",
      "Organisé\n",
      "Correct. Les environnements curés sont idéaux pour accélérer le temps de développement.\n",
      "Custom\n",
      "2. Un travail de commande échoue avec le message d’erreur indiquant qu’un module est introuvable. Le scientifique des données a utilisé un environnement curé et souhaite ajouter un package Python spécifique pour créer un environnement personnalisé et exécuter correctement le travail. Quel fichier doit être créé avant de créer l’environnement personnalisé qui utilise un environnement curé comme référence ?\n",
      "Script d’entraînement\n",
      "Image Docker\n",
      "Spécification Conda\n",
      "Correct. Vous pouvez répertorier les packages Python dans un fichier de spécification conda.\n",
      "1. Quelle est la bonne méthode pour journaliser une métrique de modèle, _rmse, dans MLflow ?\n",
      "mlflow.log(\"RMSE\", _rmse)\n",
      "mlflow.log_artifact(\"RMSE\", _rmse)\n",
      "mlflow.log_metric(\"RMSE\", _rmse)\n",
      "Correct. Le module MLflow fournit des API « Fluent » et log_metric() est la bonne méthode pour journaliser une métrique de modèle.\n",
      "2. Léon souhaite exécuter une expérience Azure Machine Learning dans Azure Databricks. Une expérience MLflow est configurée et Léon est sur le point de l’exécuter. Il se rend compte qu’il a oublié une étape. Qu’aurait-il fallu faire en premier ?\n",
      "Inscrire le modèle dans Azure Machine Learning.\n",
      "Incorrect. Il n’est pas nécessaire d’inscrire un modèle pour exécuter une expérience.\n",
      "Journaliser les métriques d’expérience avec MLflow.\n",
      "Configurer l’URI MLflow Tracking pour utiliser Azure Machine Learning.\n",
      "Correct. L’URI MLflow Tracking doit être préalablement configuré pour utiliser AML.\n",
      "3. Vous souhaitez exécuter un pipeline Azure Machine Learning. La première étape du pipeline consiste à prétraiter les données en exécutant un script Python sur Azure Databricks Compute. La configuration est définie et vous devez maintenant créer la capacité de calcul. Quel type d’objet devez-vous utiliser ?\n",
      "DatabricksAttachConfiguration\n",
      "ComputeTarget\n",
      "Correct. Une fois la configuration définie, ComputeTarget doit être utilisé pour créer la capacité de calcul.\n",
      "DatabricksCompute\n",
      "1. Plusieurs fichiers sont stockés dans un conteneur privé dans Stockage Blob Azure. Une ressource de données Azure Machine Learning inscrite existante pointe vers le dossier contenant les fichiers. Pour utiliser les fichiers du dossier comme entrée pour un travail Machine Learning, quel préfixe doit être utilisé lors de la définition de path dans le fichier YAML du travail ?\n",
      "https:\n",
      "wasbs:\n",
      "azureml:\n",
      "Correct. Utilisez azureml: lorsque vous faites référence à une ressource de données inscrite existante.\n",
      "2. Si vous souhaitez exécuter un workflow composé de plusieurs scripts Python qui doivent s’exécuter de manière séquentielle, quel type de travail devez-vous spécifier dans le fichier YAML du travail ?\n",
      "Commande\n",
      "Pipeline\n",
      "Correct. Utiliser un travail de pipeline pour exécuter plusieurs scripts regroupés dans un workflow.\n",
      "Sweep\n",
      "------------------------\n",
      "1. Qu’est-ce qu’un connecteur pris en charge pour le paramétrage intégré ?\n",
      "Azure Data Lake Storage Gen2\n",
      "Azure Synapse Analytics\n",
      "Correct. Azure Synapse Analytics est un connecteur pris en charge pour le paramétrage intégré des services liés dans Azure Data Factory.\n",
      "Azure Key Vault\n",
      "2. Qu’est-ce qu’un exemple de création de branche d’activités utilisée dans les flux de contrôle ?\n",
      "If-condition\n",
      "Correct. Un exemple de création de branche d’activités est l’activité The If-condition qui est similaire à une instruction if fournie dans des langages de programmation.\n",
      "Until-condition\n",
      "Lookup-condition\n",
      "------------------------\n",
      "1.\n",
      "Quelle cible de calcul est utilisée lorsque AutoML s’exécute via le pool Spark dans Synapse Analytics ?\n",
      "Calcul local.\n",
      "Correct. Lorsque vous n’exécutez pas votre expérience sur les ressources de calcul d’Azure Machine Learning, l’environnement d’exécution est considéré comme un calcul local.\n",
      "Calcul distant.\n",
      "Calcul cloud.\n",
      "Incorrect. Il n’existe aucune cible de calcul de ce type pour Azure Machine Learning.\n",
      "2.\n",
      "Qu’est-ce qui peut aider à s’assurer que les modèles AutoML prennent en charge le format ONNX ?\n",
      "Définir enable_onnx_compatible_models sur False dans AutoMLConfig.\n",
      "Activer la compatibilité avec les modèles ONNX sur le portail lors de la configuration du modèle d’enrichissement.\n",
      "Correct. La compatibilité avec les modèles ONNX permet de s’assurer que le modèle prend en charge ONNX.\n",
      "Utiliser l’espace de travail Azure Machine Learning au lieu de l’espace de travail Azure Synapse.\n",
      "3.\n",
      "Parmi les affirmations suivantes sur la commande T-SQL PREDICT, laquelle est correcte ?\n",
      "Elle génère une valeur prédite ou des scores basés sur un modèle stocké dans les limites de l’entrepôt de données.\n",
      "Correct. Avec T-SQL PREDICT, vous pouvez mettre vos modèles Machine Learning entraînés avec les données historiques et les scorer dans les limites sécurisées de votre entrepôt de données.\n",
      "Elle ne prend pas en charge le format ONNX pour les modèles dans Azure SQL Managed Instance, Azure SQL Edge et Azure Synapse Analytics.\n",
      "Des autorisations spéciales sont requises pour utiliser la commande PREDICT.\n",
      "------------------------\n",
      "1.\n",
      "Une personne malveillante peut bloquer votre site web en envoyant un volume important de trafic réseau à vos serveurs. Quel service Azure peut aider Tailwind Traders à protéger son instance App Service contre ce type d’attaque ?\n",
      "Pare-feu Azure\n",
      "Groupes de sécurité réseau\n",
      "Protection DDoS dans Azure\n",
      "DDoS Protection aide à protéger vos ressources Azure contre les attaques DDoS. Une attaque DDoS tente de saturer et d’épuiser les ressources d’une application, afin de ralentir l’application ou de l’empêcher de répondre aux utilisateurs légitimes.\n",
      "2.\n",
      "Quelle est la meilleure façon pour Tailwind Traders de limiter tout le trafic sortant des machines virtuelles destiné à des hôtes connus ?\n",
      "Configurer Azure DDoS Protection pour limiter l’accès réseau aux ports et hôtes de confiance.\n",
      "Créer des règles d’application dans le pare-feu Azure.\n",
      "Le pare-feu Azure vous permet de limiter le trafic HTTP/S sortant à une liste spécifiée de noms de domaines complets (FQDN).\n",
      "Veiller à ce que toutes les applications en cours d’exécution communiquent uniquement avec des ports et des hôtes de confiance.\n",
      "3.\n",
      "Comment la société Tailwind Traders peut-elle implémenter le plus facilement possible une stratégie Refuser par défaut pour que les machines virtuelles ne puissent pas se connecter entre elles ?\n",
      "Allouer chaque machine virtuelle sur son propre réseau virtuel.\n",
      "Créer une règle de groupe de sécurité réseau qui empêche l’accès à partir d’une autre machine virtuelle sur le même réseau.\n",
      "Une règle de groupe de sécurité réseau vous permet de filtrer le trafic échangé avec les ressources par adresse IP source ou de destination, port ou protocole.\n",
      "Configurer Azure DDoS Protection pour limiter l’accès réseau au sein du réseau virtuel.\n",
      "------------------------\n",
      "1.\n",
      "Quelle étape recommanderiez-vous à l’équipe d’effectuer en premier pour comparer le coût d’exécution de ces environnements sur Azure et dans son centre de données ?\n",
      "Il s’agit simplement d’environnements de test : les démarrer et regarder la facture à la fin du mois.\n",
      "Partir du principe que les coûts d’exécution dans le cloud sont à peu près les mêmes que dans le centre de données.\n",
      "Exécuter la calculatrice du coût total de possession.\n",
      "L’exécution de la calculatrice du coût total de possession est un bon choix de départ, car elle fournit une comparaison précise des coûts d’exécution de charges de travail dans le centre de données et sur Azure, certifiée par une société de recherche indépendante.\n",
      "2.\n",
      "Quel est le meilleur moyen de garantir que l’équipe de développement ne provisionne pas trop de machines virtuelles en même temps ?\n",
      "Ne rien faire. Laisser l’équipe de développement utiliser ce dont elle a besoin.\n",
      "Appliquer des limites de dépense à l’abonnement Azure de l’équipe de développement.\n",
      "Si vous dépassez votre limite de dépense, les ressources actives sont libérées. Vous pouvez ensuite décider d’augmenter votre limite ou de provisionner moins de ressources.\n",
      "Donner verbalement au responsable du développement un budget et le tenir responsable des dépassements.\n",
      "3.\n",
      "Quel est le moyen le plus efficace pour l’équipe de test de réduire les coûts des machines virtuelles le week-end, quand les testeurs ne travaillent pas ?\n",
      "Supprimer les machines virtuelles avant le week-end et créer un ensemble la semaine suivante.\n",
      "Si vous supprimez vos machines virtuelles quand elles ne sont pas utilisées, vous perdez également tous les disques durs associés. La recréation de l’environnement au début de chaque semaine peut prendre du temps.\n",
      "Libérer les machines virtuelles inutilisées.\n",
      "Quand vous libérez des machines virtuelles, les disques durs et les données associés sont conservés dans Azure. Mais la consommation du processeur ou du réseau ne vous est pas facturée, ce qui peut réduire les coûts.\n",
      "Laisser tout fonctionner. Azure facture uniquement le temps processeur que vous utilisez.\n",
      "4.\n",
      "Les ressources dans les environnements Dev et Test sont payées par différents services. Quelle est la meilleure façon de classer les coûts par service ?\n",
      "Appliquer à chaque machine virtuelle une étiquette qui identifie le service à facturer.\n",
      "Vous pouvez appliquer des étiquettes à des groupes de ressources Azure pour organiser les données de facturation.\n",
      "Répartir le coût uniformément entre les services.\n",
      "Garder une feuille de calcul listant les ressources de chaque équipe.\n",
      "------------------------\n",
      "1. Une entreprise déploie la technologie 5G. Parmi les raisons suivantes, lesquelles sont une raison valable pour déployer la 5G dans l’entreprise :\n",
      "L’entreprise doit déployer des capteurs IoT nécessitant une faible latence et une haute densité\n",
      "Un besoin de faible latence dans les scénarios de haute densité de capteurs est un cas d’utilisation valide de la 5G\n",
      "L’entreprise doit connecter des capteurs au Wi-Fi\n",
      "L’entreprise doit utiliser plusieurs mécanismes de connectivité de proximité comme le Bluetooth\n",
      "2. Le déploiement de la 5G dans une entreprise qui veut garantir la connectivité. Laquelle de ces affirmations serait la plus pragmatique pour garantir la connectivité en 5G ?\n",
      "Elle doit envisager davantage de capteurs dans l’entreprise\n",
      "Elle doit envisager des connexions plus sécurisées\n",
      "Elle doit tenir compte du LTE en plus de la 5G\n",
      "La rétrocompatibilité avec le LTE assure une connectivité garantie même si les déploiements de la 5G sont retardés.\n",
      "3.\n",
      "Les serveurs de périphérie aident dans Azure Private MEC. Quel est l’avantage de l’utilisation de serveurs en périphérie dans Azure Private MEC ?\n",
      "Les serveurs en périphérie aident à prétraiter les données\n",
      "Les serveurs en périphérie aident à prétraiter les données afin que moins de données soient envoyées au serveur.\n",
      "Les serveurs en périphérie accélèrent la connectivité\n",
      "Les serveurs en périphérie n’accélèrent pas nécessairement la connectivité.\n",
      "Les serveurs en périphérie aident à se connecter au Wi-Fi et à la 5G\n",
      "1. Les solutions 5G pour l’entreprise sont généralement le fruit d’une collaboration entre l’entreprise cliente, l’opérateur de réseau mobile et l’intégrateur système. Quelle en est la raison principale ?\n",
      "Les solutions 5G pour l’entreprise ont besoin d’une faible latence\n",
      "Bien que les solutions 5G pour l’entreprise nécessitent une faible latence, ce n’est pas la raison principale d’une approche collaborative\n",
      "Les solutions 5G pour l’entreprise doivent être rétrocompatibles avec des réseaux plus anciens comme LTE\n",
      "Les solutions 5G pour l’entreprise prennent en charge le réseau 5G, ce qui conduit à la nécessité d’une collaboration entre plusieurs partenaires\n",
      "Les solutions 5G pour l’entreprise doivent prendre en compte le réseau sous-jacent et les fonctionnalités de l’opérateur de réseau mobile.\n",
      "2. Azure Private MEC est l’interface principale des applications périphériques et de la 5G pour l’entreprise. Laquelle de ces fonctions est l’objectif principal d’Azure Private MEC ?\n",
      "Azure Private MEC vous permet d’implémenter une solution de bout en bout pour la 5G à l’aide d’outils familiers\n",
      "Azure Private MEC aide l’écosystème des développeurs en tirant parti des outils existants pour la 5G\n",
      "Azure Private MEC offre une sécurité de bout en bout\n",
      "Azure Private MEC permet d’intégrer des fournisseurs tiers\n",
      "3.\n",
      "L’analytique périphérique est utilisée dans les situations où le temps de réponse doit être rapide avec une latence très faible. Les données vidéo sont également capturées et envoyées dans le cloud. Quel type d’analytique peut être effectué dans le cloud sur la vidéo stockée par l’entreprise ?\n",
      "Analytique périphérique supplémentaire qui complète le travail effectué dans l’entreprise\n",
      "L’analytique périphérique n’est pas effectuée dans le cloud\n",
      "Tendances à long terme tirées de la vidéo collectée dans le cloud\n",
      "La vidéo stockée peut être utilisée pour analyser les tendances à long terme\n",
      "Vous exécutez des applications de vision par ordinateur\n",
      "1.\n",
      "Supposons que vous gérez des appareils qui prennent en charge plusieurs mécanismes de connectivité. Vous devez continuellement améliorer les fonctionnalités de l’appareil et répondre aux menaces de sécurité. Azure Sphere est approprié pour fournir des solutions de sécurité pour les appareils IoT pour ce scénario, car :\n",
      "Azure Sphere fonctionne avec un grand nombre de fabricants de matériel.\n",
      "Les appareils Azure Sphere peuvent recevoir des mises à jour des applications et du système d’exploitation par voie hertzienne à l’aide de mécanismes de connectivité.\n",
      "La possibilité de recevoir des mises à jour par voie hertzienne aide à répondre aux menaces de sécurité courantes.\n",
      "Azure Sphere prend en charge la connectivité cellulaire.\n",
      "2.\n",
      "Le système d’exploitation d’Azure Sphere prend actuellement en charge deux types de connectivité de réseau local. Si vous ne souhaitez pas être affecté par des facteurs locaux tels que les murs, les armoires et les interférences d’autres appareils électroniques pour accélérer la vitesse, quelle connectivité devez-vous utiliser ?\n",
      "Cellulaire\n",
      "Wi-Fi\n",
      "Ethernet\n",
      "La connectivité Ethernet n’est pas affectée par les facteurs locaux à proximité.\n",
      "3.\n",
      "Azure Sphere ne prend pas en charge la sécurité au-delà des composants de connectivité du routeur et du modem cellulaire. Lorsque vous vous connectez Azure Sphere aux réseaux cellulaires, quelles sont les précautions suivantes à suivre pour créer un système à l’aide d’un routeur mobile ?\n",
      "Les parties hors Azure Sphere de votre solution doivent être correctement sécurisées pour garantir que le système global est robuste contre les menaces de sécurité.\n",
      "Vous devez être conscient des frontières de sécurité au sein desquelles Azure Sphere vous protège. Au-delà de ces frontières, vous devez être conscient des risques liés à l’absence de protection.\n",
      "Connectez des parties hors Azure Sphere de votre solution au réseau Ethernet pour communiquer en toute sécurité avec Azure IoT Central.\n",
      "Cette option n’est pas réalisable en fonction des options de connectivité disponibles.\n",
      "Assurez-vous que les parties Azure Sphere du système restent sécurisées, même si les parties hors Azure de votre solution ne fournissent pas de sécurité.\n",
      "1.\n",
      "Votre entreprise décide d’explorer l’éventail des solutions IoT industrielles. Vous voulez simuler le fonctionnement de différentes machines dans un environnement de production. Quelle alternative pouvez-vous recommander dans cette situation ?\n",
      "Acheter des données auprès de sources externes.\n",
      "Simuler le fonctionnement d’un appareil périphérique par le biais d’un module déployé en périphérie.\n",
      "La simulation de données au moyen d’un module de périphérie offre une méthode flexible et relativement peu coûteuse pour reproduire le fonctionnement de capteurs sur le terrain.\n",
      "Utiliser des données historiques.\n",
      "2.\n",
      "Vous avez déployé un module en périphérie et vous vous attendez à ce qu’il simule des données. Comment vérifiez-vous que le module est en cours d’exécution ?\n",
      "Vous affichez les données générées.\n",
      "Si le module est en cours d’exécution, il doit simuler des données comme n’importe quel autre capteur sur le terrain.\n",
      "Vous vérifiez l’état du module.\n",
      "Vous exécutez un test pour connaître l’état du module.\n",
      "3.\n",
      "Vous pouvez visualiser les données de température générées par un appareil simulé. Que pouvez-vous faire d’autre ?\n",
      "Examiner des données relatives à la pression.\n",
      "Examiner les anomalies présentes dans les données de température.\n",
      "Vérifier que les données relatives aux conditions ambiantes sont simulées.\n",
      "Ce capteur peut être installé dans une salle de serveurs, dans une usine ou sur une éolienne. L’analyse des données ambiantes est donc pertinente.\n",
      "1. Parmi les affirmations suivantes, laquelle est vraie quand le modèle surajuste lors de l’entraînement ?\n",
      "La justesse de l’entraînement et de la validation commencent à baisser\n",
      "La justesse de la validation augmente, la justesse de l’entraînement arrête d’augmenter\n",
      "(accrocher) La justesse de l’entraînement continue d’augmenter, la justesse de la validation commence à baisser\n",
      "La justesse de la validation devient considérablement inférieure à la justesse de l’entraînement\n",
      "2. Nous avons défini un modèle à deux couches, mais il ne montre aucun avantage comparé à un modèle moncouche. Quel peut être le problème ?\n",
      "La taille de la couche masquée est supérieure à la taille de la couche de sortie\n",
      "La taille de la couche masquée est la même que la taille de la couche de sortie\n",
      "(accrocher) Nous avons oublié de spécifier la fonction d’activation entre les couches\n",
      "Lorsque nous ne spécifions pas une activation non linéaire, deux couches sont équivalentes à une couche\n",
      "1.\n",
      "Comment le nombre de paramètres dans une couche convolutive et une couche dense sont-ils corrélés ?\n",
      "Une couche convolutive contient plus de paramètres\n",
      "Une couche convolutive contient de petits filtres qui ont les mêmes poids pour l’image entière\n",
      "Une couche convolutive contient moins de paramètres\n",
      "Correct, une couche convolutive contient de petits filtres qui ont les mêmes poids pour l’image entière\n",
      "2.\n",
      "Si la taille d’une image d’entrée est 320x200x3, quelle est la taille du tenseur après avoir appliqué une couche convolutive 5x5 avec 16 filtres ?\n",
      "316x196x16\n",
      "Bonne réponse\n",
      "316x196x3\n",
      "320x200x3x16\n",
      "320x200x48\n",
      "3.\n",
      "Quelles couches appliquez-nous pour réduire considérablement la dimension spatiale dans le CNN multicouche ?\n",
      "Convolution\n",
      "Aplatir\n",
      "MaxPooling\n",
      "Les couches de regroupement (comme MaxPooling ou AveragePooling) sont utilisées pour réduire la dimension, généralement par un facteur de 2\n",
      "4.\n",
      "Quelle couche est utilisée entre la base convolutive du réseau et le classifieur linéaire final ?\n",
      "Convolution\n",
      "(accrocher) Aplatir\n",
      "L’aplatissement est utilisé pour remodeler les tenseurs spatiaux en un vecteur linéaire\n",
      "MaxPooling\n",
      "Sigmoid\n",
      "1. Pour l’apprentissage de transfert, nous utilisons un réseau VGG-16 préentraîné sur 1 000 classes. Quel est le nombre de classes pouvant être utilisées dans notre réseau ?\n",
      "(accrocher) Quelconque\n",
      "Le nombre de classes dans un réseau préentraîné n’est pas important, car nous effectuons l’apprentissage de notre propre classifieur final\n",
      "1 000\n",
      "2\n",
      "inférieur à 1 000\n",
      "------------------------\n",
      "1. Quelle caractéristique des fonctions Azure pour IoT permet de déployer des applications IoT à grande échelle ?\n",
      "(accrocher) Les fonctions Azure pour IoT peuvent gérer les fortes augmentations et baisses de demande.\n",
      "Azure Functions pour IoT est sécurisé.\n",
      "Les fonctions Azure pour IoT peuvent gérer de grands volumes de données.\n",
      "2. Une fonction Azure comprend deux parties : les déclencheurs et les liaisons. Quel avantage apportent ensemble les déclencheurs et les liaisons ?\n",
      "Les déclencheurs et les liaisons permettent de créer une solution scalable.\n",
      "(accrocher) Les déclencheurs et les liaisons évitent d’avoir à coder en dur les accès aux autres services.\n",
      "Les déclencheurs et les liaisons fournissent une réponse rapide aux événements.\n",
      "3. Identifiez la caractéristique des fonctions Azure pour IoT garantissant que vous êtes facturé uniquement pour les ressources que vous utilisez.\n",
      "Azure Functions pour IoT est plus rapide et utilise donc moins de ressources.\n",
      "Azure Functions peut être géré à l’aide d’un tableau de bord.\n",
      "(accrocher) Vous n’avez pas besoin de pré-allouer les ressources à l’avance.\n",
      "------------------------\n",
      "1. Laquelle des déclarations suivantes caractérise-t-elle Azure Functions et l’informatique sans serveur ?\n",
      "Vous pouvez apporter vos propres conteneurs.\n",
      "Vous pouvez automatiser votre processus métier.\n",
      "(accrocher) Vous pouvez intégrer des services à l’aide de déclencheurs et de liaisons.\n",
      "Vous pouvez incorporer des services Azure dans la fonction et utiliser un déclencheur pour exécuter votre code.\n",
      "2. Supposons que vous êtes responsable de la gestion de la fiabilité des données vocales provenant de différents types d’appareils IoT connectés à un service IoT Hub. Pourquoi un service IoT Hub est-il approprié pour ce scénario ?\n",
      "(accrocher) Un service IoT Hub fournit une identité unique à chaque appareil.\n",
      "Un service IoT Hub connecte en toute sécurité l’appareil au IoT en implémentant une authentification par appareil.\n",
      "Un service IoT Hub effectue le suivi d’événements tels que la création d’appareils, les défaillances d’appareil et les connexions d’appareils.\n",
      "Un service IoT Hub est hébergé dans le cloud.\n",
      "3. Supposons que vous êtes responsable de l’implémentation de solutions d’intelligence artificielle avec un appareil IoT en tant que fonction basée sur les événements. Vous souhaitez explorer la possibilité d’interagir avec Cognitive Services. Quelle solution préconiseriez-vous pour implémenter ce scénario ?\n",
      "Vous pouvez configurer une clé API Cognitive Services pour déclencher la fonction et exécuter votre code.\n",
      "Vous utilisez une clé API pour accéder au service. Elle ne déclenche pas la fonction.\n",
      "Vous pouvez écrire une fonction qui peut être déployée localement, puis exécuter la fonction en appelant l’API Cognitive Services.\n",
      "(accrocher) Vous pouvez écrire votre code en tant que fonction Azure appelée par un déclencheur, et appeler le service cognitif dans la fonction.\n",
      "Cela vous donne la flexibilité nécessaire pour interagir avec votre API Cognitive Services.\n",
      "------------------------\n",
      "1. Comment décririez-vous un code de fonction ?\n",
      "Un code de fonction est un ensemble de fonctions interdépendantes qui peuvent être exécutées sur le serveur.\n",
      "(accrocher) Un code de fonction est le contenu principal qui s’exécute et accomplit des travaux sans aucune dépendance sur le serveur.\n",
      "Un code de fonction est simplement un code déployé dans l’environnement local pour accomplir un travail.\n",
      "2. Supposons que vous soyez chargé de déployer une API de reconnaissance vocale dans votre application de fonction. Dans quel ordre accompliriez-vous les étapes suivantes pour créer une application de traduction vocale ?\n",
      "(accrocher) Déclenchement de la fonction – reconnaissance vocale – élimination des disfluences verbales – traduction – synthèse vocale\n",
      "Vous devez déclencher la fonction, reconnaître la parole pour la convertir en texte, éliminer les disfluences verbales, traduire le texte, puis produire l’audio par synthèse vocale du texte traduit.\n",
      "Déclenchement de la fonction – élimination des disfluences verbales – traduction – synthèse vocale\n",
      "Reconnaissance vocale – élimination des disfluences verbales – traduction – synthèse vocale\n",
      "3. Supposons que vous souhaitiez développer une application à l’aide de l’API Cognitive Services dans la fonction Azure. Comment accédez-vous à l’API Cognitive Services à partir de votre application ?\n",
      "Vous pouvez accéder directement à l’API Cognitive Services à l’intérieur de la fonction d’Azure une fois que celle-ci a établi la connexion.\n",
      "Vous devez générer une clé API à chaque fois pour tenter la connexion et l’accès à l’API Cognitive Services.\n",
      "(accrocher) Vous créez un compte de service cognitif qui génère une clé API utilisable pour accéder à l’API de service\n",
      "Vous avez besoin d’une clé API pour accéder à l’API Cognitive Services dans la fonction\n",
      "------------------------\n",
      "1. Laquelle des descriptions suivantes décrit le mieux le fonctionnement de la traduction vocale ?\n",
      "La traduction vocale reconnaît la parole en provenance du microphone, la convertit en un grand nombre de fichiers audio, puis reçoit de manière asynchrone les résultats de la transcription.\n",
      "(accrocher) La traduction vocale reconnaît la parole en provenance du microphone, la convertit en fichier audio, normalise le texte, puis la traduit en fichier audio.\n",
      "Elle reconnaît la parole en provenance du microphone, la convertit en fichier audio, puis la synthétise vers un haut-parleur.\n",
      "2. Comment décririez-vous une application de fonction ?\n",
      "Une application de fonction est un runtime de fonction destiné à exécuter le code de la fonction.\n",
      "Une application de fonction est une application en conteneur qui peut être appelée à l’aide d’un déclencheur.\n",
      "(accrocher) Une application de fonction est une collection d’une ou plusieurs fonctions qui sont gérées ensemble.\n",
      "1. Parmi les commandes suivantes, lesquelles sont utilisées pour superviser l’état d’un déploiement sur la périphérie ?\n",
      "az iot edge deployment summary\n",
      "(accrocher) az iot edge deployment show\n",
      "Elle affiche les détails d’un déploiement\n",
      "az iot edge deployment status\n",
      "------------------------\n",
      "1. Dans le contexte de la solution Vision sur Edge, vous déployez plusieurs modules périphériques sur votre appareil périphérique. Lequel des éléments suivants est chargé de définir l’emplacement à partir duquel les médias doivent être capturés, la façon dont ils doivent être traités et l’emplacement où les résultats doivent être remis ?\n",
      "Module web\n",
      "(accrocher) Module Live Video Analytics\n",
      "Vous définissez le graphique multimédia sur le module Live Video Analytics qui analyse les images provenant de la caméra et les envoie au module d’inférence\n",
      "Module de prédiction\n",
      "------------------------\n",
      "1. Si vous disposez de votre propre modèle Machine Learning et que vous souhaitez l’empaqueter en tant qu’image Docker, où définissez-vous les étapes de génération du processus de génération d’image ?\n",
      "Code source de votre application\n",
      "Registre Docker\n",
      "(accrocher) Dockerfile\n",
      "Un fichier Dockerfile est un fichier texte brut contenant toutes les commandes nécessaires pour générer une image\n",
      "------------------------\n",
      "1. Si vous êtes un client et que vous souhaitez utiliser une plateforme qui vous permette de déployer différents modèles Machine Learning pour différents scénarios, Vision sur Edge est un modèle de solution approprié. Parce que :\n",
      "Vous pouvez facilement déployer des modèles sur vos caméras existantes\n",
      "C’est correct, mais cette fonctionnalité ne suffit pas pour déployer des modèles Machine Learning\n",
      "La solution Vision sur Edge utilise Live Video Analytics pour créer des applications vidéo\n",
      "(accrocher) Vous pouvez créer vos modèles et les déployer sur votre appareil périphérique\n",
      "------------------------\n",
      "1. Supposons que votre corpus de texte contient 80 000 mots différents. Parmi les affirmations suivantes, laquelle choisiriez-vous pour réduire la dimensionnalité du vecteur d’entrée au classifieur neuronal ?\n",
      "Sélectionnez 10 % des mots de façon aléatoire et ignorez le reste.\n",
      "Utiliser la couche convolutive avant la couche de classifieur entièrement connecté\n",
      "(accrocher) Utiliser la couche d’incorporation avant la couche de classifieur entièrement connecté\n",
      "Sélectionner 10 % des mots les plus fréquemment utilisés et ignorer le reste\n",
      "2. Nous souhaitons entraîner un réseau neuronal afin de générer de nouveaux mots drôles pour un livre pour enfants. Quelle architecture puis-je utiliser ?\n",
      "LSTM au niveau du mot\n",
      "(accrocher) LSTM au niveau du caractère\n",
      "Correct, les LSTM de niveau caractère capturent souvent des syllabes utilisées et placent ces séquences ensemble pour générer de nouveaux mots.\n",
      "RNN au niveau du mot\n",
      "Perceptron au niveau du caractère\n",
      "3. Le réseau neuronal récurrent est appelé ainsi pour les raisons suivantes :\n",
      "(accrocher) Un réseau est appliqué à chaque élément d’entrée, et la sortie de l’application précédente est passée à la suivante\n",
      "Il est formé par un processus récurrent\n",
      "Il se compose de couches qui incluent d’autres sous-réseaux.\n",
      "4. Quelle est l’idée principale de l’architecture réseau LSTM ?\n",
      "Nombre fixe de blocs LSTM pour l’ensemble du jeu de données\n",
      "Il contient de nombreuses couches de réseaux neuronaux récurrents\n",
      "(accrocher) Gestion d’état explicite avec oubli et déclenchement d’état\n",
      "Dans LSTM, chaque bloc reçoit et génère un état, qui est manipulé à l’intérieur du bloc en fonction de l’état d’entrée et de l’état précédent.\n",
      "5. Quelle est l’idée principale de l’attention ?\n",
      "L’attention affecte un coefficient de pondération à chaque mot du vocabulaire pour montrer son importance.\n",
      "(accrocher) L’attention est une couche réseau qui utilise la matrice d’attention pour déterminer dans quelle mesure les états d’entrée de chaque étape affectent le résultat final.\n",
      "Correct. En examinant la matrice d’attention, nous pouvons évaluer visuellement les mots qui jouent un rôle plus important dans différentes parties de la phrase.\n",
      "L’attention crée une matrice de corrélation globale entre tous les mots du vocabulaire, en présentant leur co-occurrence\n",
      "------------------------\n",
      "1. Qu’est-ce que le taux d’échantillonnage ?\n",
      "Fréquence mappée au temps.\n",
      "Les canaux audio.\n",
      "(accrocher) Échantillonnage du son analogique à des intervalles de temps cohérents pour créer une représentation numérique du son.\n",
      "Correct !\n",
      "2. Qu’est-ce que la forme d’onde ?\n",
      "Fréquence mappée au temps.\n",
      "(accrocher) Taux d’échantillonnage et fréquence visualisées.\n",
      "Correct ! Nous pouvons visualiser nos données à l’aide d’une forme d’onde pour mapper le taux d’échantillonnage et la fréquence\n",
      "Les canaux audio.\n",
      "1. Lorsque vous rééchantillonnez le son, vous...\n",
      "augmentez la taille.\n",
      "(accrocher) réduisez la taille.\n",
      "Correct ! Nous pouvons réduire la taille du fichier en réduisant le taux d’échantillonnage pour la piste audio.\n",
      "2. Qu’est-ce qu’un spectrogramme ?\n",
      "(accrocher) Mappe la fréquence sur le temps d’un fichier audio.\n",
      "Correct !\n",
      "Les canaux audio.\n",
      "Taux d’échantillonnage et fréquence visualisées.\n",
      "3. La classification audio ne peut être effectuée qu’avec la vision par ordinateur sur des spectrogrammes.\n",
      "Vrai\n",
      "(accrocher) Faux\n",
      "Correct ! Il existe plusieurs façons de créer des modèles de classification audio.\n",
      "1. Quelle est la différence entre Tensor et Variable dans TensorFlow ?\n",
      "Il n’y a aucune différence.\n",
      "Les tenseurs sont mutables, les variables sont immuables.\n",
      "Les variables sont mutables, les tenseurs sont immuables.\n",
      "Correct !\n",
      "1.\n",
      "Que nous permet de faire GradientTape ?\n",
      "Il nous permet d’enregistrer les opérations afin de pouvoir les relire plus tard.\n",
      "Il nous permet de calculer les dérivés des fonctions par rapport à leurs tenseurs d’entrée.\n",
      "Correct !\n",
      "Il accélère l’exécution.\n",
      "------------------------\n",
      "1. Quels sont les principaux avantages de l’exécution graphique ?\n",
      "Nous pouvons exécuter notre modèle dans des environnements sans Python et nous obtenons de meilleures performances.\n",
      "Correct !\n",
      "Nous pouvons facilement déboguer notre code et l’exécuter où nous voulons.\n",
      "Nous pouvons écrire moins de code et obtenir des prédictions plus justes.\n",
      "------------------------\n",
      "1. Un scientifique des données entraîne et journalise un modèle avec MLflow. Quand le scientifique des données déploie le modèle, le schéma de l’entrée et de la sortie du modèle est incorrect. Que doit personnaliser le scientifique des données pour résoudre le problème ?\n",
      "Personnaliser l’environnement du modèle.\n",
      "Changer la saveur du modèle.\n",
      "Incorrect. La saveur spécifie le framework Machine Learning du modèle.\n",
      "Personnaliser la signature du modèle.\n",
      "Correct. La signature du modèle définit les schémas des entrées et sorties.\n",
      "2. Un scientifique des données a entraîné un modèle Deep Learning avec TensorFlow. Le modèle déployé nécessite beaucoup de ressources de calcul et doit utiliser le serveur d’inférence le plus optimal pour des charges de travail similaires. Quel type de modèle est compatible avec les déploiements nécessitant beaucoup de ressources de calcul et sans code ?\n",
      "MLflow\n",
      "Triton\n",
      "Correct. Triton est idéal pour les déploiements nécessitant beaucoup de ressources de calcul.\n",
      "Custom\n",
      "------------------------\n",
      "1. Dans quelles circonstances un scientifique des données peut-il tirer le meilleur parti du calcul de processeur graphique ?\n",
      "Pour entraîner un modèle de prévision sur des données de série chronologique.\n",
      "Pour entraîner un modèle de vision par ordinateur afin de classifier des images.\n",
      "C’est correct. Les algorithmes Deep Learning sont probablement ceux qui tirent le meilleur parti du processeur graphique en raison de la complexité de l’algorithme et du besoin d’un grand jeu de données.\n",
      "Pour entraîner un modèle de régression logistique afin de prédire l’attrition clients.\n",
      "2. Quelle solution de stockage est recommandée pour entraîner un modèle de vision par ordinateur avec Azure Machine Learning ?\n",
      "Stockage Blob Azure\n",
      "Azure SQL Database\n",
      "Azure Data Lake Storage Gen2\n",
      "C’est correct. Les fichiers peuvent être stockés en utilisant une structure de dossiers imbriqués dans un lac de données en raison de l’espace de noms hiérarchique.\n",
      "------------------------\n",
      "1. Un scientifique des données souhaite entraîner un modèle Deep Learning. Pour qu’il puisse réaliser des expériences avec des jeux de données et des algorithmes, quel calcul GPU doit être mis à sa disposition ?\n",
      "(accrocher) Un cluster de calcul basse priorité.\n",
      "C’est correct. Un cluster de calcul basse priorité effectue automatiquement un scale-down quand il est inactif et est plus économique qu’un cluster dédié.\n",
      "Un cluster de calcul dédié.\n",
      "Ce n’est pas correct. Un cluster dédié n’est pas l’option la plus économique.\n",
      "Une instance de calcul.\n",
      "2. Quelle approche devez-vous utiliser pour monitorer l’utilisation du GPU lors d’une exécution Azure Machine Learning spécifique ?\n",
      "Explorer les métriques de l’espace de travail Azure Machine Learning dans le portail Azure.\n",
      "Explorer l’onglet Détails de l’exécution dans Azure Machine Learning studio.\n",
      "(accrocher) Explorer l’onglet Monitoring de l’exécution dans Azure Machine Learning studio.\n",
      "C’est correct. L’onglet Monitoring indique l’utilisation moyenne du GPU pour l’exécution.\n",
      "------------------------\n",
      "1. Pour utiliser le déploiement sans code avec Triton dans Azure Machine Learning, quel format de modèle doit être spécifié lors de l’inscription du modèle ?\n",
      "ONNX\n",
      "(accrocher) Triton\n",
      "PyTorch\n",
      "2. Pour quelle raison pourriez-vous convertir votre modèle au format ONNX ?\n",
      "(accrocher) Pour optimiser les performances de votre modèle pendant l’inférence.\n",
      "C’est correct. Cela vous permet d’utiliser ONNX Runtime, qui offre des performances supérieures lors de l’inférence.\n",
      "Pour réduire le temps d’entraînement.\n",
      "Pour distribuer l’inférence de votre modèle sur différents appareils GPU.\n",
      "------------------------\n",
      "1. Un ingénieur en Machine Learning souhaite créer un espace de travail Azure Machine Learning. Quelle commande CLI (v2) doit être utilisée ?\n",
      "az ml workspace list\n",
      "az ml workspace show\n",
      "(accrocher) az ml workspace create\n",
      "La commande workspace create crée un espace de travail.\n",
      "2. Un scientifique des données souhaite créer un environnement pour exécuter un script Python sur un cluster de calcul. Pour l’utilisation de la commande az ml environment create --file basic-env.yml, que doit contenir le fichier basic-env.yml ?\n",
      "(accrocher) Nom et version de l’environnement.\n",
      "Entre autres choses, le fichier YAML de spécification doit inclure le nom et la version de l’environnement.\n",
      "Packages et bibliothèques qui doivent être installés sur le cluster de calcul pour exécuter le script.\n",
      "Type et taille du cluster de calcul à créer.\n",
      "------------------------\n",
      "1. Quel avantage offre le déploiement d’un service Cognitive Services en tant qu’appareil IoT Edge dans un conteneur ?\n",
      "Les services Cognitive Services peuvent s’exécuter plus rapidement\n",
      "    - Le déploiement en tant que conteneur sur un appareil périphérique n’implique pas nécessairement un traitement plus rapide.\n",
      "Exécution en ligne possible\n",
      "(accrocher) Exécution hors connexion possible\n",
      "2. Pour un service Cognitive Services, où l’entraînement a lieu ?\n",
      "À la périphérie\n",
      "(accrocher) Dans le cloud\n",
      "Localement\n",
      "3. Quelle est la fonction du service Azure Container Registry ?\n",
      "(accrocher) Stockage et gestion d’images conteneur\n",
      "    - Azure Container Registry permet de stocker et de gérer des images conteneur.\n",
      "Stockage et gestion d’images\n",
      "    - Azure Container Registry ne permet pas de stocker et de gérer des images.\n",
      "Stockage et gestion de services\n",
      "------------------------\n",
      "1. Voici les composants de l’écosystème Azure Sphere : kit SDK Azure Sphere, locataire Azure Sphere, système d’exploitation Azure Sphere. Il en manque un. Lequel ?\n",
      "API Azure Sphere\n",
      "(accrocher) Service de sécurité Azure Sphere\n",
      "    Le service de sécurité Azure Sphere est un composant clé de l’écosystème Azure Sphere.\n",
      "Clé de sécurité Azure Sphere\n",
      "2. Votre organisation a créé un locataire Azure Sphere, puis a revendiqué chacun de ses appareils dans ce locataire. Dans quel but ?\n",
      "(accrocher) Vous permettre de gérer ces appareils à distance et de manière sécurisée.\n",
      "    Le locataire vous permet de gérer les appareils à distance et de manière sécurisée.\n",
      "Le locataire permet aux appareils de mieux communiquer avec d’autres locataires.\n",
      "Votre organisation peut nommer des appareils individuels.\n",
      "    Le locataire ne permet pas le nommage d’appareils.\n",
      "3. Si le service de sécurité Azure Sphere était absent, quelle fonction ne pourriez-vous pas exécuter ?\n",
      "Vous ne pourriez pas recevoir d’alertes pour des anomalies.\n",
      "    L’absence des mises à jour du service de sécurité n’est pas directement liée aux alertes en cas d’anomalie.\n",
      "Vous ne pourriez pas assurer la sécurité du système d’exploitation.\n",
      "(accrocher) Vous ne pourriez pas bénéficier des dernières mises à jour de sécurité sur les appareils Azure Sphere.\n",
      "    Vous ne pourriez pas bénéficier des dernières mises à jour de sécurité sur les appareils Azure Sphere.\n",
      "4. Comment pouvez-vous exécuter des applications temps réel sur un appareil Azure Sphere ?\n",
      "Vous ne pouvez pas exécuter des applications temps réel sur un appareil Azure Sphere.\n",
      "Vous pouvez exécuter des applications temps réel sur le cœur Cortex-A7 avec noyau Linux d’Azure Sphere.\n",
      "(accrocher) Vous pouvez exécuter des applications temps réel sur les cœurs Cortex-M4 d’Azure Sphere.\n",
      "    Vous pouvez exécuter du code directement ou un système d’exploitation temps réel, comme Azure RTOS ou FreeRTOS, sur les cœurs Cortex-M4 d’Azure Sphere.\n",
      "------------------------\n",
      "1.\n",
      "Dans le contexte de l’architecture de la solution Vision sur Edge, vous utilisez l’API Custom Vision :\n",
      "Pour détecter des objets avec des étiquettes données par l’API\n",
      "Pour entraîner un modèle pour vos images en fonction du choix de votre modèle de Machine Learning\n",
      "Custom Vision ne vous permet pas d’utiliser votre modèle Machine Learning pour détecter des objets.\n",
      "Pour entraîner un modèle de vos images en fonction d’un modèle prédéfini par Microsoft\n",
      "Le service Custom Vision utilise des algorithmes de machine learning fournis par Microsoft pour analyser des images\n",
      "2.\n",
      "Quand vous utilisez un script d’installation pour déployer les ressources nécessaires dans votre abonnement, lequel des éléments suivants est chargé d’installer les modules sur les appareils périphériques ?\n",
      "Manifeste de déploiement\n",
      "Le manifeste de déploiement est un document JSON qui indique à votre appareil quels modules installer et comment les configurer pour qu’ils fonctionnent ensemble.\n",
      "Modèles Microsoft Azure Resource Manager\n",
      "Azure Container Registry\n",
      "------------------------\n",
      "1.\n",
      "Quand vous déployez la solution, comment fonctionne le déploiement IoT Edge ?\n",
      "Les données sont collectées auprès du simulateur RTSP, le module Live Video Analytics utilise le service Custom Vision pour l’entraînement et il déploie un nouveau modèle.\n",
      "Les données sont collectées auprès du simulateur RTSP, le module web envoie les images étiquetées à Custom Vision pour l’entraînement et déploie un nouveau modèle.\n",
      "Le module web capture les images et les envoie pour l’entraînement en utilisant l’API Custom Vision.\n",
      "Les données sont collectées auprès du simulateur RTSP, le module Live Video Analytics capture des images, étiquette les objets et envoie les images étiquetées au module web pour les détecter.\n",
      "------------------------\n",
      "1.\n",
      "Contoso migre un cluster HBase local vers HDInsight, s’inquiète des éventuels problèmes de latence d’écriture avec HBase et cherche à améliorer les performances d’écriture sur HBase. Quelle solution de la liste ci-dessous aimeriez-vous proposer ?\n",
      "Migrer vers HBase sur IaaS sur Azure.\n",
      "Migrer vers HDInsight HBase avec des écritures accélérées.\n",
      "Correct. HDInsight HBase avec Écritures accélérées attache un disque gére Premium SSD à chaque serveur de région HBase (Worker Node) pendant le déploiement du cluster.\n",
      "Migrer vers HDInsight HBase Regular.\n",
      "2.\n",
      "Contoso cherche à améliorer les performances de lecture des clusters HBase HDInsight. Quelle solution de la liste ci-dessous aimeriez-vous offrir en plus des écritures accélérées ?\n",
      "Utiliser une mise en réseau plus rapide.\n",
      "Utilisez des objets Blob de pages Azure.\n",
      "Utilisez des objets BLOB de blocs Premium.\n",
      "Correct. Les objets Blob de blocs Premium offrent un accès disque hautes performances pour améliorer les performances de lecture des clusters HBase HDInsight\n",
      "3.\n",
      "La stratégie de continuité des activités de Contoso exige l’activation de la réplication asynchrone entre régions Azure entre les clusters HDInsight HBase. Quelle solution recommandez-vous ?\n",
      "Importation/Exportation\n",
      "Incorrect. La fonctionnalité d’importation/exportation exporte les tables sélectionnées vers le stockage local attaché au cluster. Après l’exportation, exportez les tables qui peuvent ensuite être importées à partir du cluster cible.\n",
      "copie du dossier hbase.\n",
      "Réplication HBase\n",
      "Correct. La réplication HBase active la réplication asynchrone entre régions Azure entre les clusters HDInsight HBase.\n",
      "------------------------\n",
      "1. Quelle est la méthode de connexion à la source de données qui charge toutes les données dans le cache Power BI et exécute des requêtes sur les données ingérées ?\n",
      "DirectQuery.\n",
      "(accrocher) Importer la connexion.\n",
      "Correct. Vous pouvez utiliser Obtenir des données dans Power BI Desktop pour vous connecter à une source de données comme SQL Server à des fins d’importation.\n",
      "Connexion hybride\n",
      "2. Qu’est-ce qui peut être considéré comme un avantage des vues matérialisées ?\n",
      "(accrocher) Une distribution différente des données par rapport aux tables de base.\n",
      "Correct. Les données comprises dans une vue matérialisée peuvent être distribuées différemment dans les tables de base.\n",
      "Une diminution des performances des requêtes pour les requêtes complexes avec des jointures et fonctions d’agrégation, mais une amélioration des performances pour les requêtes simples.\n",
      "Incorrect. Plus la requête est complexe, plus le potentiel d’enregistrement au moment de l’exécution est élevé.\n",
      "Des temps de réponse des requêtes instantanés pour les modèles de requête répétitifs.\n",
      "3. Quelle est la limite de la mise en cache des jeux de résultats ?\n",
      "(accrocher) La taille maximale du cache des jeux de résultats est de 1 To par base de données.\n",
      "Correct. Quand le cache des jeux de résultats est proche de la taille maximale, une éviction du cache démarre.\n",
      "Les utilisateurs ne peuvent pas vider manuellement le cache des jeux de résultats.\n",
      "La suspension de la base de données nettoie le cache.\n",
      "------------------------\n",
      "1. Un scientifique des données souhaite créer rapidement plusieurs pipelines pour tester différents algorithmes. Un ingénieur Machine Learning en tant que composant créé pour ces pipelines. Quelle est la méthode recommandée pour que le scientifique des données puisse gagner du temps lors du test des pipelines basés sur des composants ?\n",
      "Utiliser l’interface de ligne de commande Azure Machine Learning (v2).\n",
      "Créer un fichier YAML de tâches de pipeline peut être long et complexe, et ce n’est pas l’idéal pour une expérimentation rapide.\n",
      "Utiliser le concepteur Azure Machine Learning.\n",
      "À l’aide de l’interface utilisateur, un utilisateur peut rapidement glisser-et-déposer des composants différents vers le pipeline.\n",
      "Utiliser l’espace de travail Azure Machine Learning.\n",
      "2. Quel préfixe devez-vous utiliser lorsque vous souhaitez utiliser les entrées du pipeline comme une entrée de composant dans le fichier YAML du pipeline ?\n",
      "jobs.\n",
      "outputs.\n",
      "inputs.\n",
      "Bonne réponse.\n",
      "------------------------\n",
      "1. Quel est l’objectif de la compétence/du module « Régime permanent » ?\n",
      "Bruit aléatoire jusqu’à 5 %.\n",
      "Minimiser la référence de concentration.\n",
      "Correct. Minimizing error to the concentration reference is one of the two goal objectives of our problem.\n",
      "Limite d’itération d’épisode définie sur 90.\n",
      "Emballement thermique.\n",
      "Thermal runaway is a condition we need to prevent the brain from visiting. The corresponding goal objective related to this condition is to AVOID thermal runaway.\n",
      "2. La leçon que doit apprendre la stratégie « Régime permanent » est :\n",
      "S’entraîner avec des conditions transitoires (signal de référence de concentration, Cref_signal, égal à 1).\n",
      "S’entraîner avec différentes températures d’emballement thermique.\n",
      "S’entraîner avec des conditions de régime permanent (signal de référence de concentration, Cref_signal, égal à 5).\n",
      "Correct. This is the scenario for the steady-state condition.\n",
      "Température thermique égale à 400 kelvins.\n",
      "------------------------\n",
      "1. Une organisation utilise trois environnements dans sa stratégie MLOps (Machine Learning Operations). Dans quel environnement l’assurance qualité est-elle la plus indiquée ?\n",
      "Développement\n",
      "Incorrect. L’assurance qualité n’est pas encore nécessaire pendant le développement du modèle.\n",
      "Préproduction\n",
      "Correct. L’assurance qualité est un aspect essentiel en préproduction.\n",
      "Production\n",
      "2. Quelle raison peut justifier l’ajout d’une approbation pour un environnement quand GitHub Actions est utilisé ?\n",
      "Quelqu’un doit vérifier le code et le modèle avant de déplacer le modèle vers l’environnement suivant.\n",
      "Quelqu’un doit vérifier que l’ensemble des exécutions et des tests ont réussi avant de déplacer le modèle vers l’environnement suivant.\n",
      "Correct. Même lorsque les workflows GitHub Actions ont été exécutés avec succès, quelqu’un peut toujours avoir besoin d’examiner la sortie dans l’espace de travail Azure Machine Learning.\n",
      "Quelqu’un doit déployer et tester le modèle avant de le déplacer vers l’environnement suivant.\n",
      "------------------------\n",
      "1. Quand un modèle Machine Learning doit générer des prédictions sur de nouvelles données chaque semaine, quel est le déploiement de modèle approprié ?\n",
      "Point de terminaison privé\n",
      "Point de terminaison en ligne\n",
      "(accrocher) Point de terminaison de lot\n",
      "Correct. Utilisez des points de terminaison de lot quand vous souhaitez générer un lot de prédictions.\n",
      "2. Quelles ressources sont automatiquement générées lors de l’utilisation du déploiement sans code avec un modèle MLflow ?\n",
      "Le script d’entraînement et de scoring.\n",
      "(accrocher) Le script et l’environnement de scoring.\n",
      "Correct. Quand un modèle est journalisé avec MLflow, le script de scoring et l’environnement sont créés par l’espace de travail Azure Machine Learning lors du déploiement.\n",
      "Le script de scoring et la configuration du déploiement.\n",
      "------------------------\n",
      "1. Quel déclencheur doit être utilisé dans un workflow GitHub Actions pour exécuter des vérifications de code lorsqu’une demande de tirage est créée ?\n",
      "on: workflow_dispatch\n",
      "on: [push]\n",
      "(accrocher) on: [pull_request]\n",
      "Correct. Utiliser on: [pull_request] pour déclencher un workflow lorsqu’une demande de tirage est ouverte.\n",
      "2. Un scientifique des données a mis à jour le script d’entraînement pour un modèle, et crée une demande de tirage pour fusionner les modifications. Les vérifications de code échouent, et les détails mentionnent des erreurs telles que E302 et W292. Quelle est la cause probable de ces erreurs ?\n",
      "Les tests unitaires n’ont pas été trouvés.\n",
      "Fonctions manquantes dans les scripts d’entraînement.\n",
      "(accrocher) Erreurs stylistiques telles que les lignes vides manquantes.\n",
      "Correct. E302 et W292 sont des codes d’erreur et d’avertissement retournés lors de l’exécution de linters sur du code.\n",
      "------------------------\n",
      "1. Quand est-il préférable d’utiliser le modèle de conception du cerveau de perception ?\n",
      "Quand nous devons classifier les images.\n",
      "(accrocher) Lorsqu’il y a un opérateur expert qui décide des stratégies de contrôle en fonction de la perception avancée.\n",
      "Correct. Le modèle de conception du cerveau de perception automatisera la prise de décision comme l’humain en fonction de la perception avancée, comme la vision ou le son.\n",
      "Quand nous devons prédire la demande de produits.\n",
      "Quand nous devons identifier les sons.\n",
      "2. Quand est-il préférable d’utiliser le modèle de conception du cerveau fonctionnel ?\n",
      "Lorsqu’il existe différentes tâches qui ne peuvent être contrôlées que par un seul opérateur expert à la fois.\n",
      "Quand différentes tâches sont exécutées à l’aide des mêmes actions de contrôle.\n",
      "(accrocher) Quand différentes tâches sont exécutées à l’aide d’actions de contrôle indépendantes.\n",
      "Correct. Quand nous pouvons diviser le problème en tâches qui sont exécutées par différents opérateurs experts sur des actions de contrôle indépendantes (différents travaux dans le processus), la décomposition fonctionnelle est le meilleur choix.\n",
      "Lorsqu’un opérateur expert est en train d’effectuer la tâche.\n",
      "3. Quand est-il préférable d’utiliser le modèle de conception du cerveau stratégique ?\n",
      "Quand nous identifions des stratégies contrôlées à l’aide d’actions de contrôle indépendantes.\n",
      "Lorsqu’il existe différentes tâches contrôlées par différents opérateurs experts indépendamment.\n",
      "Lorsqu’un opérateur expert est en train d’effectuer la tâche.\n",
      "(accrocher) Quand nous identifions des stratégies contrôlées à l’aide d’actions de contrôle similaires.\n",
      "Correct. Nous identifions les stratégies à l’aide de l’exercice en trois colonnes et chaque stratégie nécessite de contrôler toutes les actions de contrôle de différentes manières en fonction du scénario.\n",
      "------------------------\n",
      "1. Vous envisagez d’utiliser le service Custom Vision pour entraîner un modèle de classification d’images. Vous souhaitez créer une ressource qui pourra être utilisée pour l’entraînement du modèle, mais pas pour la prédiction. Quelle sorte de ressource devez-vous créer dans votre abonnement Azure ?\n",
      "Custom Vision\n",
      "Correct : Quand vous créez une ressource Custom Vision, vous pouvez indiquer si elle peut être utilisée pour l’entraînement, la prédiction ou les deux.\n",
      "Cognitive Services\n",
      "Vision par ordinateur\n",
      "2. Vous entraînez un modèle de classification d’images dont les métriques d’évaluation n’atteignent pas un niveau satisfaisant. Comment pouvez-vous améliorer ce modèle ?\n",
      "En réduisant la taille des images utilisées pour entraîner le modèle.\n",
      "En ajoutant une nouvelle étiquette pour les classes « inconnues ».\n",
      "En ajoutant plus d’images au jeu de données d’entraînement.\n",
      "Correct : En règle générale, l’ajout d’images supplémentaires au projet et le réentraînement du modèle suffisent pour améliorer les performances.\n",
      "3. Vous avez publié un modèle de classification d’images. Quelles informations devez-vous fournir aux développeurs qui souhaitent l’utiliser ?\n",
      "Uniquement l’ID du projet.\n",
      "L’ID du projet, le nom du modèle ainsi que la clé et le point de terminaison de la ressource de prédiction\n",
      "Correct : Pour utiliser un modèle publié, vous avez besoin de l’ID du projet, du nom du modèle ainsi que de la clé et du point de terminaison de la ressource de prédiction.\n",
      "L’ID du projet, le numéro d’itération ainsi que la clé et le point de terminaison de la ressource d’entraînement.\n",
      "------------------------\n",
      "1. Lequel des résultats suivants un modèle de détection d’objets retourne-t-il généralement pour une image ?\n",
      "Étiquette de classe et score de probabilité pour l’image\n",
      "Coordonnées du rectangle englobant qui indiquent la région de l’image où se trouvent tous les objets qu’elle contient\n",
      "Une étiquette de classe, une probabilité et un rectangle englobant pour chaque objet dans l’image\n",
      "Correct : Un modèle de détection d’objets prédit une étiquette de classe, une probabilité et un rectangle englobant pour chaque objet dans l’image\n",
      "2. Vous envisagez d’utiliser un ensemble d’images pour entraîner un modèle de détection d’objets, puis de publier le modèle en tant que service prédictif. Vous souhaitez utiliser une seule ressource Azure avec les mêmes clé et point de terminaison pour l’entraînement et la prédiction. Quel type de ressource Azure devez-vous créer ?\n",
      "Cognitive Services\n",
      "Correct : Une ressource Cognitive Services peut être utilisée à la fois pour l’entraînement et la prédiction.\n",
      "Custom Vision\n",
      "Incorrect : La création d’une ressource Custom Vision à la fois pour l’entraînement et la prédiction aboutit au provisionnement de deux ressources distinctes, chacune avec sa propre clé et son propre point de terminaison.\n",
      "Vision par ordinateur\n",
      "------------------------\n",
      "1. Vous envisagez d’utiliser Visage pour détecter les visages humains sur une image. Comment le service indique-t-il l’emplacement des visages détectés ?\n",
      "Paire de coordonnées pour chaque visage, indiquant le centre du visage\n",
      "Deux paires de coordonnées pour chaque visage, indiquant l’emplacement des yeux\n",
      "Ensemble de coordonnées pour chaque visage, définissant un cadre englobant rectangulaire autour du visage\n",
      "Correct : Les emplacements des visages détectés sont indiqués par les coordonnées d’un cadre englobant rectangulaire\n",
      "2. Qu’est-ce qu’un aspect pouvant nuire à la détection des visages ?\n",
      "Lunettes\n",
      "Incorrect : Cet attribut n’est pas répertorié comme un handicap à la détection.\n",
      "Angles extrêmes\n",
      "Correct : Les meilleurs résultats sont obtenus lorsque les visages sont entièrement (ou le plus possible) de face\n",
      "Vitesse d’obturation rapide\n",
      "------------------------\n",
      "1. Parmi les termes suivants, lequel n’est pas un type de variable ?\n",
      "Entier (int)\n",
      "Flottant (float)\n",
      "Chaîne (str)\n",
      "Mot (word)\n",
      "Correct ! Un word n’est pas un type de variable. Utilisez plutôt une chaîne (str) pour du texte.\n",
      "2. Quelle affirmation est correcte ?\n",
      "Les listes peuvent stocker un seul type de données.\n",
      "Les listes peuvent stocker tous les types de données.\n",
      "Correct ! Les listes peuvent stocker tous les types de données.\n",
      "3. Laquelle des lignes de code suivantes devez-vous utiliser pour imprimer la valeur d’une variable appelée variable et du texte ?\n",
      "print(\"The variable's value is\"; variable)\n",
      "print(\"The variable's value is\", variable)\n",
      "Correct ! Voici comment imprimer correctement une variable et du texte.\n",
      "print(\"The variable's value is\": variable)\n",
      "print(\"The variable's value is\"~ variable)\n",
      "------------------------\n",
      "1. Comment devez-vous collecter les données de télémétrie de votre ressource Azure Cognitive Services pour une analyse ultérieure ?\n",
      "Créez une alerte.\n",
      "Configurer les paramètres de diagnostic.\n",
      "Correct. Les paramètres de diagnostic vous permettent de capturer des données pour une analyse ultérieure.\n",
      "Créer un tableau de bord.\n",
      "Incorrect. Un tableau de bord vous permet d’afficher les métriques quasiment en temps réel.\n",
      "2. Vous définissez une alerte qui vous avertit lorsqu’un événement de régénération de clé est enregistré dans le journal d’activité pour votre ressource de Cognitive Services ? Que devez-vous faire ?\n",
      "Spécifiez une étendue de journal d’activité.\n",
      "Spécifiez une action qui utilise une application logique Azure pour lire le journal d’activité.\n",
      "Incorrect. Les actions sont exécutées lorsque l’alerte est déclenchée.\n",
      "Spécifiez une condition avec un type de signal Journal d’activité.\n",
      "Correct. L’événement Clé de régénération est un signal Journal d’activité.\n",
      "3. Vous affichez une métrique pour votre ressource Cognitive Services dans un graphique. Vous souhaitez combiner le graphique avec des visualisations d’autres ressources et données. Que devez-vous faire ?\n",
      "Ajoutez le graphique un tableau de bord.\n",
      "Correct. Un tableau de bord vous permet de combiner des visualisations à partir de plusieurs ressources.\n",
      "Partagez le graphique.\n",
      "Clonez le graphique.\n",
      "------------------------\n",
      "1. À quoi sert l’intelligence artificielle ?\n",
      "À générer Skynet.\n",
      "À reproduire l’intelligence humaine à l’aide d’un ordinateur.\n",
      "Correct !\n",
      "2. Laquelle des options suivantes n’est pas un indicateur fourni par Custom Vision pour vous aider à comprendre le fonctionnement d’un modèle Machine Learning ?\n",
      "Matrice de confusion\n",
      "Correct ! Une matrice de confusion est une métrique valide, mais n’est pas fournie par Azure Cognitive Services Custom Vision.\n",
      "Rappel\n",
      "Précision moyenne\n",
      "Précision\n",
      "3. Quelle est l’erreur de débutant courante lors de l’utilisation de modèles Machine Learning ?\n",
      "Utilisation de données incorrectes.\n",
      "Correct ! Des données correctes sont essentielles pour créer un bon modèle Machine Learning.\n",
      "Utilisation d’un trop grand nombre de données.\n",
      "Utilisation de données jamais vues (nouvelles) pour le test.\n",
      "4. Le modèle ne doit pas être déployé tant qu’il n’obtient pas des résultats parfaits et qu’il n’est pas 100 % juste sur toutes les classes.\n",
      "Faux\n",
      "Correct ! Vous devez décider du moment où le modèle est suffisamment performant pour le problème que vous essayez de résoudre.\n",
      "Vrai\n",
      "------------------------\n",
      "1. Quel composant de Bonsai est responsable de la génération de modèles d’apprentissage basés sur un programme d’apprentissage ?\n",
      "Le moteur de simulation\n",
      "Le programme d’apprentissage\n",
      "Le moteur d’apprentissage\n",
      "Correct. Le composant Architecte du moteur d’apprentissage génère des modèles d’apprentissage.\n",
      "2. Dans lequel des scénarios suivants Bonsai pourrait-il être avantageux pour une organisation ?\n",
      "Une organisation a conclu qu’il existe une marge considérable pour l’amélioration des systèmes de production automatisés.\n",
      "Correct. Ce scénario est idéal pour Bonsai.\n",
      "Une organisation utilise peu les systèmes d’automatisation.\n",
      "Une organisation n’a aucune simulation, et aucun expert n’est qualifié dans l’utilisation des simulations.\n",
      "3. Lors de la conception de votre modèle de simulation pour un cerveau Bonsai, quel est l’objectif de l’état de démarrage personnalisable ?\n",
      "Cela permet de garantir une évaluation en temps réel significative pendant l’apprentissage.\n",
      "Elle permet de s’assurer que le cerveau Bonsai apprend de différentes conditions.\n",
      "Correct. Un état de démarrage personnalisable signifie que vous pouvez ajuster les conditions de démarrage pour tester les réponses du cerveau aux nouveaux scénarios.\n",
      "Il permet d’identifier le moment où un système passe à un état où la progression est impossible.\n",
      "4. Laquelle des affirmations suivantes sur l’utilisation du moteur d’apprentissage dans Bonsai est vraie ?\n",
      "L’Instructeur exécute les algorithmes d’IA sous-jacents sélectionnés par l’Architecte.\n",
      "Le Prédicteur exécute le plan d’apprentissage en configurant l’Apprenant.\n",
      "L’Architecte crée et optimise les topologies d’apprentissage.\n",
      "Correct. L’Architecte propose également la configuration des topologies et des algorithmes d’apprentissage.\n",
      "5. Quelle affirmation sur les cerveaux Bonsai est exacte ?\n",
      "Votre cerveau Bonsai ne peut contenir qu’une seule version.\n",
      "Le cerveau Bonsai apprend un programme donné via un apprentissage itératif avec le moteur d’apprentissage.\n",
      "Correct. En outre, lorsque vous apportez des modifications importantes à votre programme, Bonsai crée automatiquement une nouvelle version du cerveau.\n",
      "Lorsque vous êtes satisfait de votre cerveau Bonsai, vous pouvez exporter uniquement la version la plus récente.\n",
      "Incorrect. Vous pouvez exporter n’importe quelle version du cerveau.\n",
      "------------------------\n",
      "1. Quel avantage offre le déploiement d’un service Cognitive Services en tant qu’appareil IoT Edge dans un conteneur ?\n",
      "Les services Cognitive Services peuvent s’exécuter plus rapidement\n",
      "    - Le déploiement en tant que conteneur sur un appareil périphérique n’implique pas nécessairement un traitement plus rapide.\n",
      "Exécution en ligne possible\n",
      "(accrocher) Exécution hors connexion possible\n",
      "2. Pour un service Cognitive Services, où l’entraînement a lieu ?\n",
      "À la périphérie\n",
      "(accrocher) Dans le cloud\n",
      "Localement\n",
      "3. Quelle est la fonction du service Azure Container Registry ?\n",
      "(accrocher) Stockage et gestion d’images conteneur\n",
      "    - Azure Container Registry permet de stocker et de gérer des images conteneur.\n",
      "Stockage et gestion d’images\n",
      "    - Azure Container Registry ne permet pas de stocker et de gérer des images.\n",
      "Stockage et gestion de services\n",
      "------------------------\n",
      "1. NVIDIA Triton est un logiciel open source multi-framework qui prend en charge l’exécution de l’inférence sur des frameworks de machine learning connus, comme ...\n",
      "TensorFlow, ONNX Runtime, PyTorch et NVIDIA TensorRT\n",
      "Bonne réponse. Ces frameworks sont tous pris en charge par NVIDIA Triton Inference Server.\n",
      "Studio Azure Machine Learning et Azure Cognitive Services.\n",
      "Tout framework de machine learning basé sur Python.\n",
      "2. NVIDIA Triton Inference Server peut traiter les charges de travail ONNX sur ...\n",
      "Machines équipées de CPU ou de GPU\n",
      "Bonne réponse. NVIDIA Triton Inference Server peut exécuter des charges de travail ONNX sur des systèmes basés sur CPU et tirer parti de l’accélération sur les systèmes avec un GPU présent.\n",
      "Machines équipées de CPU uniquement\n",
      "Machines équipées de GPU uniquement\n",
      "3. Microsoft Azure prend en charge les instances GPU dans le cloud en tant qu’option lors du déploiement d’une ressource de machine virtuelle en spécifiant ...\n",
      "Options de taille\n",
      "Bonne réponse. Microsoft Azure autorise les machines virtuelles à optimisation GPU comme option de taille lors du déploiement d’une ressource de machine virtuelle.\n",
      "Options de fonctionnalité matérielle\n",
      "Options de disponibilité\n",
      "------------------------\n",
      "1. Pour prendre en charge l’inférence en temps réel dans les applications de production, quel est le meilleur choix de cible de déploiement pour le service Web de scoring ?\n",
      "Azure Kubernetes Service (AKS).\n",
      "Correct. AKS est recommandé pour les déploiements de production à grande échelle. AKS fournit un temps de réponse rapide et une mise à l’échelle automatique du service déployé.\n",
      "Azure Container Instances (ACI).\n",
      "Clusters de calcul Azure Machine Learning.\n",
      "2. Vous souhaitez déployer un modèle sur Azure Container Instances. Vous avez enregistré le modèle formé, créé un script d’entrée et un environnement. Vous souhaitez combiner le script d’entrée et l’environnement. Quel genre d’objet devez-vous utiliser ?\n",
      "InferenceConfig\n",
      "Correct. InferenceConfig représente les paramètres de configuration d’un environnement personnalisé utilisé pour le déploiement et utilise un script d’entrée et un environnement comme paramètres d’entrée.\n",
      "WebserviceDeploymentConfiguration\n",
      "ComputeTarget\n",
      "------------------------\n",
      "1. Dans quelle version de SQL Server les projets SSIS ont-ils fait leur première apparition ?\n",
      "SQL Server 2008.\n",
      "SQL Server 2012.\n",
      "Correct. Les projets SSIS ont fait leur première apparition dans SQL Server 2012, et constituent l’unité de déploiement des solutions SSIS.\n",
      "SQL Server 2016.\n",
      "2. Quel outil est utilisé pour évaluer la migration des packages SSIS vers les services Azure SQL Database ?\n",
      "L’Assistant Migration de données.\n",
      "Correct. L’Assistant Migration de données est utilisé pour évaluer la migration des packages SSIS vers les services Azure SQL Database.\n",
      "L’évaluation des migrations de données.\n",
      "Le service de migration des données.\n",
      "3. Quel outil est utilisé pour créer et déployer des packages d’intégration SQL Server sur Azure-SSIS Integration Runtime ou sur une instance locale de SQL Server ?\n",
      "SQL Server Data Tools.\n",
      "Correct. SQL Server Data Tools est généralement utilisé pour créer et déployer des packages SQL Server Integration Services (SSIS).\n",
      "SQL Server Management Studio.\n",
      "dtexec.\n",
      "------------------------\n",
      "1. Dans quel type de système de fichiers Apache Hadoop stocke-t-il des données ?\n",
      "Il n’a aucun stockage, les données sont stockées en mémoire.\n",
      "HDFS\n",
      "HDFS est l’acronyme de « Hadoop Distributed File System » et est le magasin de données d’un système Hadoop.\n",
      "RDD\n",
      "2. Quel composant Hadoop est chargé de gérer les ressources sur un système Hadoop ?\n",
      "Spark\n",
      "MapReduce\n",
      "MapReduce est un modèle de programmation qui vous permet de traiter et d’analyser des données.\n",
      "YARN\n",
      "Il s’agit du composant chargé de gérer les ressources sur un cluster Hadoop\n",
      "------------------------\n",
      "1. Dans le cadre de la configuration d’un workflow GitHub Actions pour l’exécution d’un travail Azure Machine Learning, quel déclencheur peut se révéler pratique pour vérifier si le workflow fonctionne comme prévu ?\n",
      "on: workflow_dispatch\n",
      "Incorrect. Utiliser on: workflow_dispatch pour déclencher manuellement un workflow.\n",
      "on: [push]\n",
      "on: [pull_request]\n",
      "Correct. Utiliser on: [pull_request] pour déclencher un workflow lorsqu’une demande de tirage est ouverte.\n",
      "2. Quelles étapes doivent être incluses dans un workflow GitHub Actions pour entraîner un modèle ?\n",
      "Dupliquer (fork) le dépôt, se connecter à Azure, installer l’interface CLI et exécuter un travail Azure Machine Learning.\n",
      "Extraire le dépôt, se connecter à Azure, installer l’extension ML et déclencher un travail Azure Machine Learning.\n",
      "Correct. Le travail peut entraîner un modèle à l’aide de la capacité de calcul Azure Machine Learning.\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(out)):\n",
    "    rr = out[j].split(\"\\n\")\n",
    "    rr = [i.replace(\"\\n\", \"\") for i in rr if any(i)]\n",
    "    for i in rr:\n",
    "        print(f'{i}')\n",
    "    # OU\n",
    "    f = open(f\"Q_Azure/out{j}.txt\", \"wt\")\n",
    "    for i in rr:\n",
    "        f.write(f\"{i}\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f0e35",
   "metadata": {},
   "source": [
    "# Utiliser le programme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb5cf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import PySimpleGUI as sg\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb0d33c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reddit'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sg.theme(\"DarkBlue15\")\n",
    "#sg.theme(\"LightGreen1\")\n",
    "sg.theme(\"reddit\")\n",
    "#sg.theme(\"DarkTeal2\")\n",
    "#sg.theme(\"Black\")\n",
    "#sg.theme(\"TealMono\")\n",
    "#sg.theme(\"TanBlue\")\n",
    "#sg.theme(\"Tan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba1116a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gen_rand = 206653513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8e37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credential :  <azure.identity._credentials.default.DefaultAzureCredential object at 0x7fb5ba6565f0>\n",
      "Primary key for storage account: ZJbEmvowrnI5issx8xXdG86dx+dPIn05vSG5dSbBf3Q8/pe3g+inOwDHhgQSwcc8qOyz3MzLi2ao+AStnppyTw==\n",
      "Connection string: DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=storagename206653513;AccountKey=ZJbEmvowrnI5issx8xXdG86dx+dPIn05vSG5dSbBf3Q8/pe3g+inOwDHhgQSwcc8qOyz3MzLi2ao+AStnppyTw==\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Lire des fichiers d'Azure\n",
    "# ----------------------------------\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "print('credential : ', credential)\n",
    "\n",
    "# Obtenir Nom d'abonnement\n",
    "subscription_id = '425ecd08-433c-4c16-bcb0-f1ad27233c6d'\n",
    "\n",
    "# Obtenir Resource Group Name\n",
    "RESOURCE_GROUP_NAME = f\"gdr-name-{num_gen_rand}\"\n",
    "\n",
    "# -------------------------------------\n",
    "\n",
    "# Connect à Azure compte de stockage\n",
    "STORAGE_ACCOUNT_NAME = f\"storagename{num_gen_rand}\"\n",
    "\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "\n",
    "# Provision the storage account, starting with a management object\n",
    "storage_client = StorageManagementClient(credential, subscription_id)\n",
    "\n",
    "keys = storage_client.storage_accounts.list_keys(RESOURCE_GROUP_NAME, STORAGE_ACCOUNT_NAME)\n",
    "\n",
    "print(f\"Primary key for storage account: {keys.keys[0].value}\")\n",
    "\n",
    "# Verifier acces: Avec mot de passe - obtenir le chaîne de connexion\n",
    "os.environ['AZURE_STORAGE_CONNECTION_STRING'] = f\"DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName={STORAGE_ACCOUNT_NAME};AccountKey={keys.keys[0].value}\"\n",
    "\n",
    "connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "print(f\"Connection string: {connection_string}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1eb7916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir un liste des fichiers dans le compte de stockage\n",
    "from azure.storage.fileshare import ShareDirectoryClient\n",
    "from azure.storage.fileshare import ShareFileClient\n",
    "\n",
    "# ----------------------------------\n",
    "share_name = f\"fichiershare-{num_gen_rand}\"\n",
    "directory_path = f\"directory-{num_gen_rand}\"\n",
    "\n",
    "parent_dir = ShareDirectoryClient.from_connection_string(conn_str=connection_string, \n",
    "                                                         share_name=share_name, \n",
    "                                                         directory_path=directory_path)\n",
    "\n",
    "my_list = list(parent_dir.list_directories_and_files())\n",
    "listes_de_fichiers = [my_list[i].name for i in range(len(my_list))]\n",
    "# ----------------------------------\n",
    "\n",
    "# ----------------------------------\n",
    "# Parce des noms de fichiers\n",
    "tmp = sorted(listes_de_fichiers)\n",
    "replace_le = ['out', '.txt']\n",
    "replace_avec = ['', '']\n",
    "\n",
    "tmp1 = []\n",
    "for nom in tmp:\n",
    "    for ind, i in enumerate(replace_le):\n",
    "        nom = nom.replace(i, replace_avec[ind])\n",
    "    tmp1.append(int(nom))\n",
    "ordd = np.argsort(tmp1)\n",
    "\n",
    "listes_de_fichiers_sort = [tmp[i] for i in ordd]\n",
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "725d6fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(textout):\n",
    "    to_replace = [\";\", '\\n', '</p>', '<a', 'id=', \"href=\", 'title=', 'class=', '</a>', \n",
    "                  '</sup>', '<p>', '</b>', '<sup', '>', '<', '\\\\']\n",
    "    replace_with = ''\n",
    "\n",
    "    for ind, tr in enumerate(to_replace):\n",
    "        textout = [i.replace(tr, replace_with) for i in textout]\n",
    "    return textout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e73377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt reponse:  ['Pour répondre à ces exigences, vous devez créer un jeu de données']\n",
      "myres:  0\n",
      "vrai_rep:  3. Créez un jeu de données tabulaire qui référence la banque de données et spécifie explicitement chaque fichier 'sales/mm-yyyy/sales.csv'. Enregistrez le jeu de données avec le nom sales_dataset chaque mois en tant que nouvelle version et avec une balise nommée mois indiquant le mois et l’année où il a été enregistré. Utilisez ce jeu de données pour toutes les expériences, en identifiant la version à utiliser en fonction de la balise month si nécessaire.\n",
      "gpt:  1. Créez un jeu de données tabulaire qui référence la banque de données et spécifie explicitement chaque fichier « sales/mm-yyyy/sales.csv » chaque mois. Enregistrez le jeu de données avec le nom sales_dataset chaque mois, en remplaçant le jeu de données existant et en spécifiant une balise nommée mois indiquant le mois et l’année où il a été enregistré. Utilisez ce jeu de données pour toutes les expériences.\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Erreur de faux négatif.']\n",
      "myres:  Faux négatifs\n",
      "vrai_rep:  Faux négatifs\n",
      "gpt:  Faux négatifs\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Une visualisation couramment utilisée pour évaluer la précision d'un modèle d'apprent\"]\n",
      "myres:  Courbe ROC (Receiver Operating Characteristic)\n",
      "vrai_rep:  Courbe ROC (Receiver Operating Characteristic)\n",
      "gpt:  Tracé de violon\n",
      "---------------------------------------\n",
      "gpt reponse:  ['1. Utilisez des techniques de régularisation telles que le dropout et la régularisation L2 pour']\n",
      "myres:  Ajouter une régularisation L1/L2 et Utiliser l’augmentation des données d’apprentissage\n",
      "vrai_rep:  Ajouter une régularisation L1/L2 et Utiliser l’augmentation des données d’apprentissage\n",
      "gpt:  Ajouter une régularisation L1/L2 et Utiliser l’augmentation des données d’apprentissage\n",
      "---------------------------------------\n",
      "gpt reponse:  ['pca = PCA(n_components=10).fit(X_train)x_train = pca.transform']\n",
      "myres:  0\n",
      "vrai_rep:  Box1: PCA(n_components=10); Box2: pca; Box3: transform(x_test)\n",
      "gpt:  Box1: PCA(n_components=10); Box2: pca; Box3: transform(x_test)\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"L'intégration continue peut inclure des activités telles que la compilation, le test unitaire, le test\"]\n",
      "myres:  1. Vérification lint\n",
      "vrai_rep:  1. Vérification lint\n",
      "gpt:  1. Vérification lint\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Non, le modèle de régression linéaire ne peut pas atteindre l'objectif. Les mesures\"]\n",
      "myres:  Non\n",
      "vrai_rep:  Non\n",
      "gpt:  Non\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Lorsqu'une liste est multipliée par 5, chaque élément de la liste est multiplié par\"]\n",
      "myres:  La nouvelle liste créée a la longueur 5 fois la longueur d’origine avec la séquence répétée 5 fois.\n",
      "vrai_rep:  La nouvelle liste créée a la longueur 5 fois la longueur d’origine avec la séquence répétée 5 fois.\n",
      "gpt:  None\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Ceci est une mesure de précision absolue.']\n",
      "myres:  Erreur quadratique moyenne (RMSE)\n",
      "vrai_rep:  Erreur quadratique moyenne (RMSE)\n",
      "gpt:  Rien\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Scikit-learn est une bibliothèque open source pour l'apprentissage automatique en Python. Il\"]\n",
      "myres:  Offrir une analyse prédictive des données simple et efficace\n",
      "vrai_rep:  Offrir une analyse prédictive des données simple et efficace\n",
      "gpt:  Fournir des capacités de machine learning et de deep learning\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Les valeurs de données qui influencent les modèles de prédiction comprennent les variables']\n",
      "myres:  Caractéristiques\n",
      "vrai_rep:  Caractéristiques\n",
      "gpt:  Variables dépendantes\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Le type de machine learning le plus approprié pour ce type de problème serait l'apprentissage supervisé\"]\n",
      "myres:  Régression\n",
      "vrai_rep:  Régression\n",
      "gpt:  None\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Vous devriez utiliser un modèle de machine learning appelé régression. La régression est un type de mod']\n",
      "myres:  Régression\n",
      "vrai_rep:  Régression\n",
      "gpt:  Régression\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"La tâche d'apprentissage automatique qui conviendrait le mieux à cette application serait\"]\n",
      "myres:  Sélection des fonctionnalités\n",
      "vrai_rep:  Sélection des fonctionnalités\n",
      "gpt:  None\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Vrai.']\n",
      "myres:  False\n",
      "vrai_rep:  False\n",
      "gpt:  True\n",
      "---------------------------------------\n",
      "gpt reponse:  ['- Machine Learning (apprentissage automatique)']\n",
      "myres:  Classification\n",
      "vrai_rep:  Classification\n",
      "gpt:  Clustering\n",
      "---------------------------------------\n",
      "gpt reponse:  ['-Matrice de confusion-Courbe ROC-Score de précision-Score F1']\n",
      "myres:  Taux de vrais positifs\n",
      "vrai_rep:  Taux de vrais positifs\n",
      "gpt:  Coefficient de détermination (R2)\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Faux. L'apprentissage automatique nécessite des données d'apprentissage pr\"]\n",
      "myres:  0\n",
      "vrai_rep:  Faux\n",
      "gpt:  Faux\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Vrai.']\n",
      "myres:  True\n",
      "vrai_rep:  True\n",
      "gpt:  True\n",
      "---------------------------------------\n",
      "gpt reponse:  [\"Le type de machine learning le plus approprié pour ce type d'application CRM serait l'apprentissage supervisé\"]\n",
      "myres:  Clustering\n",
      "vrai_rep:  Clustering\n",
      "gpt:  Classification\n",
      "---------------------------------------\n",
      "gpt reponse:  ['']\n",
      "myres:  A. 0, 1, 4\n",
      "vrai_rep:  A. 0, 1, 4\n",
      "gpt:  A. 0, 1, 4\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Les deux paramètres supplémentaires à ajouter au fichier config.json pour se connecter à l']\n",
      "myres:  Resource_group et Subscription_id\n",
      "vrai_rep:  Resource_group et Subscription_id\n",
      "gpt:  Region\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Pour le scénario 1, vous devez utiliser un environnement de calcul Azure Machine Learning Compute pour ex']\n",
      "myres:  1 mlc_cluster, 2 aks_cluster\n",
      "vrai_rep:  1 mlc_cluster, 2 aks_cluster\n",
      "gpt:  1 mlc_cluster, 2 aks_cluster\n",
      "---------------------------------------\n",
      "gpt reponse:  ['Vous devriez utiliser un cluster de calcul Azure HDInsight pour déployer votre espace de travail de']\n",
      "myres:  Azure Container Instances\n",
      "vrai_rep:  Azure Container Instances\n",
      "gpt:  Apache Spark pour HDInsight\n",
      "---------------------------------------\n",
      "gpt reponse:  [' )datastore = Datastore.register_azure_blob_container(workspace=ws,                                 ']\n",
      "myres:  register_azure_blob_container, create_if_not_exists = False\n",
      "vrai_rep:  register_azure_blob_container, create_if_not_exists = False\n",
      "gpt:  register_azure_blob_container, overwrite = True\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "gptcnt = 0\n",
    "SCORE = 0\n",
    "GPT_SCORE = 0\n",
    "noms = ['QUESTION', 'REPONSE0', 'REPONSE1', 'REPONSE2', 'REPONSE3', 'vrai_rep']\n",
    "\n",
    "myres = 0\n",
    "\n",
    "for ind, nom_de_fichier in enumerate(listes_de_fichiers_sort):\n",
    "    \n",
    "    # ----------------------------------\n",
    "    # Lire des fichiers d'Azure\n",
    "    # ----------------------------------\n",
    "    # Téléchargez le fichier à un path\n",
    "    file_client = ShareFileClient.from_connection_string(conn_str=connection_string, \n",
    "                                                     share_name=share_name, \n",
    "                                                     file_path=f\"{directory_path}/{nom_de_fichier}\")\n",
    "\n",
    "    with open(\"DEST_FILE\", \"wb\") as file_handle:\n",
    "        data = file_client.download_file()\n",
    "        data.readinto(file_handle)\n",
    "    # ----------------------------------\n",
    "    \n",
    "    # ----------------------------------\n",
    "    # Lire des fichiers de mon PC\n",
    "    # ----------------------------------\n",
    "    out = []\n",
    "    with open(\"DEST_FILE\", 'r') as reader:\n",
    "        out.append(reader.read())\n",
    "    out = out[0].split('\\n')\n",
    "    out = [i for i in out if any(i)]\n",
    "    # ----------------------------------\n",
    "    \n",
    "    d = dict(zip(noms, out))\n",
    "  \n",
    "    \n",
    "    # ----------------------------------\n",
    "    prompt = d['QUESTION']\n",
    "    response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, \n",
    "                                        temperature=0.1, max_tokens=30, \n",
    "                                        top_p=1, n=1, logprobs=2)\n",
    "    \n",
    "    \n",
    "    df = pd.json_normalize(response.choices)\n",
    "    d['chatgpt_rep'] = clean_text(df.text.to_numpy())\n",
    "    print('gpt reponse: ', d['chatgpt_rep'])\n",
    "    \n",
    "    # Determiner laquelle est similar à le reponse de chatgpt\n",
    "    a = ['REPONSE0', 'REPONSE1', 'REPONSE2', 'REPONSE3']\n",
    "    out = []\n",
    "    for i in a:\n",
    "        out.append(fuzz.partial_ratio(d['chatgpt_rep'], d[i]))\n",
    "    v_ind = np.argmax(out)\n",
    "    v = a[v_ind]\n",
    "    d['chatgpt_choix'] = d[v] \n",
    "    # ----------------------------------\n",
    "    \n",
    "    if ind == len(listes_de_fichiers)-1:\n",
    "        layout = [[sg.Text(f'Current SCORE: {SCORE}')], \n",
    "                  [sg.Text(d['QUESTION'])]]\n",
    "    else:\n",
    "        layout = [[sg.Text(f'Q{ind}, MY Current SCORE: {SCORE}, chatgpt: {v_ind+1}, GPTCHAT Score: {GPT_SCORE}')], \n",
    "                  [sg.Multiline(d['QUESTION'], size=(40,10))],\n",
    "              [sg.Button(d['REPONSE0'])],  \n",
    "              [sg.Button(d['REPONSE1'])],\n",
    "              [sg.Button(d['REPONSE2'])],\n",
    "              [sg.Button(d['REPONSE3'])]]\n",
    "    \n",
    "    # Create the window\n",
    "    window = sg.Window(\"DataLogger\", layout, margins=(200, 200))\n",
    "    \n",
    "    event, values = window.read()\n",
    "    \n",
    "    if event == d['REPONSE0'] and d['vrai_rep'] == d['REPONSE0']:  \n",
    "        cnt = cnt + 1\n",
    "        myres = d['REPONSE0']\n",
    "    if event == d['REPONSE1'] and d['vrai_rep'] == d['REPONSE1']:\n",
    "        cnt = cnt + 1\n",
    "        myres = d['REPONSE1']\n",
    "    if event == d['REPONSE2'] and d['vrai_rep'] == d['REPONSE2']:\n",
    "        cnt = cnt + 1\n",
    "        myres = d['REPONSE2']\n",
    "    if event == d['REPONSE3'] and d['vrai_rep'] == d['REPONSE3']:\n",
    "        cnt = cnt + 1\n",
    "        myres = d['REPONSE3']\n",
    "        \n",
    "    SCORE = cnt/(ind+1) * 100\n",
    "    \n",
    "    # ----------------------------------\n",
    "    \n",
    "    if d['chatgpt_choix'] == d['REPONSE0'] and d['vrai_rep'] == d['REPONSE0']:  \n",
    "        gptcnt = gptcnt + 1\n",
    "    if d['chatgpt_choix'] == d['REPONSE1'] and d['vrai_rep'] == d['REPONSE1']:\n",
    "        gptcnt = gptcnt + 1\n",
    "    if d['chatgpt_choix'] == d['REPONSE2'] and d['vrai_rep'] == d['REPONSE2']:\n",
    "        gptcnt = gptcnt + 1\n",
    "    if d['chatgpt_choix'] == d['REPONSE3'] and d['vrai_rep'] == d['REPONSE3']:\n",
    "        gptcnt = gptcnt + 1\n",
    "        \n",
    "    GPT_SCORE = gptcnt/(ind+1) * 100\n",
    "    \n",
    "    # ----------------------------------\n",
    "    \n",
    "    print('myres: ', myres)\n",
    "    print('vrai_rep: ', d['vrai_rep'])\n",
    "    print('gpt: ', d['chatgpt_choix'])\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "    if myres == 0:\n",
    "        f = open(\"resultants.txt\", \"a\")\n",
    "        f.write(f\"{d['QUESTION']}\\n{myres}\\n{d['vrai_rep']}\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.close()\n",
    "        \n",
    "    myres = 0\n",
    "    window.close()\n",
    "\n",
    "\n",
    "sleep(1)  # Wait 10 second\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3543a39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c003a63c",
   "metadata": {},
   "source": [
    "# PLUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19804790",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Charger des fichiers au sharefile \n",
    "from azure.storage.fileshare import ShareFileClient\n",
    "\n",
    "file_client = ShareFileClient.from_connection_string(conn_str=connection_string, \n",
    "                                                     share_name=share_name, \n",
    "                                                     file_path=f\"{directory_path}/{nom_de_fichier}\")\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "path = \"/home/oem2/Documents/PROGRAMMING/Github_analysis_PROJECTS/Créer_des_questionnaires/Q_Azure\"\n",
    "listes_de_fichiers = os.listdir(path)\n",
    "\n",
    "# Parce des noms de fichiers\n",
    "tmp = sorted(listes_de_fichiers)\n",
    "replace_le = ['out', '.txt']\n",
    "replace_avec = ['', '']\n",
    "\n",
    "tmp1 = []\n",
    "for nom in tmp:\n",
    "    for ind, i in enumerate(replace_le):\n",
    "        nom = nom.replace(i, replace_avec[ind])\n",
    "    tmp1.append(int(nom))\n",
    "ordd = np.argsort(tmp1)\n",
    "\n",
    "listes_de_fichiers_sort = [tmp[i] for i in ordd]\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "for ind, nom_de_fichier in enumerate(listes_de_fichiers_sort):\n",
    "\n",
    "    with open(f\"Q_Azure/{nom_de_fichier}\", \"rb\") as source_file:\n",
    "        file_client.upload_file(source_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62878d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e66a967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
